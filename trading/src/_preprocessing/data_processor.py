"""
ìµœì í™”ëœ ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ - ê¸°ë³¸ OHLCV + í•µì‹¬ ì§€í‘œë§Œ ìƒì„±
feature_selectorê°€ ì¤‘ë³µ ì œê±°ë¥¼ ë‹´ë‹¹í•˜ë„ë¡ ì—­í•  ë¶„ë‹´
"""
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Union, Any
from sklearn.preprocessing import RobustScaler, StandardScaler
import talib as ta
from datetime import datetime
from pathlib import Path
from src.preprocessing.feature_selector import FeatureSelector
from src.preprocessing.news_processor import NewsProcessor
import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)
from src.config.config import (
    WINDOW_SIZE,
    TRAIN_RATIO,
    VALID_RATIO,
    TEST_RATIO,
    DATA_DIR,
    LOGGER
)
from src.utils.utils import create_directory, save_to_csv, load_from_csv

class DataProcessor:
    """
    ì£¼ì‹ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤ - íš¨ìœ¨ì ì¸ í”¼ì²˜ ìƒì„± + ìŠ¤ë§ˆíŠ¸í•œ ì¤‘ë³µ ì œê±°
    """
    
    def __init__(self, window_size: int = WINDOW_SIZE, enable_feature_selection: bool = True, 
                 correlation_threshold: float = 0.85, variance_threshold: float = 0.0001
                #  ,enable_news_sentiment: bool = True
                 ):
        """
        DataProcessor í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            window_size: ê´€ì¸¡ ìœˆë„ìš° í¬ê¸°
            enable_feature_selection: í”¼ì²˜ ì„ íƒ í™œì„±í™” ì—¬ë¶€
            correlation_threshold: ìƒê´€ê´€ê³„ ì„ê³„ê°’ (85% ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼)
            variance_threshold: ë¶„ì‚° ì„ê³„ê°’ (0.0001 ì´í•˜ë©´ ë³€ë™ì„± ì—†ëŠ” í”¼ì²˜ë¡œ ê°„ì£¼)
            enable_news_sentiment: ë‰´ìŠ¤ ê°ì„±ë¶„ì„ í™œì„±í™” ì—¬ë¶€
        """
        self.window_size = window_size
        self.scalers = {}
        self.normalized_data_columns = {}
        self.enable_feature_selection = enable_feature_selection
        # self.enable_news_sentiment = enable_news_sentiment
        
        # í•„ìˆ˜ ë³´ì¡´ ì»¬ëŸ¼ë“¤ (ì ˆëŒ€ ì œê±°ë˜ë©´ ì•ˆ ë˜ëŠ” ì»¬ëŸ¼ë“¤)
        self.preserve_columns = ['open', 'high', 'low', 'close', 'volume']
        
        self.feature_selector = FeatureSelector(
            correlation_threshold=correlation_threshold,
            variance_threshold=variance_threshold,
            preserve_columns=self.preserve_columns,
            remove_duplicates_only=True
        ) if enable_feature_selection else None
        
        # ë‰´ìŠ¤ ê°ì„±ë¶„ì„ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        # self.news_processor = NewsProcessor() if enable_news_sentiment else None
        
        LOGGER.info(f"ğŸ“Š íš¨ìœ¨ì ì¸ DataProcessor ì´ˆê¸°í™” ì™„ë£Œ")
        LOGGER.info(f"   ğŸ”§ ìœˆë„ìš° í¬ê¸°: {window_size}")
        LOGGER.info(f"   ğŸ¯ í”¼ì²˜ ì„ íƒ: {'í™œì„±í™”' if enable_feature_selection else 'ë¹„í™œì„±í™”'}")
        LOGGER.info(f"   ğŸ“ ìƒê´€ê´€ê³„ ì„ê³„ê°’: {correlation_threshold}")
        LOGGER.info(f"   ğŸ“ ë¶„ì‚° ì„ê³„ê°’: {variance_threshold}")
        LOGGER.info(f"   ğŸ”’ í•„ìˆ˜ ë³´ì¡´ ì»¬ëŸ¼: {self.preserve_columns}")
        # LOGGER.info(f"   ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„: {'í™œì„±í™”' if enable_news_sentiment else 'ë¹„í™œì„±í™”'}")
    
    def preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        ì£¼ì‹ ë°ì´í„° ì „ì²˜ë¦¬ (ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì´ìƒì¹˜ ì œê±° ë“±)
        """
        if data.empty:
            LOGGER.warning("ë¹ˆ ë°ì´í„°í”„ë ˆì„ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return data

        # âœ… ë³µì‚¬ í›„ ì •ë ¬ (ìˆœì„œ ì¤‘ìš”)
        data = data.copy()
        data = data.sort_index()  # ì‹œê°„ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬

        # âœ… ë©”íƒ€ ì»¬ëŸ¼ ì œê±°
        meta_cols = ['symbol', 'ticker', 'asset', 'exchange']
        data.drop(columns=[col for col in meta_cols if col in data.columns], inplace=True)

        LOGGER.info(f"ì „ì²˜ë¦¬ ì‹œì‘ - ì»¬ëŸ¼ ëª©ë¡: {list(data.columns)}")

        # âœ… ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
        data.fillna(method='ffill', inplace=True)
        data.fillna(method='bfill', inplace=True)

        # âœ… ì¤‘ë³µ ì¸ë±ìŠ¤ ì œê±°
        data = data[~data.index.duplicated(keep='first')]

        # âœ… í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ë° ì²˜ë¦¬
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in required_cols:
            if col not in data.columns:
                upper_col = col.upper()
                if upper_col in data.columns:
                    data[col] = data.pop(upper_col)
                    LOGGER.warning(f"{col} ì»¬ëŸ¼ì„ {upper_col}ì—ì„œ ê°€ì ¸ì™€ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
                else:
                    LOGGER.warning(f"ë°ì´í„°ì— í•„ìˆ˜ ì»¬ëŸ¼ '{col}'ì´ ì—†ìŠµë‹ˆë‹¤.")

        # âœ… ë°ì´í„° íƒ€ì… ì •ë¦¬
        for col in required_cols:
            if col in data.columns:
                try:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
                    LOGGER.debug(f"{col} ì»¬ëŸ¼ì„ float64 íƒ€ì…ìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤")
                except Exception as e:
                    LOGGER.error(f"{col} ì»¬ëŸ¼ ë³€í™˜ ì˜¤ë¥˜: {e}")

        # âœ… 0 ì´í•˜ ê°’ ì²˜ë¦¬
        for col in ['open', 'high', 'low', 'close']:
            if col in data.columns:
                mask = data[col] <= 0
                if mask.any():
                    LOGGER.warning(f"{col} ì—´ì— 0 ì´í•˜ ê°’ {mask.sum()}ê°œ â†’ ì´ì „ ê°’ìœ¼ë¡œ ëŒ€ì²´")
                    data.loc[mask, col] = data[col].shift(1)[mask]

        # âœ… ë§ˆì§€ë§‰ ê²°ì¸¡ì¹˜ ì ê²€
        if data.isna().any().any():
            LOGGER.warning(f"ë‚¨ì€ ê²°ì¸¡ì¹˜ {data.isna().sum().sum()}ê°œ ì¡´ì¬ â†’ ì¶”ê°€ ì²˜ë¦¬")
            data.fillna(method='ffill', inplace=True)
            data.fillna(method='bfill', inplace=True)
            for col in data.columns:
                if data[col].isna().any():
                    if data[col].dtype.kind in 'iufc':
                        data[col].fillna(data[col].mean(), inplace=True)
                    else:
                        data[col].fillna(0, inplace=True)

        LOGGER.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(data)} í–‰, {len(data.columns)} ì»¬ëŸ¼")
        return data
        
    def extract_features(self, data: pd.DataFrame, symbol: str = None) -> pd.DataFrame:
        """
        í•µì‹¬ ê¸°ìˆ ì  ì§€í‘œë§Œ ì¶”ê°€ - íš¨ìœ¨ì„± ì¤‘ì‹¬ ì ‘ê·¼
        ê¸°ë³¸ OHLCV + í•µì‹¬ ì§€í‘œë§Œ ìƒì„±í•˜ì—¬ feature_selectorê°€ ìµœì í™”í•˜ë„ë¡ í•¨
        
        Args:
            data: ì „ì²˜ë¦¬ëœ ì£¼ì‹ ë°ì´í„° ë°ì´í„°í”„ë ˆì„
            symbol: ì£¼ì‹ ì‹¬ë³¼ (ë‰´ìŠ¤ ê°ì„±ë¶„ì„ìš©)
            
        Returns:
            í•µì‹¬ íŠ¹ì„±ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        if data.empty:
            LOGGER.warning("ë¹ˆ ë°ì´í„°í”„ë ˆì„ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return data
        
        df = data.copy()
        
        # ê¸°ë³¸ ê°€ê²© ë°ì´í„° í™•ì¸
        if not all(col in df.columns for col in ['open', 'high', 'low', 'close', 'volume']):
            LOGGER.error("í•„ìˆ˜ OHLCV ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return df
        
        open_prices = df['open'].values
        high_prices = df['high'].values
        low_prices = df['low'].values
        close_prices = df['close'].values
        volumes = df['volume'].values
        
        LOGGER.info("í•µì‹¬ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹œì‘...")
        
        try:
            # =============================================================================
            # ğŸ“Š 1. ì´ë™í‰ê· ì„  (ë‹¤ì–‘í•œ ê¸°ê°„) - íŠ¸ë Œë“œ íŒŒì•…ìš©
            # =============================================================================
            LOGGER.debug("ì´ë™í‰ê· ì„  ê³„ì‚° ì¤‘...")
            df['sma_5'] = self._safe_ta(ta.SMA, close_prices, timeperiod=5)
            df['sma_10'] = self._safe_ta(ta.SMA, close_prices, timeperiod=10)
            df['sma_20'] = self._safe_ta(ta.SMA, close_prices, timeperiod=20)
            df['sma_50'] = self._safe_ta(ta.SMA, close_prices, timeperiod=50)
            df['sma_200'] = self._safe_ta(ta.SMA, close_prices, timeperiod=200)
            
            # ì§€ìˆ˜ì´ë™í‰ê· 
            df['ema_12'] = self._safe_ta(ta.EMA, close_prices, timeperiod=12)
            df['ema_26'] = self._safe_ta(ta.EMA, close_prices, timeperiod=26)
            df['ema_50'] = self._safe_ta(ta.EMA, close_prices, timeperiod=50)
            
            # =============================================================================
            # ğŸ“ˆ 2. ëª¨ë©˜í…€ ì§€í‘œ - ë§¤ë§¤ ì‹ í˜¸ìš©
            # =============================================================================
            LOGGER.debug("ëª¨ë©˜í…€ ì§€í‘œ ê³„ì‚° ì¤‘...")
            
            # MACD
            macd, macd_signal, macd_hist = self._safe_ta(ta.MACD, close_prices, fastperiod=12, slowperiod=26, signalperiod=9)
            df['macd'] = macd
            df['macd_signal'] = macd_signal
            df['macd_histogram'] = macd_hist
            
            # RSI
            df['rsi_14'] = self._safe_ta(ta.RSI, close_prices, timeperiod=14)
            df['rsi_30'] = self._safe_ta(ta.RSI, close_prices, timeperiod=30)
            
            # Stochastic
            slowk, slowd = self._safe_ta(ta.STOCH, high_prices, low_prices, close_prices, 
                                         fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)
            df['stoch_k'] = slowk
            df['stoch_d'] = slowd
            
            # Williams %R
            df['williams_r'] = self._safe_ta(ta.WILLR, high_prices, low_prices, close_prices, timeperiod=14)
            
            # =============================================================================
            # ğŸ“Š 3. ë³€ë™ì„± ì§€í‘œ - ë¦¬ìŠ¤í¬ ê´€ë¦¬ìš©
            # =============================================================================
            LOGGER.debug("ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì¤‘...")
            
            # Bollinger Bands
            bb_upper, bb_middle, bb_lower = self._safe_ta(ta.BBANDS, close_prices, timeperiod=20, nbdevup=2, nbdevdn=2)
            df['bb_upper'] = bb_upper
            df['bb_middle'] = bb_middle
            df['bb_lower'] = bb_lower
            df['bb_width'] = (bb_upper - bb_lower) / bb_middle
            df['bb_position'] = (close_prices - bb_lower) / (bb_upper - bb_lower)
            
            # ATR (Average True Range)
            df['atr_14'] = self._safe_ta(ta.ATR, high_prices, low_prices, close_prices, timeperiod=14)
            df['atr_30'] = self._safe_ta(ta.ATR, high_prices, low_prices, close_prices, timeperiod=30)
            
            # =============================================================================
            # ğŸ¯ 4. ì¶”ì„¸ ì§€í‘œ - ë°©í–¥ì„± íŒŒì•…ìš©
            # =============================================================================
            LOGGER.debug("ì¶”ì„¸ ì§€í‘œ ê³„ì‚° ì¤‘...")
            
            # ADX (Average Directional Index)
            df['adx'] = self._safe_ta(ta.ADX, high_prices, low_prices, close_prices, timeperiod=14)
            df['di_plus'] = self._safe_ta(ta.PLUS_DI, high_prices, low_prices, close_prices, timeperiod=14)
            df['di_minus'] = self._safe_ta(ta.MINUS_DI, high_prices, low_prices, close_prices, timeperiod=14)
            
            # CCI (Commodity Channel Index)
            df['cci_14'] = self._safe_ta(ta.CCI, high_prices, low_prices, close_prices, timeperiod=14)
            
            # =============================================================================
            # ğŸ’° 5. ê±°ë˜ëŸ‰ ì§€í‘œ - ê°•ë„ ì¸¡ì •ìš©
            # =============================================================================
            LOGGER.debug("ê±°ë˜ëŸ‰ ì§€í‘œ ê³„ì‚° ì¤‘...")
            
            # OBV (On Balance Volume)
            df['obv'] = self._safe_ta(ta.OBV, close_prices, volumes)
            
            # MFI (Money Flow Index)
            df['mfi_14'] = self._safe_ta(ta.MFI, high_prices, low_prices, close_prices, volumes, timeperiod=14)
            
            # Volume moving averages
            df['volume_sma_10'] = self._safe_ta(ta.SMA, volumes, timeperiod=10)
            df['volume_sma_30'] = self._safe_ta(ta.SMA, volumes, timeperiod=30)
            
            # =============================================================================
            # ğŸ”§ 6. ê¸°ë³¸ íŒŒìƒ ì§€í‘œ - ìˆ˜í•™ì  ë³€í™˜
            # =============================================================================
            LOGGER.debug("ê¸°ë³¸ íŒŒìƒ ì§€í‘œ ê³„ì‚° ì¤‘...")
            
            # ìˆ˜ìµë¥  ì§€í‘œ
            df['returns_1d'] = df['close'].pct_change(1)
            df['returns_5d'] = df['close'].pct_change(5)
            df['returns_10d'] = df['close'].pct_change(10)
            
            # ê°€ê²© ë¹„ìœ¨
            df['high_low_ratio'] = df['high'] / df['low']
            df['close_open_ratio'] = df['close'] / df['open']
            
            # ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
            df['volume_change'] = df['volume'].pct_change(1)
            df['volume_ratio_10d'] = df['volume'] / df['volume_sma_10']
            
            # ë³€ë™ì„± ì§€í‘œ
            df['price_volatility'] = (df['high'] - df['low']) / df['close']
            
            # =============================================================================
            # ğŸ• 7. ì‹œê°„ ê¸°ë°˜ íŒ¨í„´ (ê¸°ë³¸ì ì¸ ìˆœí™˜ íŒ¨í„´)
            # =============================================================================
            LOGGER.debug("ì‹œê°„ íŒ¨í„´ ê³„ì‚° ì¤‘...")
            
            # ì‹œê°„ ìˆœí™˜ íŒ¨í„´ (if timestamp available)
            if hasattr(df.index, 'hour'):
                df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)
                df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24)
            else:
                df['hour_sin'] = 0
                df['hour_cos'] = 1
                
            if hasattr(df.index, 'dayofweek'):
                df['day_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7)
                df['day_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7)
            else:
                df['day_sin'] = 0
                df['day_cos'] = 1
            
            # =============================================================================
            # ğŸ¯ 8. íŠ¸ë ˆì´ë”© ì‹ í˜¸ ì§€í‘œ (ê°„ë‹¨í•œ ì¡°í•©)
            # =============================================================================
            LOGGER.debug("íŠ¸ë ˆì´ë”© ì‹ í˜¸ ê³„ì‚° ì¤‘...")
            
            # ì´ë™í‰ê·  êµì°¨
            df['sma_5_above_20'] = (df['sma_5'] > df['sma_20']).astype(int)
            df['sma_20_above_50'] = (df['sma_20'] > df['sma_50']).astype(int)
            
            # ê°€ê²© ìœ„ì¹˜ ì§€í‘œ
            df['price_above_sma_20'] = (df['close'] > df['sma_20']).astype(int)
            df['price_above_sma_50'] = (df['close'] > df['sma_50']).astype(int)
            
            # RSI ë ˆë²¨
            df['rsi_oversold'] = (df['rsi_14'] < 30).astype(int)
            df['rsi_overbought'] = (df['rsi_14'] > 70).astype(int)
            
            # ë‰´ìŠ¤ ê°ì„±ë¶„ì„ í”¼ì²˜ ì¶”ê°€
            # if self.enable_news_sentiment and self.news_processor is not None and symbol is not None:
            #     LOGGER.info(f"ğŸ“° {symbol} ë‰´ìŠ¤ ê°ì„±ë¶„ì„ í”¼ì²˜ ì¶”ê°€ ì¤‘... (ê¸°ê°„: {df.index[0].strftime('%Y-%m-%d')} ~ {df.index[-1].strftime('%Y-%m-%d')})")
            #     df = self.news_processor.add_sentiment_features(df, symbol)
            #     LOGGER.info(f"âœ… {symbol} ë‰´ìŠ¤ ê°ì„±ë¶„ì„ í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ")
            
        except Exception as e:
            LOGGER.error(f"ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ê¸°ë³¸ ì§€í‘œë¼ë„ ì¶”ê°€
            df['returns_1d'] = df['close'].pct_change(1)
            df['price_volatility'] = (df['high'] - df['low']) / df['close']
        
        # ì´ìƒì¹˜ ë° ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
        df = self._handle_outliers_and_infinities(df)
        
        # NaN ê°’ ì²˜ë¦¬
        df.fillna(method='ffill', inplace=True)
        df.fillna(method='bfill', inplace=True)
        df.fillna(0, inplace=True)
        
        # ğŸ”’ **ì¤‘ìš”**: close ì»¬ëŸ¼ì´ ë§ˆì§€ë§‰ì— ìœ„ì¹˜í•˜ë„ë¡ ë³´ì¥
        df = self._ensure_close_column_last(df)
        
        initial_features = len(df.columns) - 5  # OHLCV ì œì™¸
        LOGGER.info(f"í•µì‹¬ ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€ ì™„ë£Œ: {initial_features}ê°œ íŒŒìƒ ì§€í‘œ ìƒì„±")
        LOGGER.info(f"ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ (OHLCV + {initial_features} íŒŒìƒì§€í‘œ)")
        
        return df
    
    def _ensure_close_column_last(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        close ì»¬ëŸ¼ì´ ë§ˆì§€ë§‰ì— ìœ„ì¹˜í•˜ë„ë¡ ë³´ì¥
        trading_envì—ì„œ [-1] ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•¨
        """
        if 'close' in df.columns:
            # close ì»¬ëŸ¼ì„ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼
            other_columns = [col for col in df.columns if col != 'close']
            # closeë¥¼ ë§ˆì§€ë§‰ì— ë°°ì¹˜
            reordered_columns = other_columns + ['close']
            df = df[reordered_columns]
            LOGGER.debug("close ì»¬ëŸ¼ì„ ë§ˆì§€ë§‰ ìœ„ì¹˜ë¡œ ì´ë™ ì™„ë£Œ")
        else:
            LOGGER.warning("close ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return df
    
    def _safe_ta(self, ta_func, *args, **kwargs):
        """
        TA-Lib í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ í˜¸ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        """
        try:
            # ì…ë ¥ ë°°ì—´ íƒ€ì… í™•ì¸ ë° ê°•ì œ ë³€í™˜
            new_args = []
            for arg in args:
                if isinstance(arg, np.ndarray):
                    if arg.dtype != np.float64:
                        arg = arg.astype(np.float64)
                    
                    # NaN ê°’ ì²˜ë¦¬
                    if np.isnan(arg).any():
                        mean_val = np.nanmean(arg) if np.any(~np.isnan(arg)) else 0
                        arg = np.nan_to_num(arg, nan=mean_val)
                new_args.append(arg)
            
            result = ta_func(*new_args, **kwargs)
            return result
        except Exception as e:
            LOGGER.warning(f"ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨ ({ta_func.__name__}): {str(e)}")
            # ë‹¤ì¤‘ ì¶œë ¥ í•¨ìˆ˜ì¸ì§€ í™•ì¸
            if ta_func.__name__ in ['BBANDS', 'MACD', 'STOCH', 'AROON']:
                shapes = [len(args[0])] * 3
                return [np.full(shape, np.nan) for shape in shapes]
            else:
                return np.full(len(args[0]), np.nan)
    
    def _handle_outliers_and_infinities(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ì´ìƒì¹˜ ë° ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬ (ë³´ìˆ˜ì  ì ‘ê·¼)
        """
        # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ì€ ì œì™¸í•˜ê³  ì´ìƒì¹˜ ì²˜ë¦¬
        protected_cols = self.preserve_columns
        
        for col in df.columns:
            if col in protected_cols:
                continue  # í•„ìˆ˜ ì»¬ëŸ¼ì€ ì´ìƒì¹˜ ì²˜ë¦¬ ì œì™¸
                
            if df[col].dtype == np.float64 or df[col].dtype == np.int64:
                # ê²°ì¸¡ì¹˜ê°€ ë„ˆë¬´ ë§ì€ ì»¬ëŸ¼ì€ ê±´ë„ˆëœ€
                if df[col].isna().sum() > len(df) * 0.5:
                    continue
                    
                # 99.5%/0.5% ê²½ê³„ê°’ìœ¼ë¡œ í´ë¦¬í•‘
                q_low = df[col].quantile(0.005)
                q_high = df[col].quantile(0.995)
                df[col] = df[col].clip(lower=q_low, upper=q_high)
        
        # ë‚¨ì€ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        # ê²€ì¦
        nan_counts = df.isna().sum().sum()
        if nan_counts > 0:
            LOGGER.warning(f"ê²°ì¸¡ì¹˜ {nan_counts}ê°œê°€ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤")
        
        numeric_data = df.select_dtypes(include=[np.number])
        inf_counts = np.isinf(numeric_data.values).sum()
        if inf_counts > 0:
            LOGGER.warning(f"ë¬´í•œëŒ€ ê°’ {inf_counts}ê°œê°€ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤")
            df = df.replace([np.inf, -np.inf], 0)
        
        return df
    
    def normalize_features(self, data: pd.DataFrame, symbol: str, is_training: bool = True) -> pd.DataFrame:
        """
        íŠ¹ì„± ì •ê·œí™” (í•„ìˆ˜ ì»¬ëŸ¼ ë³´ì¡´)
        """
        if data.empty:
            return data
        
        df = data.copy()
        
        # ì •ê·œí™”ì—ì„œ ì œì™¸í•  ì»¬ëŸ¼ (í•„ìˆ˜ ë³´ì¡´ + ë‚ ì§œ)
        exclude_cols = self.preserve_columns.copy()
        if 'date' in df.columns:
            exclude_cols.append('date')
        
        # ì •ê·œí™”í•  ì»¬ëŸ¼ (íŒŒìƒ ì§€í‘œë“¤ë§Œ)
        cols_to_normalize = [col for col in df.columns if col not in exclude_cols]
        
        if not cols_to_normalize:
            LOGGER.warning("ì •ê·œí™”í•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return df
        
        if is_training:
            # í•™ìŠµ ë°ì´í„°ì¸ ê²½ìš° ìƒˆ ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±
            if symbol not in self.scalers:
                self.scalers[symbol] = {}
            
            scaler = RobustScaler()
            self.scalers[symbol]['scaler'] = scaler
            normalized_data = scaler.fit_transform(df[cols_to_normalize])
        else:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì¸ ê²½ìš° ê¸°ì¡´ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©
            if symbol not in self.scalers or 'scaler' not in self.scalers[symbol]:
                LOGGER.warning(f"{symbol}ì— ëŒ€í•œ ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                scaler = RobustScaler()
                if symbol not in self.scalers:
                    self.scalers[symbol] = {}
                self.scalers[symbol]['scaler'] = scaler
                normalized_data = scaler.fit_transform(df[cols_to_normalize])
            else:
                scaler = self.scalers[symbol]['scaler']
                normalized_data = scaler.transform(df[cols_to_normalize])
        
        # ì •ê·œí™”ëœ ë°ì´í„°ë¡œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        normalized_df = df[exclude_cols].copy()  # í•„ìˆ˜ ì»¬ëŸ¼ ë¨¼ì € ë³µì‚¬
        
        # ì •ê·œí™”ëœ íŒŒìƒ ì§€í‘œë“¤ ì¶”ê°€
        normalized_features_df = pd.DataFrame(
            normalized_data, 
            index=df.index, 
            columns=cols_to_normalize
        )
        
        # í•©ì¹˜ê¸° (í•„ìˆ˜ ì»¬ëŸ¼ + ì •ê·œí™”ëœ íŒŒìƒ ì§€í‘œ)
        result_df = pd.concat([normalized_df, normalized_features_df], axis=1)
        
        # ğŸ”’ closeê°€ ë§ˆì§€ë§‰ ìœ„ì¹˜ì— ìˆëŠ”ì§€ ì¬í™•ì¸
        result_df = self._ensure_close_column_last(result_df)
        
        LOGGER.debug(f"ì •ê·œí™” ì™„ë£Œ: {len(exclude_cols)}ê°œ í•„ìˆ˜ ì»¬ëŸ¼ ë³´ì¡´, {len(cols_to_normalize)}ê°œ ì»¬ëŸ¼ ì •ê·œí™”")
        
        return result_df

    def split_data(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ë°ì´í„° ë¶„í• """
        if data.empty:
            LOGGER.warning("ë¹ˆ ë°ì´í„°ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
    
        # ë°ì´í„° ë¶„í•  ì¸ë±ìŠ¤ ê³„ì‚°
        train_idx = int(len(data) * TRAIN_RATIO)
        valid_idx = int(len(data) * (TRAIN_RATIO + VALID_RATIO))
    
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë¶„í•  (ë¯¸ë˜ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
        train = data.iloc[:train_idx].copy()
        valid = data.iloc[train_idx:valid_idx].copy()
        test = data.iloc[valid_idx:].copy()
    
        LOGGER.info(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ: í•™ìŠµ {len(train)}ê°œ, ê²€ì¦ {len(valid)}ê°œ, í…ŒìŠ¤íŠ¸ {len(test)}ê°œ")
        return train, valid, test
    
    def process_symbol_data(self, data: pd.DataFrame, symbol: str, use_windows: bool = False) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì‹¬ë³¼ ë°ì´í„°ì— ëŒ€í•œ ì „ì²´ ì „ì²˜ë¦¬ ê³¼ì • ìˆ˜í–‰ (íš¨ìœ¨ì ì¸ í”¼ì²˜ ì„ íƒ í¬í•¨)
        """
        LOGGER.info(f"{symbol} ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        
        # 1. ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = self.preprocess_data(data)
        if processed_data.empty:
            LOGGER.error(f"{symbol} ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            return {}
        
        # 2. í•µì‹¬ íŠ¹ì„± ì¶”ì¶œ (ê¸°ë³¸ OHLCV + í•µì‹¬ ì§€í‘œë§Œ)
        featured_data = self.extract_features(processed_data, symbol)
        
        # 3. ìŠ¤ë§ˆíŠ¸í•œ í”¼ì²˜ ì„ íƒ (í™œì„±í™”ëœ ê²½ìš°ë§Œ)
        if self.enable_feature_selection and self.feature_selector is not None:
            initial_count = len(featured_data.columns)
            LOGGER.info(f"{symbol} ìŠ¤ë§ˆíŠ¸ í”¼ì²˜ ì„ íƒ ì‹œì‘: {initial_count}ê°œ í”¼ì²˜")
            featured_data = self.feature_selector.fit_transform(featured_data, target_col='close')
            final_count = len(featured_data.columns)
            removed_count = initial_count - final_count
            LOGGER.info(f"{symbol} í”¼ì²˜ ì„ íƒ ì™„ë£Œ: {removed_count}ê°œ ì¤‘ë³µ ì œê±°, {final_count}ê°œ ìµœì¢… í”¼ì²˜")
        
        # 4. íŠ¹ì„± ì •ê·œí™”
        normalized_data = self.normalize_features(featured_data, symbol, is_training=True)
        
        # 5. ë°ì´í„° ë¶„í• 
        train, valid, test = self.split_data(normalized_data)
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            'processed_data': processed_data,
            'featured_data': featured_data,
            'normalized_data': normalized_data,
            'train': train,
            'valid': valid,
            'test': test
        }
        
        # ì„ íƒëœ í”¼ì²˜ ì •ë³´ ì¶”ê°€
        if self.feature_selector is not None and hasattr(self.feature_selector, 'selected_features'):
            result['selected_features'] = self.feature_selector.selected_features
            result['removed_features_info'] = getattr(self.feature_selector, 'removed_features_info', {})
        
        LOGGER.info(f"{symbol} íš¨ìœ¨ì ì¸ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
        return result
    
    def process_all_symbols(self, raw_data_dict: Dict[str, pd.DataFrame], use_windows: bool = False) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        ëª¨ë“  ì‹¬ë³¼ ë°ì´í„° ì²˜ë¦¬ (íš¨ìœ¨ì ì¸ í”¼ì²˜ ì„ íƒ í¬í•¨)
        """
        # í”¼ì²˜ ì„ íƒì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ í™œì„±í™”
        if not self.enable_feature_selection:
            LOGGER.info("âš ï¸  í”¼ì²˜ ì„ íƒì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ìë™ìœ¼ë¡œ í™œì„±í™”í•©ë‹ˆë‹¤.")
            self.enable_feature_selection = True
            self.feature_selector = FeatureSelector(
                correlation_threshold=0.85, 
                variance_threshold=0.0001,
                preserve_columns=self.preserve_columns,
                remove_duplicates_only=True
            )
        
        results = {}
        first_symbol = True

        LOGGER.info(f"ğŸ”„ {len(raw_data_dict)}ê°œ ì‹¬ë³¼ íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
        
        for i, (symbol, raw_data) in enumerate(raw_data_dict.items(), 1):
            LOGGER.info(f"[{i}/{len(raw_data_dict)}] {symbol} ì²˜ë¦¬ ì¤‘...")

            # 1. ì „ì²˜ë¦¬ ë° í•µì‹¬ íŠ¹ì„± ì¶”ì¶œ
            preprocessed = self.preprocess_data(raw_data)
            featured = self.extract_features(preprocessed, symbol)
            
            # 2. ìŠ¤ë§ˆíŠ¸í•œ í”¼ì²˜ ì„ íƒ
            if first_symbol:
                initial_features = len(featured.columns)
                LOGGER.info(f"   ğŸ“‹ ìŠ¤ë§ˆíŠ¸ í”¼ì²˜ ì„ íƒ ì¤‘: {initial_features}ê°œ í”¼ì²˜ì—ì„œ...")
                featured = self.feature_selector.fit_transform(featured, target_col='close')
                final_features = len(featured.columns)
                removed_features = initial_features - final_features
                first_symbol = False
                LOGGER.info(f"   âœ¨ í”¼ì²˜ ì„ íƒ ì™„ë£Œ: {removed_features}ê°œ ì¤‘ë³µ ì œê±°, {final_features}ê°œ ìµœì¢… í”¼ì²˜")
            else:
                LOGGER.info(f"   ğŸ”„ ë™ì¼í•œ í”¼ì²˜ ê¸°ì¤€ ì ìš©...")
                featured = self.feature_selector.transform(featured, target_col='close')

            # 3. ë°ì´í„° ë¶„í•  ë° ì •ê·œí™”
            train_data = featured.iloc[:int(len(featured)*TRAIN_RATIO)].copy()
            valid_data = featured.iloc[int(len(featured)*TRAIN_RATIO):int(len(featured)*(TRAIN_RATIO + VALID_RATIO))].copy()
            test_data = featured.iloc[int(len(featured)*(TRAIN_RATIO + VALID_RATIO)):].copy()

            train_data_norm = self.normalize_features(train_data, symbol=symbol, is_training=True)
            valid_data_norm = self.normalize_features(valid_data, symbol=symbol, is_training=False)
            test_data_norm = self.normalize_features(test_data, symbol=symbol, is_training=False)

            results[symbol] = {
                'processed_data': preprocessed,
                'featured_data': featured,
                'train': train_data_norm,
                'valid': valid_data_norm,
                'test': test_data_norm
            }

            LOGGER.info(f"   âœ… ì™„ë£Œ: train {len(train_data)}, valid {len(valid_data)}, test {len(test_data)}")

        # ìµœì¢… í”¼ì²˜ ìš”ì•½ ì¶œë ¥
        if self.feature_selector and hasattr(self.feature_selector, 'selected_features'):
            LOGGER.info(f"\nğŸ¯ ìµœì¢… íš¨ìœ¨ì ì¸ í”¼ì²˜ì…‹ ìš”ì•½:")
            LOGGER.info("=" * 60)
            
            removed_info = getattr(self.feature_selector, 'removed_features_info', {})
            
            LOGGER.info(f"ğŸ“Š ì œê±° ìš”ì•½:")
            total_removed = 0
            for category, removed_list in removed_info.items():
                if removed_list:
                    total_removed += len(removed_list)
                    LOGGER.info(f"   - {category.replace('_', ' ').title()}: {len(removed_list)}ê°œ")
            
            LOGGER.info(f"\nğŸ“ˆ ìµœì¢… ìœ íš¨ í”¼ì²˜ë“¤ ({len(self.feature_selector.selected_features)}ê°œ):")
            
            # í•„ìˆ˜ ë³´ì¡´ í”¼ì²˜ë“¤
            preserved_features = [f for f in self.feature_selector.selected_features if f in self.preserve_columns]
            derived_features = [f for f in self.feature_selector.selected_features if f not in self.preserve_columns]
            
            LOGGER.info(f"\nğŸ”’ í•„ìˆ˜ ë³´ì¡´ í”¼ì²˜ ({len(preserved_features)}ê°œ):")
            for i, feature in enumerate(preserved_features, 1):
                LOGGER.info(f"{i:2d}. {feature}")
            
            LOGGER.info(f"\nâš¡ ì„ ë³„ëœ íŒŒìƒ í”¼ì²˜ ({len(derived_features)}ê°œ):")
            for i, feature in enumerate(derived_features, 1):
                LOGGER.info(f"{i:2d}. {feature}")
            
            LOGGER.info("=" * 60)

        return results
    
    def save_processed_data(self, results: Dict[str, Dict[str, Any]], base_dir: Union[str, Path] = None) -> None:
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        """
        if base_dir is None:
            base_dir = DATA_DIR / "processed"
        
        create_directory(base_dir)
        
        for symbol, result in results.items():
            # ë°ì´í„°í”„ë ˆì„ë§Œ ì €ì¥
            for key in ['processed_data', 'featured_data', 'normalized_data', 'train', 'valid', 'test']:
                if key in result and isinstance(result[key], pd.DataFrame):
                    save_dir = base_dir / symbol
                    create_directory(save_dir)
                    file_path = save_dir / f"{key}.csv"
                    save_to_csv(result[key], file_path)
        
        LOGGER.info(f"íš¨ìœ¨ì ìœ¼ë¡œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {base_dir}")


if __name__ == "__main__":
    import argparse
    from src.data_collection.data_collector import DataCollector
    from src.config.config import config
    
    # 1. ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description="íš¨ìœ¨ì ì¸ ë°ì´í„° ì „ì²˜ë¦¬")
    parser.add_argument("--symbols", nargs="+", help="ì „ì²˜ë¦¬í•  ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸", default=config.trading_symbols)
    parser.add_argument("--correlation_threshold", type=float, default=0.85, help="ìƒê´€ê´€ê³„ ì„ê³„ê°’")
    parser.add_argument("--variance_threshold", type=float, default=0.0001, help="ë¶„ì‚° ì„ê³„ê°’")
    args = parser.parse_args()
    
    # 2. ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ í• ë‹¹
    symbols = args.symbols
    
    # ë°ì´í„° ìˆ˜ì§‘
    LOGGER.info("ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    collector = DataCollector(symbols=symbols)
    data = collector.load_all_data()
    
    # íš¨ìœ¨ì ì¸ ë°ì´í„° ì „ì²˜ë¦¬
    processor = DataProcessor(
        window_size=config.WINDOW_SIZE, 
        enable_feature_selection=True,
        # enable_news_sentiment=True,
        correlation_threshold=args.correlation_threshold,
        variance_threshold=args.variance_threshold
    )
    
    LOGGER.info("ğŸš€ íš¨ìœ¨ì ì¸ í”¼ì²˜ ìƒì„± + ìŠ¤ë§ˆíŠ¸í•œ ì¤‘ë³µ ì œê±° ì‹œì‘...")
    results = processor.process_all_symbols(data, use_windows=False)
    
    # ê²°ê³¼ ì €ì¥
    processor.save_processed_data(results)
    
    # ì²« ë²ˆì§¸ ì¢…ëª©ì˜ ê²°ê³¼ í™•ì¸
    if results:
        symbol = list(results.keys())[0]
        LOGGER.info(f"\nğŸ“Š {symbol} íš¨ìœ¨ì ì¸ ì „ì²˜ë¦¬ ê²°ê³¼:")
        LOGGER.info(f"   ì›ë³¸ ë°ì´í„° í¬ê¸°: {data[symbol].shape}")
        LOGGER.info(f"   ì „ì²˜ë¦¬ ë°ì´í„° í¬ê¸°: {results[symbol]['processed_data'].shape}")
        LOGGER.info(f"   íŠ¹ì„± ì¶”ì¶œ ë°ì´í„° í¬ê¸°: {results[symbol]['featured_data'].shape}")
        LOGGER.info(f"   í•™ìŠµ ë°ì´í„° í¬ê¸°: {results[symbol]['train'].shape}")
        LOGGER.info(f"   ê²€ì¦ ë°ì´í„° í¬ê¸°: {results[symbol]['valid'].shape}")
        LOGGER.info(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {results[symbol]['test'].shape}")

        LOGGER.info(f"\nâœ… ì´ {len(results)}ê°œ ì‹¬ë³¼ íš¨ìœ¨ì ì¸ ì²˜ë¦¬ ì™„ë£Œ!")
        LOGGER.info(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ìœ„ì¹˜: {config.DATA_DIR}/processed/")
        
        # íš¨ìœ¨ì ì¸ í”¼ì²˜ ì„ íƒ ê²°ê³¼ ìš”ì•½
        if processor.feature_selector and hasattr(processor.feature_selector, 'selected_features'):
            removed_info = getattr(processor.feature_selector, 'removed_features_info', {})
            total_removed = sum(len(removed_list) for removed_list in removed_info.values() if removed_list)
            
            LOGGER.info(f"\nğŸ¯ íš¨ìœ¨ì ì¸ í”¼ì²˜ ê´€ë¦¬ ê²°ê³¼:")
            original_count = len(processor.feature_selector.selected_features) + total_removed
            LOGGER.info(f"   ì›ë³¸ í”¼ì²˜: {original_count}ê°œ (OHLCV + íŒŒìƒì§€í‘œ)")
            LOGGER.info(f"   ì œê±°ëœ í”¼ì²˜: {total_removed}ê°œ (ì¤‘ë³µ/ë¬´ìš©)")
            LOGGER.info(f"   ìµœì¢… ìœ íš¨ í”¼ì²˜: {len(processor.feature_selector.selected_features)}ê°œ")
            LOGGER.info(f"   íš¨ìœ¨ì„± ê°œì„ : {total_removed/original_count:.1%} ì¤‘ë³µ ì œê±°")
            
            # close ì»¬ëŸ¼ì´ ë§ˆì§€ë§‰ì— ìˆëŠ”ì§€ í™•ì¸
            final_columns = list(results[symbol]['train'].columns)
            if final_columns[-1] == 'close':
                LOGGER.info(f"   âœ… close ì»¬ëŸ¼ ë§ˆì§€ë§‰ ìœ„ì¹˜ ë³´ì¥ë¨")
            else:
                LOGGER.warning(f"   âš ï¸ close ì»¬ëŸ¼ì´ ë§ˆì§€ë§‰ ìœ„ì¹˜ì— ì—†ìŒ: {final_columns[-1]}")
        
        LOGGER.info(f"\nğŸš€ SAC ê°•í™”í•™ìŠµì— ìµœì í™”ëœ íš¨ìœ¨ì ì¸ í”¼ì²˜ì…‹ ì¤€ë¹„ ì™„ë£Œ!")