"""
ê°œì„ ëœ í”¼ì²˜ ì„ íƒ ë° ì¤‘ë³µ ì œê±° ëª¨ë“ˆ
OHLCV ë³´ì¡´ + ìŠ¤ë§ˆíŠ¸í•œ ì¤‘ë³µ ì œê±°ì— íŠ¹í™”
"""
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class FeatureSelector:
    """
    ê°•í™”í•™ìŠµ ìµœì í™” í”¼ì²˜ ì„ íƒ í´ë˜ìŠ¤
    - í•„ìˆ˜ ì»¬ëŸ¼ ì ˆëŒ€ ë³´í˜¸
    - ë¶„ì‚° + ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    - close ì»¬ëŸ¼ ë§ˆì§€ë§‰ ìœ„ì¹˜ ë³´ì¥
    """
    
    def __init__(self, 
                 correlation_threshold: float = 0.85, 
                 variance_threshold: float = 0.0001,
                 preserve_columns: List[str] = None,
                 remove_duplicates_only: bool = True):
        """
        ì´ˆê¸°í™”
        
        Args:
            correlation_threshold: ìƒê´€ê´€ê³„ ì„ê³„ê°’ (85% ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ì œê±°)
            variance_threshold: ë¶„ì‚° ì„ê³„ê°’ (0.0001 ì´í•˜ë©´ ë³€ë™ì„± ì—†ëŠ” í”¼ì²˜ë¡œ ê°„ì£¼í•˜ì—¬ ì œê±°)
            preserve_columns: ì ˆëŒ€ ì œê±°í•˜ë©´ ì•ˆ ë˜ëŠ” í•„ìˆ˜ ì»¬ëŸ¼ë“¤
            remove_duplicates_only: Trueë©´ ì¤‘ë³µ ì œê±°ë§Œ, Falseë©´ ê°œìˆ˜ ì œí•œë„ ì ìš©
        """
        self.correlation_threshold = correlation_threshold
        self.variance_threshold = variance_threshold
        self.preserve_columns = preserve_columns or ['open', 'high', 'low', 'close', 'volume']
        self.remove_duplicates_only = remove_duplicates_only
        self.selected_features = None
        self.feature_importance_dict = {}
        self.removed_features_info = {}
        
        print(f"ğŸ”§ FeatureSelector ì´ˆê¸°í™”:")
        print(f"   ğŸ“ ìƒê´€ê´€ê³„ ì„ê³„ê°’: {correlation_threshold}")
        print(f"   ğŸ“ ë¶„ì‚° ì„ê³„ê°’: {variance_threshold}")
        print(f"   ğŸ”’ í•„ìˆ˜ ë³´ì¡´ ì»¬ëŸ¼: {self.preserve_columns}")
        print(f"   ğŸ¯ ì¤‘ë³µ ì œê±° ì „ìš©: {remove_duplicates_only}")
        
    def remove_low_variance_features(self, df: pd.DataFrame, target_col: str = 'close') -> pd.DataFrame:
        """
        ë¶„ì‚°ì´ ë‚®ì€ í”¼ì²˜ë“¤ ì œê±° (ê±°ì˜ ë³€í•˜ì§€ ì•ŠëŠ” í”¼ì²˜ë“¤)
        í•„ìˆ˜ ì»¬ëŸ¼ì€ ì ˆëŒ€ ì œê±°í•˜ì§€ ì•ŠìŒ
        """
        # ë³´í˜¸í•  ì»¬ëŸ¼ê³¼ ë¶„ì„í•  ì»¬ëŸ¼ ë¶„ë¦¬
        protected_cols = [col for col in self.preserve_columns if col in df.columns]
        if target_col not in protected_cols and target_col in df.columns:
            protected_cols.append(target_col)
        
        analyzable_cols = [col for col in df.columns if col not in protected_cols]
        low_variance_features = []
        
        print(f"\nğŸ” ë¶„ì‚° ë¶„ì„ ì‹œì‘:")
        print(f"   ğŸ”’ ë³´í˜¸ëœ ì»¬ëŸ¼: {len(protected_cols)}ê°œ")
        print(f"   ğŸ“Š ë¶„ì„ ëŒ€ìƒ ì»¬ëŸ¼: {len(analyzable_cols)}ê°œ")
        
        for col in analyzable_cols:
            if df[col].dtype in ['float64', 'int64']:
                variance = df[col].var()
                if variance < self.variance_threshold:
                    low_variance_features.append(col)
                    print(f"   âŒ {col}: ë¶„ì‚°={variance:.8f} (ì œê±°)")
        
        if low_variance_features:
            print(f"\nğŸ“‰ ë‚®ì€ ë¶„ì‚°ìœ¼ë¡œ ì œê±°í•  í”¼ì²˜: {len(low_variance_features)}ê°œ")
        else:
            print(f"\nâœ… ë¶„ì‚° ê¸°ì¤€ì„ í†µê³¼í•œ ëª¨ë“  í”¼ì²˜")
        
        # ë‚®ì€ ë¶„ì‚° í”¼ì²˜ ì œê±° (ë³´í˜¸ëœ ì»¬ëŸ¼ì€ ìœ ì§€)
        remaining_features = [col for col in analyzable_cols if col not in low_variance_features]
        final_cols = protected_cols + remaining_features
        
        self.removed_features_info['low_variance'] = low_variance_features
        print(f"ğŸ“Š ë¶„ì‚° í•„í„°ë§ í›„: {len(analyzable_cols)} â†’ {len(remaining_features)} í”¼ì²˜ (ë³´í˜¸ëœ {len(protected_cols)}ê°œ + ì„ ë³„ëœ {len(remaining_features)}ê°œ)")
        
        return df[final_cols]
    
    def remove_correlated_features(self, df: pd.DataFrame, target_col: str = 'close') -> pd.DataFrame:
        """
        ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ í”¼ì²˜ë“¤ ì œê±° (ì¤‘ë³µ ì •ë³´ ì œê±°)
        í•„ìˆ˜ ì»¬ëŸ¼ì€ ì ˆëŒ€ ì œê±°í•˜ì§€ ì•ŠìŒ
        """
        # ë³´í˜¸í•  ì»¬ëŸ¼ê³¼ ë¶„ì„í•  ì»¬ëŸ¼ ë¶„ë¦¬
        protected_cols = [col for col in self.preserve_columns if col in df.columns]
        if target_col not in protected_cols and target_col in df.columns:
            protected_cols.append(target_col)
        
        analyzable_cols = [col for col in df.columns if col not in protected_cols]
        
        if len(analyzable_cols) < 2:
            print(f"ğŸ” ìƒê´€ê´€ê³„ ë¶„ì„: ë¶„ì„ ê°€ëŠ¥í•œ ì»¬ëŸ¼ì´ {len(analyzable_cols)}ê°œë¿ì´ë¯€ë¡œ ê±´ë„ˆëœ€")
            self.removed_features_info['high_correlation'] = []
            return df
        
        print(f"\nğŸ” ìƒê´€ê´€ê³„ ë¶„ì„ ì‹œì‘:")
        print(f"   ğŸ”’ ë³´í˜¸ëœ ì»¬ëŸ¼: {len(protected_cols)}ê°œ")
        print(f"   ğŸ“Š ë¶„ì„ ëŒ€ìƒ ì»¬ëŸ¼: {len(analyzable_cols)}ê°œ")
        
        # ë¶„ì„ ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤ë§Œìœ¼ë¡œ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        feature_df = df[analyzable_cols].copy()
        corr_matrix = feature_df.corr().abs()
        
        # ìƒì‚¼ê° ë§¤íŠ¸ë¦­ìŠ¤ë§Œ ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
        upper_tri = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        # ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ í”¼ì²˜ ìŒë“¤ ì°¾ê¸°
        high_corr_pairs = []
        high_corr_features = set()
        
        for column in upper_tri.columns:
            corr_values = upper_tri[column].dropna()
            high_corr_items = corr_values[corr_values > self.correlation_threshold]
            
            for pair_feature, corr_value in high_corr_items.items():
                high_corr_pairs.append((column, pair_feature, corr_value))
                
                # ë‘ í”¼ì²˜ ì¤‘ íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ê°€ ë‚®ì€ ê²ƒì„ ì œê±° ëŒ€ìƒìœ¼ë¡œ ì„ íƒ
                if target_col in df.columns:
                    target_corr_col = abs(df[column].corr(df[target_col]))
                    target_corr_pair = abs(df[pair_feature].corr(df[target_col]))
                    
                    # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ê°€ ë‚®ì€ í”¼ì²˜ë¥¼ ì œê±° ëŒ€ìƒìœ¼ë¡œ
                    if target_corr_col < target_corr_pair:
                        high_corr_features.add(column)
                        print(f"   ğŸ”„ {column} vs {pair_feature}: ìƒê´€ê´€ê³„={corr_value:.3f} â†’ {column} ì œê±° (íƒ€ê²Ÿ ìƒê´€ê´€ê³„: {target_corr_col:.3f} < {target_corr_pair:.3f})")
                    else:
                        high_corr_features.add(pair_feature)
                        print(f"   ğŸ”„ {column} vs {pair_feature}: ìƒê´€ê´€ê³„={corr_value:.3f} â†’ {pair_feature} ì œê±° (íƒ€ê²Ÿ ìƒê´€ê´€ê³„: {target_corr_pair:.3f} < {target_corr_col:.3f})")
                else:
                    # íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í”¼ì²˜ë¥¼ ì œê±°
                    high_corr_features.add(column)
                    print(f"   ğŸ”„ {column} vs {pair_feature}: ìƒê´€ê´€ê³„={corr_value:.3f} â†’ {column} ì œê±°")
        
        if high_corr_pairs:
            print(f"\nğŸ”— ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ ìŒ: {len(high_corr_pairs)}ê°œ")
            print(f"âŒ ì¤‘ë³µìœ¼ë¡œ ì œê±°í•  í”¼ì²˜: {len(high_corr_features)}ê°œ")
        else:
            print(f"\nâœ… ìƒê´€ê´€ê³„ ê¸°ì¤€ì„ í†µê³¼í•œ ëª¨ë“  í”¼ì²˜")
        
        # ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ ì œê±° (ë³´í˜¸ëœ ì»¬ëŸ¼ì€ ìœ ì§€)
        remaining_features = [col for col in analyzable_cols if col not in high_corr_features]
        final_cols = protected_cols + remaining_features
        
        self.removed_features_info['high_correlation'] = list(high_corr_features)
        print(f"ğŸ“Š ìƒê´€ê´€ê³„ í•„í„°ë§ í›„: {len(analyzable_cols)} â†’ {len(remaining_features)} í”¼ì²˜")
        
        return df[final_cols]
    
    def _ensure_close_column_last(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        close ì»¬ëŸ¼ì´ ë§ˆì§€ë§‰ì— ìœ„ì¹˜í•˜ë„ë¡ ë³´ì¥
        trading_envì—ì„œ [-1] ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•¨
        """
        if 'close' in df.columns:
            # close ì»¬ëŸ¼ì„ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼
            other_columns = [col for col in df.columns if col != 'close']
            # closeë¥¼ ë§ˆì§€ë§‰ì— ë°°ì¹˜
            reordered_columns = other_columns + ['close']
            df = df[reordered_columns]
            print(f"ğŸ”’ close ì»¬ëŸ¼ì„ ë§ˆì§€ë§‰ ìœ„ì¹˜ë¡œ ì´ë™ ì™„ë£Œ")
        else:
            print(f"âš ï¸ close ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return df
    
    def fit_transform(self, df: pd.DataFrame, target_col: str = 'close') -> pd.DataFrame:
        """
        ìŠ¤ë§ˆíŠ¸í•œ ì¤‘ë³µ í”¼ì²˜ ì œê±° í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            
        Returns:
            ì¤‘ë³µì´ ì œê±°ëœ í”¼ì²˜ë“¤ë¡œ êµ¬ì„±ëœ ë°ì´í„°í”„ë ˆì„
        """
        print(f"\n" + "="*60)
        print(f"ğŸš€ ìŠ¤ë§ˆíŠ¸í•œ í”¼ì²˜ ì¤‘ë³µ ì œê±° ì‹œì‘")
        print(f"="*60)
        print(f"ğŸ“Š ì´ˆê¸° í”¼ì²˜ ìˆ˜: {len(df.columns)}ê°œ")
        print(f"ğŸ¯ íƒ€ê²Ÿ ì»¬ëŸ¼: {target_col}")
        print(f"ğŸ”’ í•„ìˆ˜ ë³´ì¡´ ì»¬ëŸ¼: {self.preserve_columns}")
        
        # 1ë‹¨ê³„: ë¶„ì‚°ì´ ë‚®ì€ í”¼ì²˜ ì œê±° (ê±°ì˜ ë³€í•˜ì§€ ì•ŠëŠ” í”¼ì²˜ë“¤)
        print(f"\nğŸ“ 1ë‹¨ê³„: ë¶„ì‚° ê¸°ë°˜ í•„í„°ë§ (ì„ê³„ê°’: {self.variance_threshold})")
        df_filtered = self.remove_low_variance_features(df, target_col)
        
        # 2ë‹¨ê³„: ìƒê´€ê´€ê³„ ë†’ì€ í”¼ì²˜ ì œê±° (ì¤‘ë³µ ì •ë³´ í”¼ì²˜ë“¤)
        print(f"\nğŸ”— 2ë‹¨ê³„: ìƒê´€ê´€ê³„ ê¸°ë°˜ í•„í„°ë§ (ì„ê³„ê°’: {self.correlation_threshold})")
        df_filtered = self.remove_correlated_features(df_filtered, target_col)
        
        # 3ë‹¨ê³„: close ì»¬ëŸ¼ì„ ë§ˆì§€ë§‰ ìœ„ì¹˜ë¡œ ì´ë™
        print(f"\nğŸ”’ 3ë‹¨ê³„: close ì»¬ëŸ¼ ìœ„ì¹˜ ìµœì í™”")
        df_filtered = self._ensure_close_column_last(df_filtered)
        
        # ìµœì¢… ì„ íƒëœ í”¼ì²˜ë“¤
        selected_features = [col for col in df_filtered.columns if col != target_col]
        if target_col in df.columns:
            final_features = selected_features + [target_col]
        else:
            final_features = selected_features
        
        self.selected_features = selected_features
        
        # ê²°ê³¼ ìš”ì•½
        initial_count = len(df.columns)
        final_count = len(df_filtered.columns)
        removed_count = initial_count - final_count
        efficiency = (removed_count / initial_count) * 100 if initial_count > 0 else 0
        
        print(f"\n" + "="*60)
        print(f"âœ¨ ìŠ¤ë§ˆíŠ¸í•œ í”¼ì²˜ ì¤‘ë³µ ì œê±° ì™„ë£Œ!")
        print(f"="*60)
        print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   ğŸ¯ ì´ˆê¸° í”¼ì²˜: {initial_count}ê°œ")
        print(f"   âœ… ìµœì¢… í”¼ì²˜: {final_count}ê°œ")
        print(f"   âŒ ì œê±°ëœ í”¼ì²˜: {removed_count}ê°œ")
        print(f"   ğŸ“ˆ íš¨ìœ¨ì„± ê°œì„ : {efficiency:.1f}% ì¤‘ë³µ ì œê±°")
        
        # ì œê±°ëœ í”¼ì²˜ë“¤ ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
        total_removed = 0
        for category, removed_list in self.removed_features_info.items():
            if removed_list:
                total_removed += len(removed_list)
                print(f"   - {category.replace('_', ' ').title()}: {len(removed_list)}ê°œ")
        
        # ìµœì¢… í”¼ì²˜ êµ¬ì„± ìš”ì•½
        protected_features = [f for f in self.selected_features if f in self.preserve_columns]
        derived_features = [f for f in self.selected_features if f not in self.preserve_columns]
        
        print(f"\nğŸ“‹ ìµœì¢… í”¼ì²˜ êµ¬ì„±:")
        print(f"   ğŸ”’ í•„ìˆ˜ ë³´ì¡´: {len(protected_features)}ê°œ")
        print(f"   âš¡ ì„ ë³„ëœ íŒŒìƒ: {len(derived_features)}ê°œ")
        print(f"   ğŸ¯ íƒ€ê²Ÿ ({target_col}): ë§ˆì§€ë§‰ ìœ„ì¹˜ ë³´ì¥")
        print(f"="*60)
        
        return df_filtered
    
    def transform(self, df: pd.DataFrame, target_col: str = 'close') -> pd.DataFrame:
        """
        í•™ìŠµëœ í”¼ì²˜ ì„ íƒ ê¸°ì¤€ìœ¼ë¡œ ìƒˆ ë°ì´í„° ë³€í™˜
        
        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            
        Returns:
            ì„ íƒëœ í”¼ì²˜ë“¤ë¡œ êµ¬ì„±ëœ ë°ì´í„°í”„ë ˆì„
        """
        if self.selected_features is None:
            raise ValueError("ë¨¼ì € fit_transformì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        print(f"ğŸ”„ í•™ìŠµëœ í”¼ì²˜ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        
        # ì„ íƒëœ í”¼ì²˜ë“¤ë§Œ ì¶”ì¶œ
        available_features = [f for f in self.selected_features if f in df.columns]
        missing_features = [f for f in self.selected_features if f not in df.columns]
        
        if missing_features:
            print(f"âš ï¸ ëˆ„ë½ëœ í”¼ì²˜ë“¤: {missing_features}")
        
        if target_col in df.columns:
            final_features = available_features + [target_col]
        else:
            final_features = available_features
        
        result_df = df[final_features]
        
        # close ì»¬ëŸ¼ ìœ„ì¹˜ ì¬í™•ì¸
        result_df = self._ensure_close_column_last(result_df)
        
        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(available_features)}ê°œ í”¼ì²˜ + {target_col}")
        
        return result_df


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_improved_feature_selection():
    """
    ê°œì„ ëœ í”¼ì²˜ ì„ íƒ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
    """
    print("ğŸ§ª ê°œì„ ëœ í”¼ì²˜ ì„ íƒ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*50)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ìƒí™© ì‹œë®¬ë ˆì´ì…˜)
    np.random.seed(42)
    n_samples = 1000
    
    # ê¸°ë³¸ OHLCV ë°ì´í„°
    close_prices = 100 + np.cumsum(np.random.randn(n_samples) * 0.02)
    open_prices = close_prices + np.random.randn(n_samples) * 0.1
    high_prices = np.maximum(open_prices, close_prices) + np.abs(np.random.randn(n_samples) * 0.2)
    low_prices = np.minimum(open_prices, close_prices) - np.abs(np.random.randn(n_samples) * 0.2)
    volumes = np.random.randint(1000, 10000, n_samples)
    
    # ìƒ˜í”Œ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì˜ë„ì ìœ¼ë¡œ ì¤‘ë³µ í”¼ì²˜ë“¤ í¬í•¨)
    sample_data = pd.DataFrame({
        'open': open_prices,
        'high': high_prices,
        'low': low_prices,
        'close': close_prices,
        'volume': volumes,
        
        # ìœ ì‚¬í•œ ì´ë™í‰ê· ë“¤ (ë†’ì€ ìƒê´€ê´€ê³„)
        'sma_5': close_prices + np.random.randn(n_samples) * 0.01,
        'sma_10': close_prices + np.random.randn(n_samples) * 0.02,
        'sma_20': close_prices + np.random.randn(n_samples) * 0.03,
        'ema_5': close_prices + np.random.randn(n_samples) * 0.01,  # sma_5ì™€ ë†’ì€ ìƒê´€ê´€ê³„
        'ema_10': close_prices + np.random.randn(n_samples) * 0.02,  # sma_10ê³¼ ë†’ì€ ìƒê´€ê´€ê³„
        
        # RSI ê³„ì—´ (ìœ ì‚¬í•œ ì§€í‘œë“¤)
        'rsi_14': 50 + np.random.randn(n_samples) * 20,
        'rsi_30': 50 + np.random.randn(n_samples) * 18,  # rsi_14ì™€ ìƒê´€ê´€ê³„ ìˆìŒ
        
        # ê±°ì˜ ë³€í•˜ì§€ ì•ŠëŠ” í”¼ì²˜ (ë‚®ì€ ë¶„ì‚°)
        'constant_feature': np.full(n_samples, 1.0) + np.random.randn(n_samples) * 0.00001,
        'almost_constant': np.full(n_samples, 0.5) + np.random.randn(n_samples) * 0.00005,
        
        # ìœ ìš©í•œ ë…ë¦½ì ì¸ í”¼ì²˜ë“¤
        'macd': np.random.randn(n_samples) * 0.5,
        'bb_width': np.random.randn(n_samples) * 0.3,
        'atr_14': np.abs(np.random.randn(n_samples) * 0.4),
        'volume_change': np.random.randn(n_samples) * 0.3,
        'returns_1d': np.random.randn(n_samples) * 0.02,
        
        # ì¶”ê°€ ì¤‘ë³µ í›„ë³´ë“¤
        'price_volatility': (high_prices - low_prices) / close_prices,
        'high_low_ratio': high_prices / low_prices,
        'similar_to_volatility': (high_prices - low_prices) / close_prices + np.random.randn(n_samples) * 0.001,  # price_volatilityì™€ ê±°ì˜ ë™ì¼
    })
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {sample_data.shape}")
    print(f"ğŸ”’ í•„ìˆ˜ ë³´ì¡´ ì»¬ëŸ¼: ['open', 'high', 'low', 'close', 'volume']")
    print(f"âš¡ íŒŒìƒ ì§€í‘œ: {len(sample_data.columns) - 5}ê°œ")
    
    # í”¼ì²˜ ì„ íƒ ì‹¤í–‰
    print(f"\nğŸš€ ê°œì„ ëœ í”¼ì²˜ ì„ íƒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    selector = FeatureSelector(
        correlation_threshold=0.85,
        variance_threshold=0.0001,
        preserve_columns=['open', 'high', 'low', 'close', 'volume'],
        remove_duplicates_only=True
    )
    
    selected_data = selector.fit_transform(sample_data, target_col='close')
    
    print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    print(f"   ğŸ“Š ì›ë³¸ ë°ì´í„° shape: {sample_data.shape}")
    print(f"   âœ… ì„ íƒëœ ë°ì´í„° shape: {selected_data.shape}")
    print(f"   ğŸ”’ í•„ìˆ˜ ì»¬ëŸ¼ ë³´ì¡´ í™•ì¸: {all(col in selected_data.columns for col in ['open', 'high', 'low', 'close', 'volume'])}")
    print(f"   ğŸ¯ close ì»¬ëŸ¼ ë§ˆì§€ë§‰ ìœ„ì¹˜: {selected_data.columns[-1] == 'close'}")
    
    print(f"\nğŸ“‹ ìµœì¢… ì„ íƒëœ ì»¬ëŸ¼ë“¤:")
    for i, col in enumerate(selected_data.columns, 1):
        marker = "ğŸ”’" if col in ['open', 'high', 'low', 'close', 'volume'] else "âš¡"
        print(f"   {i:2d}. {marker} {col}")
    
    print(f"\nğŸ§ª ê°œì„ ëœ í”¼ì²˜ ì„ íƒ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    return selected_data


def demonstrate_feature_selection_benefits():
    """
    í”¼ì²˜ ì„ íƒì˜ íš¨ê³¼ë¥¼ ì‹œì—°í•˜ëŠ” í•¨ìˆ˜
    """
    print("\n" + "="*60)
    print("ğŸ’¡ í”¼ì²˜ ì„ íƒì˜ íš¨ê³¼ ì‹œì—°")
    print("="*60)
    
    # ê·¹ë‹¨ì ì¸ ì¤‘ë³µ ë°ì´í„° ìƒì„±
    np.random.seed(123)
    n_samples = 500
    base_signal = np.cumsum(np.random.randn(n_samples) * 0.1)
    
    demo_data = pd.DataFrame({
        # í•„ìˆ˜ OHLCV
        'open': base_signal + np.random.randn(n_samples) * 0.01,
        'high': base_signal + 1 + np.random.randn(n_samples) * 0.01,
        'low': base_signal - 1 + np.random.randn(n_samples) * 0.01,
        'close': base_signal,
        'volume': np.random.randint(1000, 5000, n_samples),
        
        # ì˜ë„ì ì¸ ì¤‘ë³µ í”¼ì²˜ë“¤
        'feature_1': base_signal + np.random.randn(n_samples) * 0.001,  # closeì™€ ê±°ì˜ ë™ì¼
        'feature_2': base_signal + np.random.randn(n_samples) * 0.002,  # closeì™€ ê±°ì˜ ë™ì¼
        'feature_3': base_signal * 1.1 + np.random.randn(n_samples) * 0.001,  # closeì™€ ë†’ì€ ìƒê´€ê´€ê³„
        'feature_4': base_signal * 0.9 + np.random.randn(n_samples) * 0.001,  # closeì™€ ë†’ì€ ìƒê´€ê´€ê³„
        
        # ì™„ì „íˆ ë¬´ìš©í•œ í”¼ì²˜ë“¤
        'constant_noise': np.full(n_samples, 100),  # ì™„ì „ ìƒìˆ˜
        'tiny_variance': np.full(n_samples, 50) + np.random.randn(n_samples) * 0.00001,  # ê·¹ë„ë¡œ ë‚®ì€ ë¶„ì‚°
        
        # ìœ ìš©í•œ ë…ë¦½ì ì¸ í”¼ì²˜
        'useful_feature_1': np.random.randn(n_samples),  # ë…ë¦½ì ì¸ ì‹ í˜¸
        'useful_feature_2': np.sin(np.arange(n_samples) / 50),  # ì£¼ê¸°ì  íŒ¨í„´
    })
    
    print(f"ğŸ“Š ì‹œì—°ìš© ë°ì´í„°: {demo_data.shape}")
    
    # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ì¶œë ¥ (ì£¼ìš” í”¼ì²˜ë“¤ë§Œ)
    main_features = ['close', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'useful_feature_1', 'useful_feature_2']
    corr_matrix = demo_data[main_features].corr()
    
    print(f"\nğŸ”— ì£¼ìš” í”¼ì²˜ë“¤ ê°„ ìƒê´€ê´€ê³„:")
    print(f"   close vs feature_1: {corr_matrix.loc['close', 'feature_1']:.3f}")
    print(f"   close vs feature_2: {corr_matrix.loc['close', 'feature_2']:.3f}")
    print(f"   close vs feature_3: {corr_matrix.loc['close', 'feature_3']:.3f}")
    print(f"   close vs useful_feature_1: {corr_matrix.loc['close', 'useful_feature_1']:.3f}")
    
    # ë¶„ì‚° ì •ë³´
    print(f"\nğŸ“ ë¶„ì‚° ì •ë³´:")
    print(f"   constant_noise: {demo_data['constant_noise'].var():.8f}")
    print(f"   tiny_variance: {demo_data['tiny_variance'].var():.8f}")
    print(f"   close: {demo_data['close'].var():.8f}")
    print(f"   useful_feature_1: {demo_data['useful_feature_1'].var():.8f}")
    
    # í”¼ì²˜ ì„ íƒ ì ìš©
    selector = FeatureSelector(
        correlation_threshold=0.90,  # 90% ì´ìƒ ìƒê´€ê´€ê³„
        variance_threshold=0.0001,
        preserve_columns=['open', 'high', 'low', 'close', 'volume'],
        remove_duplicates_only=True
    )
    
    result = selector.fit_transform(demo_data, target_col='close')
    
    print(f"\nâœ¨ ì‹œì—° ê²°ê³¼:")
    print(f"   ğŸ“Š ì²˜ë¦¬ ì „: {demo_data.shape[1]}ê°œ í”¼ì²˜")
    print(f"   âœ… ì²˜ë¦¬ í›„: {result.shape[1]}ê°œ í”¼ì²˜")
    print(f"   ğŸ¯ íš¨ìœ¨ì„±: {((demo_data.shape[1] - result.shape[1]) / demo_data.shape[1] * 100):.1f}% ì¤‘ë³µ ì œê±°")
    
    return result


if __name__ == "__main__":
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_result = test_improved_feature_selection()
    
    # íš¨ê³¼ ì‹œì—°
    demo_result = demonstrate_feature_selection_benefits()
    
    print("\n" + "="*60)
    print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)
    print("âœ… ê°œì„ ëœ FeatureSelector ê²€ì¦ ì™„ë£Œ")
    print("âœ… OHLCV ë³´ì¡´ í™•ì¸")
    print("âœ… close ì»¬ëŸ¼ ë§ˆì§€ë§‰ ìœ„ì¹˜ ë³´ì¥")
    print("âœ… ìŠ¤ë§ˆíŠ¸í•œ ì¤‘ë³µ ì œê±° ë™ì‘ í™•ì¸")
    print("="*60)