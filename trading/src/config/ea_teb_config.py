"""
ë°±í…ŒìŠ¤íŒ…/í•™ìŠµ ì „ìš© ì„¤ì • íŒŒì¼ (í†µí•© ê°„ì†Œí™” ë²„ì „)
SAC íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ - í•™ìŠµ, í‰ê°€, ë°±í…ŒìŠ¤íŠ¸ìš©
config.pyë¥¼ ìƒì†í•˜ì—¬ ê³ ìœ  ê¸°ëŠ¥ë§Œ ì¶”ê°€
"""
import os
import re

# config.pyì—ì„œ ëª¨ë“  ê¸°ë³¸ ì„¤ì • import
from src.config.config import *

# ========================================
# ea_teb_config.py ê³ ìœ  ì„¤ì •ë§Œ ì •ì˜
# ========================================

# ëª¨ë¸ íŒ¨í„´ ì •ì˜ (ea_teb_config.py ê³ ìœ )
MODEL_PATTERNS = {
    'mlp': r'final_sac_model_([A-Z]+)_(\d{8}_\d{6})',
    'cnn': r'final_cnn_sac_model_(\d{8}_\d{6})',
    'lstm': r'final_lstm_sac_model_(\d{8}_\d{6})',
    'transformer': r'final_transformer_sac_model_(\d{8}_\d{6})'
}

# MySQL ì„¤ì • (ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ìš©) - ea_teb_config.py ê³ ìœ 
MYSQL_HOST = os.getenv("MYSQL_HOST", "192.168.40.199")
MYSQL_DATABASE = os.getenv("MYSQL_DATABASE", "trading")
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "mysecretpassword")
MYSQL_PORT = int(os.getenv("MYSQL_PORT", "3306"))
SAVE_TO_DATABASE = True
SKIP_DB_ON_ERROR = True

# ========================================
# ëª¨ë¸ë³„ ìµœì í™”ëœ ê¸°ë³¸ê°’ ì„¤ì • (ê°œì„  ë²„ì „)
# ========================================

# ëª¨ë¸ë³„ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (íŠ¸ë ˆì´ë”© ìµœì í™”ë¨)
MODEL_DEFAULTS = {
    'mlp': {
        'learning_rate_factor': 1.0,        # ê¸°ë³¸ í•™ìŠµë¥  ìœ ì§€
        'dropout_rate': 0.1,                # ê°€ë²¼ìš´ ì •ê·œí™”
        'alpha_init': ALPHA_INIT,           # ê¸°ë³¸ ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
        'gradient_clip': GRADIENT_CLIP,     # ê¸°ë³¸ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        'buffer_type': 'sequential',        # ì‹œê³„ì—´ íŠ¹ì„± í™œìš©
        'sequence_length': 32,              # ì ë‹¹í•œ ì‹œí€€ìŠ¤ ê¸¸ì´
        
        # MLP ì „ìš© ìµœì í™”
        'hidden_layers': 3,                 # ì€ë‹‰ì¸µ ê°œìˆ˜
        'activation': 'relu',               # í™œì„±í™” í•¨ìˆ˜
        'batch_norm': False,                # MLPëŠ” ë°°ì¹˜ì •ê·œí™” ìƒëµ
        'weight_decay': 1e-4,               # ê°€ë²¼ìš´ L2 ì •ê·œí™”
    },
    'cnn': {
        'learning_rate_factor': 0.25,       # ë”ìš± ë³´ìˆ˜ì ì¸ í•™ìŠµë¥  (ê¸°ì¡´ 0.3 â†’ 0.25)
        'dropout_rate': 0.2,                # ì¡°ê¸ˆ ë” ê°•í•œ ì •ê·œí™” (0.15 â†’ 0.2)
        'alpha_init': ALPHA_INIT * 0.8,     # ë” ë‚®ì€ ì´ˆê¸° ì—”íŠ¸ë¡œí”¼ (ì•ˆì •ì„±)
        'gradient_clip': GRADIENT_CLIP * 0.5, # ë” ê°•í•œ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        'buffer_type': 'sequential',        # ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
        'sequence_length': 64,              # ê¸´ ì‹œí€€ìŠ¤ë¡œ íŒ¨í„´ í¬ì°©
        
        # CNN ì „ìš© ìµœì í™”
        'conv_layers': 3,                   # ì»¨ë³¼ë£¨ì…˜ ì¸µ ê°œìˆ˜
        'kernel_sizes': [3, 5, 7],          # ë‹¤ì–‘í•œ ì»¤ë„ í¬ê¸°
        'pooling_type': 'adaptive',         # ì ì‘í˜• í’€ë§
        'batch_norm': True,                 # ë°°ì¹˜ ì •ê·œí™” í•„ìˆ˜
        'weight_decay': 1e-5,               # ë” ê°•í•œ L2 ì •ê·œí™”
        'early_stopping_patience': 50,     # ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬
    },
    'lstm': {
        'learning_rate_factor': 0.6,        # ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì¡°ì • (0.8 â†’ 0.6)
        'dropout_rate': 0.25,               # ë” ê°•í•œ ë“œë¡­ì•„ì›ƒ (ê³¼ì í•© ë°©ì§€)
        'alpha_init': ALPHA_INIT * 1.2,     # ë” ë†’ì€ ì´ˆê¸° ì—”íŠ¸ë¡œí”¼ (íƒìƒ‰)
        'gradient_clip': GRADIENT_CLIP,     # ê¸°ë³¸ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        'buffer_type': 'sequential',        # ìˆœì°¨ì  ê²½í—˜ í•„ìˆ˜
        'sequence_length': 48,              # ë” ê¸´ ì‹œí€€ìŠ¤ (32 â†’ 48)
        
        # LSTM ì „ìš© ìµœì í™”
        'lstm_layers': 2,                   # LSTM ì¸µ ê°œìˆ˜
        'lstm_hidden_dim': 128,             # LSTM ì€ë‹‰ ì°¨ì›
        'bidirectional': False,             # ë‹¨ë°©í–¥ (ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© í˜¸í™˜)
        'lstm_dropout': 0.15,               # LSTM ë‚´ë¶€ ë“œë¡­ì•„ì›ƒ
        'forget_bias': 1.0,                 # ë§ê° ê²Œì´íŠ¸ í¸í–¥
        'weight_decay': 2e-5,               # ì¤‘ê°„ ì •ë„ L2 ì •ê·œí™”
        'early_stopping_patience': 75,     # ë” ê¸´ ì¸ë‚´ì‹¬ (LSTMì€ ëŠë¦¬ê²Œ í•™ìŠµ)
    }
}

# ğŸ†• ì„±ëŠ¥ ê¸°ë°˜ ë™ì  ì¡°ì • ì„¤ì •
PERFORMANCE_ADAPTIVE_SETTINGS = {
    'enable_adaptive_lr': True,             # ì„±ëŠ¥ ê¸°ë°˜ í•™ìŠµë¥  ì¡°ì •
    'performance_window': 50,               # ì„±ëŠ¥ í‰ê°€ ìœˆë„ìš°
    'lr_boost_threshold': 0.1,              # í•™ìŠµë¥  ì¦ê°€ ì„ê³„ê°’ (ì¢‹ì€ ì„±ëŠ¥)
    'lr_reduce_threshold': -0.05,           # í•™ìŠµë¥  ê°ì†Œ ì„ê³„ê°’ (ë‚˜ìœ ì„±ëŠ¥)
    'max_lr_multiplier': 2.0,               # ìµœëŒ€ í•™ìŠµë¥  ë°°ìˆ˜
    'min_lr_multiplier': 0.1,               # ìµœì†Œ í•™ìŠµë¥  ë°°ìˆ˜
}

# ğŸ†• ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê¸°ë°˜ ì„¤ì •
RISK_MANAGEMENT_SETTINGS = {
    'conservative_mode': False,             # ë³´ìˆ˜ì  ëª¨ë“œ (ë¦¬ìŠ¤í¬ ìµœì†Œí™”)
    'aggressive_mode': False,               # ê³µê²©ì  ëª¨ë“œ (ìˆ˜ìµ ìµœëŒ€í™”)
    
    # ë³´ìˆ˜ì  ëª¨ë“œ ì„¤ì • (conservative_mode=True ì‹œ ì ìš©)
    'conservative_lr_factor': 0.5,          # í•™ìŠµë¥  ì ˆë°˜
    'conservative_dropout_boost': 1.5,      # ë“œë¡­ì•„ì›ƒ 1.5ë°°
    'conservative_alpha_factor': 0.7,       # ë” ë‚®ì€ ì—”íŠ¸ë¡œí”¼
    
    # ê³µê²©ì  ëª¨ë“œ ì„¤ì • (aggressive_mode=True ì‹œ ì ìš©)
    'aggressive_lr_factor': 1.5,            # í•™ìŠµë¥  1.5ë°°
    'aggressive_dropout_factor': 0.8,       # ë“œë¡­ì•„ì›ƒ 20% ê°ì†Œ
    'aggressive_alpha_factor': 1.3,         # ë” ë†’ì€ ì—”íŠ¸ë¡œí”¼
}

# ğŸ†• ê³µí†µ ê³ ê¸‰ ì„¤ì • (ì—…ë°ì´íŠ¸ë¨)
COMMON_ADVANCED_SETTINGS = {
    'use_gradient_clipping': True,
    'use_lr_scheduling': True, 
    'target_update_method': 'soft',
    'adaptive_alpha': True,
    'use_batch_norm': True,                 # ëª¨ë¸ë³„ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥
    'weight_decay': 1e-5,                   # ê¸°ë³¸ê°’ (ëª¨ë¸ë³„ë¡œ ì˜¤ë²„ë¼ì´ë“œ)
    
    # ğŸ†• ìƒˆë¡œìš´ ê³ ê¸‰ ê¸°ëŠ¥
    'use_layer_norm': False,                # ë ˆì´ì–´ ì •ê·œí™” (ì‹¤í—˜ì )
    'use_spectral_norm': False,             # ìŠ¤í™íŠ¸ëŸ´ ì •ê·œí™” (ì•ˆì •ì„±)
    'warmup_steps': 1000,                   # í•™ìŠµë¥  ì›Œë°ì—…
    'cosine_annealing': False,              # ì½”ì‚¬ì¸ ì–´ë‹ë§ ìŠ¤ì¼€ì¤„ëŸ¬
    'polyak_averaging': True,               # í´ë¦¬ì•… í‰ê· í™”
}



# ========================================
# ëª¨ë¸ë³„ ì—”íŠ¸ë¡œí”¼ ì„¤ì •
# ========================================

MODEL_ENTROPY_CONFIGS = {
    'mlp': {
        'use_adaptive': False,
        'fixed_target': -1.0,
        'description': 'MLP ê³ ì • ì—”íŠ¸ë¡œí”¼ (-1.0)'
    },
    'cnn': {
        'use_adaptive': True,
        'entropy_range': (-1.2, -0.5),
        'initial_target': -0.8,
        'description': 'CNN ì ì‘ì  ì—”íŠ¸ë¡œí”¼ (-1.2 ~ -0.5)'
    },
    'lstm': {
        'use_adaptive': True,
        'entropy_range': (-1.0, -0.8),
        'initial_target': -0.9,
        'description': 'LSTM ì ì‘ì  ì—”íŠ¸ë¡œí”¼ (-1.0 ~ -0.8)'
    }
}

# ========================================
# ea_teb_config.py ê³ ìœ  í•¨ìˆ˜ë“¤
# ========================================

def find_latest_model_by_pattern(model_type: str, symbol: str = None, base_dir: str = None) -> str:
    """
    íŒ¨í„´ë³„ë¡œ ìµœì‹  ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°

    Args:
        model_type: ëª¨ë¸ íƒ€ì… ('mlp', 'cnn', 'lstm', 'transformer')
        symbol: ì£¼ì‹ ì‹¬ë³¼ (MLP ëª¨ë¸ì˜ ê²½ìš° í•„ìˆ˜)
        base_dir: ëª¨ë¸ ê²€ìƒ‰ ê¸°ë³¸ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: models/)

    Returns:
        str: ìµœì‹  ëª¨ë¸ì˜ ì „ì²´ ê²½ë¡œ, ì—†ìœ¼ë©´ None
    """
    if base_dir is None:
        base_dir = os.path.join(ROOT_DIR, "models")

    if not os.path.exists(base_dir):
        print(f"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
        return None

    pattern = MODEL_PATTERNS.get(model_type.lower())
    if not pattern:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
        return None

    # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  í´ë” ìŠ¤ìº”
    model_dirs = []

    try:
        for item in os.listdir(base_dir):
            item_path = os.path.join(base_dir, item)
            if os.path.isdir(item_path):
                match = re.match(pattern, item)
                if match:
                    # MLP ëª¨ë¸ì˜ ê²½ìš° ì‹¬ë³¼ ë§¤ì¹­ í™•ì¸
                    if model_type.lower() == 'mlp':
                        if symbol and match.group(1) == symbol:
                            timestamp = match.group(2)
                            model_dirs.append((timestamp, item_path))
                    else:
                        # CNN, LSTM ë“±ì€ ì‹¬ë³¼ ë¬´ê´€
                        timestamp = match.group(1)
                        model_dirs.append((timestamp, item_path))

        if not model_dirs:
            if model_type.lower() == 'mlp' and symbol:
                print(f"âŒ {symbol}ì— ëŒ€í•œ {model_type.upper()} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"âŒ {model_type.upper()} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìµœì‹  ëª¨ë¸ ì„ íƒ
        model_dirs.sort(key=lambda x: x[0], reverse=True)
        latest_model_path = model_dirs[0][1]

        # config.pth íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        config_path = os.path.join(latest_model_path, 'config.pth')
        if os.path.exists(config_path):
            print(f"âœ… ìµœì‹  {model_type.upper()} ëª¨ë¸ ë°œê²¬: {os.path.basename(latest_model_path)}")
            return latest_model_path
        else:
            print(f"âš ï¸  config.pth íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {latest_model_path}")
            return None

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def find_model_for_backtest(symbol: str = None, model_type: str = None, model_path: str = None) -> dict:
    """
    ë°±í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ê²€ìƒ‰

    Args:
        symbol: ì£¼ì‹ ì‹¬ë³¼ (ì„ íƒì‚¬í•­)
        model_type: ëª¨ë¸ íƒ€ì… (ì„ íƒì‚¬í•­: 'mlp', 'cnn', 'lstm')
        model_path: ì§ì ‘ ì§€ì •í•œ ëª¨ë¸ ê²½ë¡œ (ì„ íƒì‚¬í•­)

    Returns:
        dict: {'path': str, 'type': str, 'symbol': str} ë˜ëŠ” None
    """
    # 1. ì§ì ‘ ê²½ë¡œ ì§€ì •ëœ ê²½ìš°
    if model_path:
        if os.path.exists(os.path.join(model_path, 'config.pth')):
            # ê²½ë¡œì—ì„œ ëª¨ë¸ íƒ€ì… ì¶”ë¡ 
            model_name = os.path.basename(model_path)
            detected_type = 'mlp'  # ê¸°ë³¸ê°’
            detected_symbol = None

            for mtype, pattern in MODEL_PATTERNS.items():
                if re.match(pattern, model_name):
                    detected_type = mtype
                    if mtype == 'mlp':
                        match = re.match(pattern, model_name)
                        if match:
                            detected_symbol = match.group(1)
                    break

            return {
                'path': model_path,
                'type': detected_type,
                'symbol': detected_symbol or symbol
            }

    # 2. ìë™ ê²€ìƒ‰
    if model_type and symbol:
        # íŠ¹ì • íƒ€ì…ê³¼ ì‹¬ë³¼ë¡œ ê²€ìƒ‰
        latest_model = find_latest_model_by_pattern(model_type, symbol)
        if latest_model:
            return {
                'path': latest_model,
                'type': model_type,
                'symbol': symbol
            }
    elif model_type:
        # íƒ€ì…ë§Œ ì§€ì • (CNN, LSTM ë“±)
        latest_model = find_latest_model_by_pattern(model_type)
        if latest_model:
            return {
                'path': latest_model,
                'type': model_type,
                'symbol': symbol
            }
    elif symbol:
        # ì‹¬ë³¼ë§Œ ì§€ì • (MLP ìš°ì„  ê²€ìƒ‰)
        for mtype in ['mlp', 'cnn', 'lstm']:
            latest_model = find_latest_model_by_pattern(mtype, symbol if mtype == 'mlp' else None)
            if latest_model:
                return {
                    'path': latest_model,
                    'type': mtype,
                    'symbol': symbol
                }

    return None

def list_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡")
    print("=" * 60)

    base_dir = os.path.join(ROOT_DIR, "models")
    if not os.path.exists(base_dir):
        print("âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    models_found = []

    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path) and os.path.exists(os.path.join(item_path, 'config.pth')):
            # ëª¨ë¸ íƒ€ì…ê³¼ ì‹¬ë³¼ ê°ì§€
            model_info = {'name': item, 'path': item_path, 'type': 'unknown', 'symbol': None}

            for mtype, pattern in MODEL_PATTERNS.items():
                match = re.match(pattern, item)
                if match:
                    model_info['type'] = mtype
                    if mtype == 'mlp' and len(match.groups()) >= 2:
                        model_info['symbol'] = match.group(1)
                    break

            models_found.append(model_info)

    # íƒ€ì…ë³„ë¡œ ì •ë ¬
    models_found.sort(key=lambda x: (x['type'], x['symbol'] or '', x['name']))

    for model in models_found:
        symbol_info = f" ({model['symbol']})" if model['symbol'] else ""
        print(f"âœ… {model['type'].upper()}{symbol_info}: {model['name']}")

    print(f"\nâœ… ì´ {len(models_found)}ê°œ ëª¨ë¸ ë°œê²¬")
    print("=" * 60 + "\n")

def get_model_config(model_type: str, symbol: str = None):
    """ë™ì  ëª¨ë¸ ì„¤ì • ë°˜í™˜ (ê³µí†µ ì„¤ì • í¬í•¨)"""
    defaults = MODEL_DEFAULTS.get(model_type.lower(), MODEL_DEFAULTS['mlp'])
    
    config = {
        # ğŸ”¥ í•™ìŠµë¥  (ëª¨ë¸ë³„ ì°¨ë³„í™”)
        'actor_lr': LEARNING_RATE_ACTOR * defaults['learning_rate_factor'],
        'critic_lr': LEARNING_RATE_CRITIC * defaults['learning_rate_factor'],
        'alpha_lr': LEARNING_RATE_ALPHA * defaults['learning_rate_factor'],
        
        # ğŸ”¥ ëª¨ë¸ë³„ ì„¤ì •
        'alpha_init': defaults['alpha_init'],
        'dropout_rate': defaults['dropout_rate'],
        'gradient_clip': defaults['gradient_clip'],
        'buffer_type': defaults['buffer_type'],
        'sequence_length': defaults['sequence_length'],
        'model_type': model_type.lower(),
        
        # ğŸ†• ê³µí†µ ê³ ê¸‰ ì„¤ì • ì¶”ê°€ (LSTM, CNNì—ì„œ ëª¨ë‘ ë„ì›€)
        **COMMON_ADVANCED_SETTINGS
    }
    
    # ğŸ†• ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì„¤ì •
    if model_type.lower() == 'cnn':
        config.update({
            'use_batch_norm': True,
            'conv_layers': 3,
            'adaptive_pooling': True,
        })
    elif model_type.lower() == 'lstm':
        config.update({
            'bidirectional': False,
            'lstm_layers': 2,
            'lstm_hidden_dim': 128,
        })
    
    return config

def get_entropy_config_for_model(model_type: str, action_dim: int = 1):
    """
    ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì—”íŠ¸ë¡œí”¼ ì„¤ì • ë°˜í™˜
    
    Args:
        model_type: 'mlp', 'cnn', 'lstm' ì¤‘ í•˜ë‚˜
        action_dim: í–‰ë™ ì°¨ì› (ê¸°ë³¸ê°’ 1)
        
    Returns:
        dict: ì—”íŠ¸ë¡œí”¼ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    config = MODEL_ENTROPY_CONFIGS.get(model_type.lower(), MODEL_ENTROPY_CONFIGS['mlp'])
    
    # action_dimì— ë”°ë¥¸ ìŠ¤ì¼€ì¼ë§ ì ìš©
    if config['use_adaptive']:
        entropy_min, entropy_max = config['entropy_range']
        scaled_config = {
            'use_adaptive': True,
            'entropy_range': (entropy_min * action_dim, entropy_max * action_dim),
            'initial_target': config['initial_target'] * action_dim,
            'description': f"{config['description']} (action_dim={action_dim})"
        }
    else:
        scaled_config = {
            'use_adaptive': False,
            'fixed_target': config['fixed_target'] * action_dim,
            'description': f"{config['description']} (action_dim={action_dim})"
        }
    
    return scaled_config

def get_model_config(model_type: str, symbol: str = None, **overrides):
    """ë™ì  ëª¨ë¸ ì„¤ì • ë°˜í™˜ (ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë° ì„±ëŠ¥ ì ì‘ í¬í•¨)"""
    defaults = MODEL_DEFAULTS.get(model_type.lower(), MODEL_DEFAULTS['mlp'])
    
    # ê¸°ë³¸ ì„¤ì • êµ¬ì„±
    config = {
        # ğŸ”¥ í•™ìŠµë¥  (ëª¨ë¸ë³„ ì°¨ë³„í™”)
        'actor_lr': LEARNING_RATE_ACTOR * defaults['learning_rate_factor'],
        'critic_lr': LEARNING_RATE_CRITIC * defaults['learning_rate_factor'],
        'alpha_lr': LEARNING_RATE_ALPHA * defaults['learning_rate_factor'],
        
        # ğŸ”¥ ëª¨ë¸ë³„ ì„¤ì •
        'alpha_init': defaults['alpha_init'],
        'dropout_rate': defaults['dropout_rate'],
        'gradient_clip': defaults['gradient_clip'],
        'buffer_type': defaults['buffer_type'],
        'sequence_length': defaults['sequence_length'],
        'model_type': model_type.lower(),
        
        # ğŸ†• ê³µí†µ ê³ ê¸‰ ì„¤ì • ì¶”ê°€
        **COMMON_ADVANCED_SETTINGS
    }
    
    # ğŸ†• ë¦¬ìŠ¤í¬ ê´€ë¦¬ ëª¨ë“œ ì ìš©
    if RISK_MANAGEMENT_SETTINGS['conservative_mode']:
        config['actor_lr'] *= RISK_MANAGEMENT_SETTINGS['conservative_lr_factor']
        config['critic_lr'] *= RISK_MANAGEMENT_SETTINGS['conservative_lr_factor']
        config['dropout_rate'] *= RISK_MANAGEMENT_SETTINGS['conservative_dropout_boost']
        config['alpha_init'] *= RISK_MANAGEMENT_SETTINGS['conservative_alpha_factor']
        LOGGER.info("ğŸ›¡ï¸ ë³´ìˆ˜ì  ëª¨ë“œ í™œì„±í™”")
        
    elif RISK_MANAGEMENT_SETTINGS['aggressive_mode']:
        config['actor_lr'] *= RISK_MANAGEMENT_SETTINGS['aggressive_lr_factor']
        config['critic_lr'] *= RISK_MANAGEMENT_SETTINGS['aggressive_lr_factor']  
        config['dropout_rate'] *= RISK_MANAGEMENT_SETTINGS['aggressive_dropout_factor']
        config['alpha_init'] *= RISK_MANAGEMENT_SETTINGS['aggressive_alpha_factor']
        LOGGER.info("ğŸš€ ê³µê²©ì  ëª¨ë“œ í™œì„±í™”")
    
    # ğŸ†• ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì„¤ì • (ì—…ë°ì´íŠ¸ë¨)
    if model_type.lower() == 'cnn':
        config.update({
            'conv_layers': defaults['conv_layers'],
            'kernel_sizes': defaults['kernel_sizes'],
            'pooling_type': defaults['pooling_type'],
            'batch_norm': defaults['batch_norm'],
            'weight_decay': defaults['weight_decay'],
            'early_stopping_patience': defaults['early_stopping_patience'],
        })
    elif model_type.lower() == 'lstm':
        config.update({
            'lstm_layers': defaults['lstm_layers'],
            'lstm_hidden_dim': defaults['lstm_hidden_dim'],
            'bidirectional': defaults['bidirectional'],
            'lstm_dropout': defaults['lstm_dropout'],
            'forget_bias': defaults['forget_bias'],
            'weight_decay': defaults['weight_decay'],
            'early_stopping_patience': defaults['early_stopping_patience'],
        })
    elif model_type.lower() == 'mlp':
        config.update({
            'hidden_layers': defaults['hidden_layers'],
            'activation': defaults['activation'],
            'batch_norm': defaults['batch_norm'],
            'weight_decay': defaults['weight_decay'],
        })
    
    # ğŸ†• ì‚¬ìš©ì ì˜¤ë²„ë¼ì´ë“œ ì ìš©
    config.update(overrides)
    
    return config

# ğŸ†• ê°„í¸ ì„¤ì • í•¨ìˆ˜ë“¤
def enable_conservative_mode():
    """ë³´ìˆ˜ì  ëª¨ë“œ í™œì„±í™” (ë¦¬ìŠ¤í¬ ìµœì†Œí™”)"""
    RISK_MANAGEMENT_SETTINGS['conservative_mode'] = True
    RISK_MANAGEMENT_SETTINGS['aggressive_mode'] = False
    LOGGER.info("ğŸ›¡ï¸ ë³´ìˆ˜ì  ëª¨ë“œ í™œì„±í™”ë¨")

def enable_aggressive_mode():
    """ê³µê²©ì  ëª¨ë“œ í™œì„±í™” (ìˆ˜ìµ ìµœëŒ€í™”)"""
    RISK_MANAGEMENT_SETTINGS['aggressive_mode'] = True
    RISK_MANAGEMENT_SETTINGS['conservative_mode'] = False
    LOGGER.info("ğŸš€ ê³µê²©ì  ëª¨ë“œ í™œì„±í™”ë¨")

def reset_to_default_mode():
    """ê¸°ë³¸ ëª¨ë“œë¡œ ë¦¬ì…‹"""
    RISK_MANAGEMENT_SETTINGS['conservative_mode'] = False
    RISK_MANAGEMENT_SETTINGS['aggressive_mode'] = False
    LOGGER.info("âš–ï¸ ê¸°ë³¸ ëª¨ë“œë¡œ ë¦¬ì…‹ë¨")

def adjust_learning_rates(multiplier: float):
    """ëª¨ë“  ëª¨ë¸ì˜ í•™ìŠµë¥  ì¼ê´„ ì¡°ì •"""
    for model_type in MODEL_DEFAULTS:
        MODEL_DEFAULTS[model_type]['learning_rate_factor'] *= multiplier
    LOGGER.info(f"ğŸ“ˆ ëª¨ë“  í•™ìŠµë¥ ì´ {multiplier}ë°° ì¡°ì •ë¨")

# ========================================
# Config í´ë˜ìŠ¤ í™•ì¥ 
# ========================================

class EaTebConfig(Config):
    """
    config.Configë¥¼ ìƒì†ë°›ì•„ ë°±í…ŒìŠ¤íŒ…/í•™ìŠµ ì „ìš© ì„¤ì • ì¶”ê°€ 
    """
    
    # MySQL ì„¤ì • ì¶”ê°€
    MYSQL_HOST = MYSQL_HOST
    MYSQL_DATABASE = MYSQL_DATABASE
    MYSQL_USER = MYSQL_USER
    MYSQL_PASSWORD = MYSQL_PASSWORD
    MYSQL_PORT = MYSQL_PORT
    SAVE_TO_DATABASE = SAVE_TO_DATABASE
    SKIP_DB_ON_ERROR = SKIP_DB_ON_ERROR
    
    # ëª¨ë¸ íŒ¨í„´ ì¶”ê°€
    MODEL_PATTERNS = MODEL_PATTERNS
    MODEL_DEFAULTS = MODEL_DEFAULTS
    MODEL_ENTROPY_CONFIGS = MODEL_ENTROPY_CONFIGS
    
    # í—¬í¼ ë©”ì„œë“œ ì¶”ê°€
    @staticmethod
    def find_latest_model_by_pattern(model_type: str, symbol: str = None, base_dir: str = None):
        return find_latest_model_by_pattern(model_type, symbol, base_dir)
    
    @staticmethod
    def find_model_for_backtest(symbol: str = None, model_type: str = None, model_path: str = None):
        return find_model_for_backtest(symbol, model_type, model_path)
    
    @staticmethod
    def list_available_models():
        return list_available_models()

    @staticmethod
    def get_model_config(model_type: str, symbol: str = None):
        return get_model_config(model_type, symbol)
    
    @staticmethod
    def get_entropy_config_for_model(model_type: str, action_dim: int = 1):
        return get_entropy_config_for_model(model_type, action_dim)

# ê¸€ë¡œë²Œ config ì¸ìŠ¤í„´ìŠ¤ (í™•ì¥ëœ ë²„ì „)
config = EaTebConfig()

# âœ… ì„¤ì • í™•ì¸ ë©”ì‹œì§€ (ê°„ì†Œí™”ë¨)
print("=" * 60)
print("âœ… ë°±í…ŒìŠ¤íŒ…/í•™ìŠµ ì „ìš© ì„¤ì • ë¡œë“œë¨ (í†µí•© ê°„ì†Œí™” ë²„ì „)")
print("=" * 60)
print(f"âœ… ëŒ€ìƒ ì‹¬ë³¼: {TARGET_SYMBOLS}")
print(f"âœ… ì—°ì‚° ì¥ì¹˜: {DEVICE}")
print(f"âœ… ëª¨ë¸ ë””ë ‰í† ë¦¬: {MODELS_DIR}")
print(f"âœ… MySQL ì—°ê²°: {MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}")
print(f"âœ… ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print(f"âœ… í•™ìŠµ ì—í”¼ì†Œë“œ: {NUM_EPISODES}")
print(f"âœ… ì§€ì› ëª¨ë¸ íƒ€ì…: {list(MODEL_PATTERNS.keys())}")
print("=" * 60)
print("ğŸ”§ ëª¨ë¸ë³„ ê¸°ë³¸ ì„¤ì •:")