"""
ê°•í™”í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë”© í™˜ê²½ ëª¨ë“ˆ (ë¡œê·¸ ìµœì í™” ë²„ì „)
debug_level íŒŒë¼ë¯¸í„° ì¶”ê°€: 'INFO' (í•„ìˆ˜ ë¡œê·¸), 'DEBUG' (ìƒì„¸ ë¡œê·¸)
"""
import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Union, Optional, Any
import gym
from gym import spaces
from src.data_collection.data_collector import DataCollector
from decimal import Decimal, ROUND_DOWN, getcontext

# ì €ì¥ ê²½ë¡œ í´ë” ìƒì„±
save_dir = './results'
os.makedirs(save_dir, exist_ok=True)

from src.config.ea_teb_config import (
    INITIAL_BALANCE,
    MAX_TRADING_UNITS,
    TRANSACTION_FEE_PERCENT,
    WINDOW_SIZE,
    LOGGER
)

class TradingEnvironment:
    """
    ê°•í™”í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë”© í™˜ê²½ í´ë˜ìŠ¤ (ë¡œê·¸ ìµœì í™” ë²„ì „)
    """
    
    def __init__(
        self,
        data: pd.DataFrame,
        raw_data: pd.DataFrame = None,
        window_size: int = WINDOW_SIZE,
        initial_balance: float = INITIAL_BALANCE,
        max_trading_units: int = MAX_TRADING_UNITS,
        transaction_fee_percent: float = TRANSACTION_FEE_PERCENT,
        symbol: str = None,
        train_data: bool = True,
        reward_mode: str = 'combined',
        detailed_logging: bool = False,  # ê¸°ë³¸ê°’ì„ Falseë¡œ ë³€ê²½
        debug_level: str = 'INFO'  # ìƒˆë¡œ ì¶”ê°€: INFO, DEBUG
    ):
        """
        TradingEnvironment í´ë˜ìŠ¤ ì´ˆê¸°í™”
        """
        # ğŸ“Š ê¸°ë³¸ ë°ì´í„° ì„¤ì •
        self.data = data
        if raw_data is not None:
            self.raw_data = raw_data
        else:
            LOGGER.warning("raw_dataê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.raw_data = data
        
        # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
        self.debug_level = debug_level
        self.detailed_logging = detailed_logging
        
        # ì´ˆê¸°í™” ë¡œê·¸ (í•„ìˆ˜ ì •ë³´ë§Œ)
        LOGGER.info(f"í™˜ê²½ ì´ˆê¸°í™”: {symbol} | ë°ì´í„°: {data.shape} | ëª¨ë“œ: {reward_mode}")
        
        # â° íƒ€ì„ìŠ¤íƒ¬í”„ ì´ˆê¸°í™”
        if hasattr(self.raw_data.index, 'values'):
            self.timestamps = self.raw_data.index.values
        else:
            self.timestamps = np.array([None] * len(self.raw_data))

        # ğŸ¯ í™˜ê²½ ì„¤ì • íŒŒë¼ë¯¸í„°
        self.window_size = window_size
        self.initial_balance = initial_balance
        self.max_trading_units = max_trading_units
        self.transaction_fee_percent = transaction_fee_percent
        self.symbol = symbol if symbol else "UNKNOWN"
        self.train_data = train_data
        self.reward_mode = reward_mode
        
        # ğŸ“ ë°ì´í„° ê´€ë ¨ ë³€ìˆ˜
        self.feature_dim = data.shape[1]
        self.data_length = len(data)
        
        # ğŸ’° í™˜ê²½ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
        self._init_state_variables()
        
        # ğŸ® í–‰ë™ ê³µê°„: [-1.0, 1.0] ë²”ìœ„ì˜ ì—°ì†ì ì¸ ê°’
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)
        
        # ğŸ‘€ ê´€ì¸¡ ê³µê°„: ê°€ê²© ë°ì´í„° + í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ
        self.observation_space = spaces.Dict({
            'market_data': spaces.Box(
                low=-np.inf, high=np.inf, shape=(self.window_size, self.feature_dim), dtype=np.float32
            ),
            'portfolio_state': spaces.Box(
                low=0, high=np.inf, shape=(2,), dtype=np.float32
            )
        })
        
        # ğŸ” ì›ë³¸ ë°ì´í„° ê²€ì¦
        if 'close' not in self.raw_data.columns:
            LOGGER.warning("raw_dataì— 'close' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ì»¬ëŸ¼ì„ ì¢…ê°€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # âœ… ìœ íš¨ì„± ê²€ì‚¬
        valid_modes = ['combined', 'trade_only', 'market_only', 'separated']
        if reward_mode not in valid_modes:
            raise ValueError(f"reward_modeëŠ” {valid_modes} ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
        
    def _init_state_variables(self):
        """ğŸ’° í™˜ê²½ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”"""
        self.current_step = 0
        self.balance = self.initial_balance
        self.shares_held = 0
        self.cost_basis = 0
        self.total_shares_purchased = 0
        self.total_shares_sold = 0
        self.total_sales_value = 0
        self.total_commission = 0
        self.position = "í™€ë“œ"
        self.trade_executed = False 
        self.previous_shares_held = 0
        self.invalid_sell_penalty = False
        
        # ğŸ“ˆ ì—í”¼ì†Œë“œ íˆìŠ¤í† ë¦¬
        self.states_history = []
        self.actions_history = []
        self.rewards_history = []
        self.portfolio_values_history = []
        self.trade_effects_history = []
        self.market_effects_history = []
        self.combined_effects_history = []
    
    def reset(self) -> Dict[str, np.ndarray]:
        """
        ğŸ”„ í™˜ê²½ ì´ˆê¸°í™” (ë¡œê·¸ ê°„ì†Œí™”)
        """
        if self.debug_level == 'DEBUG':
            LOGGER.debug(f"í™˜ê²½ ë¦¬ì…‹: {self.symbol} | ì´ˆê¸°ì”ê³ : ${self.initial_balance:.2f}")
        
        self._init_state_variables()
        return self._get_observation()

    def _get_current_price(self) -> float:
        """
        ğŸ’µ í˜„ì¬ ì£¼ê°€ ë°˜í™˜ (ë¡œê·¸ ìµœì í™”)
        """
        # ğŸ” ì¸ë±ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬
        if self.current_step >= len(self.raw_data):
            last_valid_index = len(self.raw_data) - 1
            if self.debug_level == 'DEBUG':
                LOGGER.warning(f"ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼: {self.current_step} >= {len(self.raw_data)}")
            
            if 'close' in self.raw_data.columns:
                price = float(self.raw_data.iloc[last_valid_index]['close'])
            else:
                price = float(self.raw_data.iloc[last_valid_index][-1])
        else:
            # ğŸ“Š ì •ìƒì ì¸ ê²½ìš°
            if 'close' in self.raw_data.columns:
                price = float(self.raw_data.iloc[self.current_step]['close'])
            else:
                price = float(self.raw_data.iloc[self.current_step][-1])
        
        # ğŸš¨ ê°€ê²©ì´ 0 ì´í•˜ì¸ ê²½ìš° ì²˜ë¦¬
        if price <= 0:
            LOGGER.warning(f"í˜„ì¬ ê°€ê²©ì´ 0 ì´í•˜ì…ë‹ˆë‹¤: {price}")
            price = 0.01
            
        return price

    def _execute_trade_action(self, action: float) -> None:
        """
        ğŸ”„ ê±°ë˜ í–‰ë™ ì‹¤í–‰ (ë¡œê·¸ ê°„ì†Œí™”)
        """
        current_price = self._get_current_price()
        
        if current_price <= 0:
            LOGGER.warning(f"í˜„ì¬ ê°€ê²©ì´ 0 ì´í•˜ì…ë‹ˆë‹¤: {current_price}")
            return
        
        action_value = action[0] if isinstance(action, np.ndarray) else action
        
        # ğŸ® ê¸°ë³¸ ìƒíƒœ ì„¤ì •
        self.trade_executed = False
        self.position = "í™€ë“œ"
        self.invalid_sell_penalty = False
        
        # ğŸ’ª ìµœì†Œ í–‰ë™ ì„ê³„ê°’ ì²˜ë¦¬
        min_action_threshold = 0.1
        if 0 < abs(action_value) < min_action_threshold:
            action_value = min_action_threshold * (1 if action_value > 0 else -1)
        
        getcontext().prec = 10
        
        # ê±°ë˜ ì „ ìƒíƒœ (DEBUG ë ˆë²¨ì—ì„œë§Œ ìƒì„¸ ë¡œê·¸)
        if self.debug_level == 'DEBUG':
            old_portfolio = self.balance + self.shares_held * current_price
            LOGGER.debug(f"ê±°ë˜ì‹¤í–‰: Step {self.current_step} | í–‰ë™: {action_value:.4f} | "
                        f"ê°€ê²©: ${current_price:.2f} | í¬íŠ¸í´ë¦¬ì˜¤: ${old_portfolio:.2f}")
        
        if action_value > 0:  # ğŸ’° ë§¤ìˆ˜
            self._execute_buy(action_value, current_price)
        elif action_value < 0:  # ğŸ’¸ ë§¤ë„
            self._execute_sell(action_value, current_price)
        
        # ê±°ë˜ ê²°ê³¼ ë¡œê·¸ (ê°„ì†Œí™”)
        if self.trade_executed:
            new_portfolio = self.balance + self.shares_held * current_price
            if self.debug_level == 'DEBUG':
                LOGGER.debug(f"ê±°ë˜ì™„ë£Œ: {self.position} | ì”ê³ : ${self.balance:.2f} | "
                            f"ë³´ìœ ëŸ‰: {self.shares_held:.4f} | í¬íŠ¸í´ë¦¬ì˜¤: ${new_portfolio:.2f}")

    def _execute_buy(self, action_value: float, current_price: float) -> None:
        """ğŸ’° ë§¤ìˆ˜ ì‹¤í–‰ (ë¡œê·¸ ê°„ì†Œí™”)"""
        max_affordable = self.balance / (current_price * (1 + self.transaction_fee_percent))
        shares_to_buy = min(max_affordable, self.max_trading_units * action_value)
        
        if shares_to_buy > 0:
            shares_to_buy = float(Decimal(str(shares_to_buy)).quantize(Decimal('0.0001'), rounding=ROUND_DOWN))
            
            buy_cost = shares_to_buy * current_price
            commission = buy_cost * self.transaction_fee_percent
            total_cost = buy_cost + commission
            
            if self.balance >= total_cost:
                # âœ… ë§¤ìˆ˜ ì‹¤í–‰
                self.balance -= total_cost
                self.shares_held += shares_to_buy
                self.total_shares_purchased += shares_to_buy
                self.total_commission += commission
                
                self.trade_executed = True
                self.position = "ë§¤ìˆ˜"
                
                # ğŸ“ˆ í‰ê·  ë§¤ìˆ˜ ë‹¨ê°€ ì—…ë°ì´íŠ¸
                if self.shares_held > 0:
                    self.cost_basis = ((self.cost_basis * (self.shares_held - shares_to_buy)) + buy_cost) / self.shares_held
                
                if self.debug_level == 'DEBUG':
                    LOGGER.debug(f"ë§¤ìˆ˜ì„±ê³µ: {shares_to_buy:.4f}ì£¼ @ ${current_price:.2f} | ë¹„ìš©: ${total_cost:.2f}")
            else:
                if self.debug_level == 'DEBUG':
                    LOGGER.debug(f"ë§¤ìˆ˜ì‹¤íŒ¨: ì”ê³ ë¶€ì¡± (í•„ìš”: ${total_cost:.2f}, ë³´ìœ : ${self.balance:.2f})")

    def _execute_sell(self, action_value: float, current_price: float) -> None:
        """ğŸ’¸ ë§¤ë„ ì‹¤í–‰ (ë¡œê·¸ ê°„ì†Œí™”)"""
        shares_to_sell = min(self.shares_held, self.max_trading_units * abs(action_value))
        
        if shares_to_sell > 0:
            shares_to_sell = float(Decimal(str(shares_to_sell)).quantize(Decimal('0.0001'), rounding=ROUND_DOWN))
            
            if shares_to_sell <= 0:
                return
            
            gross_sell_value = shares_to_sell * current_price
            commission = gross_sell_value * self.transaction_fee_percent
            net_value = gross_sell_value - commission
            
            # âœ… ë§¤ë„ ì‹¤í–‰
            self.balance += net_value
            self.shares_held -= shares_to_sell
            self.total_shares_sold += shares_to_sell
            self.total_sales_value += gross_sell_value
            self.total_commission += commission
            
            self.trade_executed = True
            self.position = "ë§¤ë„"
            
            if self.debug_level == 'DEBUG':
                LOGGER.debug(f"ë§¤ë„ì„±ê³µ: {shares_to_sell:.4f}ì£¼ @ ${current_price:.2f} | ìˆ˜ìµ: ${net_value:.2f}")
        else:
            if self.debug_level == 'DEBUG':
                LOGGER.debug("ë§¤ë„ì‹¤íŒ¨: ë³´ìœ ëŸ‰ ë¶€ì¡±")

    def _get_portfolio_value(self) -> float:
        """
        ğŸ’ í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚° (ë¡œê·¸ ìµœì í™”)
        """
        current_price = self._get_current_price()
        stock_value = self.shares_held * current_price
        total_value = self.balance + stock_value
        
        # ğŸš¨ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ìµœì†Œ ê°€ì¹˜ ë³´ì¥
        if total_value <= 0:
            return max(self.balance, 1.0)
            
        return total_value

    def _get_observation(self) -> Dict[str, np.ndarray]:
        """
        ğŸ‘€ í˜„ì¬ ê´€ì¸¡ê°’ ë°˜í™˜
        """
        # ğŸ“Š ìœˆë„ìš° í¬ê¸°ë§Œí¼ì˜ ì •ê·œí™”ëœ ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        start_idx = max(0, self.current_step - self.window_size + 1)
        end_idx = self.current_step + 1
        
        # ğŸ”§ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš° íŒ¨ë”© ì²˜ë¦¬
        if start_idx == 0 and end_idx - start_idx < self.window_size:
            market_data = np.zeros((self.window_size, self.feature_dim), dtype=np.float32)
            actual_data = self.data.iloc[start_idx:end_idx].values
            market_data[-len(actual_data):] = actual_data
        else:
            market_data = self.data.iloc[start_idx:end_idx].values
            if len(market_data) < self.window_size:
                padding = np.zeros((self.window_size - len(market_data), self.feature_dim), dtype=np.float32)
                market_data = np.vstack([padding, market_data])
        
        # ğŸ’ í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ê³„ì‚°
        portfolio_value = self._get_portfolio_value()
        stock_value = self.shares_held * self._get_current_price()
        
        portfolio_state = np.array([
            self.balance / portfolio_value,  # í˜„ê¸ˆ ë¹„ìœ¨
            stock_value / portfolio_value    # ì£¼ì‹ ë¹„ìœ¨
        ], dtype=np.float32)
        
        # ğŸ“Š ê´€ì¸¡ê°’ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        observation = {
            'market_data': market_data.astype(np.float32),
            'portfolio_state': portfolio_state
        }
        
        self.states_history.append(observation)
        return observation

    def _calculate_reward(self, prev_portfolio_value: float, portfolio_after_trade: float, 
                         current_portfolio_value: float) -> Union[float, Dict[str, float]]:
        """
        ğŸ ë³´ìƒ ê³„ì‚° (ë¡œê·¸ ê°„ì†Œí™”)
        """
        # ğŸ” ì•ˆì „í•œ ì´ì „ í¬íŠ¸í´ë¦¬ì˜¤ ê°’ ì„¤ì •
        if prev_portfolio_value <= 0:
            prev_portfolio_value = max(self.balance, 1.0)
        
        # ğŸ“Š ê° íš¨ê³¼ ê³„ì‚°
        if prev_portfolio_value > 0:
            trade_effect = (portfolio_after_trade - prev_portfolio_value) / prev_portfolio_value
        else:
            trade_effect = 0
            
        if portfolio_after_trade > 0:
            market_effect = (current_portfolio_value - portfolio_after_trade) / portfolio_after_trade
        else:
            market_effect = 0
            
        if prev_portfolio_value > 0:
            total_effect = (current_portfolio_value - prev_portfolio_value) / prev_portfolio_value
        else:
            total_effect = 0
        
        # ë³´ìƒ ëª¨ë“œì— ë”°ë¥¸ ê³„ì‚° í´ë¦¬í•‘
        if self.reward_mode == 'trade_only':
            reward = np.clip(trade_effect * 100, -2, 2)
        elif self.reward_mode == 'market_only':
            reward = np.clip(market_effect * 50, -5, 5)
        elif self.reward_mode == 'separated':
            trade_reward = np.clip(trade_effect * 100, -2, 2)
            market_reward = np.clip(market_effect * 50, -5, 5)
            reward = {
                'trade': trade_reward,
                'market': market_reward,
                'total': trade_reward + market_reward
            }
        else:  # 'combined'
            reward = np.clip(total_effect * 50, -5, 5)
        
        # ğŸš¨ ì˜ëª»ëœ ë§¤ë„ í˜ë„í‹°
        if self.invalid_sell_penalty:
            penalty = -1.0
            if isinstance(reward, dict):
                reward['trade'] += penalty
                reward['total'] += penalty
            else:
                reward += penalty
            if self.debug_level == 'DEBUG':
                LOGGER.debug(f"ì˜ëª»ëœ ë§¤ë„ í˜ë„í‹°: {penalty}")
        
        # ğŸ“Š ìƒì„¸ ë¡œê¹… (DEBUG ë ˆë²¨ì—ì„œë§Œ)
        if self.debug_level == 'DEBUG':
            if isinstance(reward, dict):
                LOGGER.debug(f"ë³´ìƒê³„ì‚°: ê±°ë˜={reward['trade']:.6f}, ì‹œì¥={reward['market']:.6f}, ì´={reward['total']:.6f}")
            else:
                LOGGER.debug(f"ë³´ìƒê³„ì‚° ({self.reward_mode}): {reward:.6f}")
        
        return reward

    def step(self, action: float) -> Tuple[Dict[str, np.ndarray], Union[float, Dict[str, float]], bool, Dict[str, Any]]:
        """
        ğŸš€ í™˜ê²½ì—ì„œ í•œ ìŠ¤í… ì§„í–‰ (ë¡œê·¸ ëŒ€í­ ê°„ì†Œí™”)
        """
        # ğŸ”„ ì´ˆê¸°í™”
        self.invalid_sell_penalty = False
        self.actions_history.append(action)
        
        # ğŸ“Š 1. ê±°ë˜ ì „ ìƒíƒœ ì €ì¥
        current_price_before = self._get_current_price()
        prev_portfolio_value = self._get_portfolio_value()
        
        # ğŸ”„ 2. ê±°ë˜ ì‹¤í–‰
        self._execute_trade_action(action)
        
        # ğŸ’° 3. ê±°ë˜ í›„ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚°
        portfolio_after_trade = self.balance + self.shares_held * current_price_before
        
        # â° 4. ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
        self.current_step += 1
        
        # ğŸš¨ 5. ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        if self.current_step >= len(self.raw_data):
            if self.debug_level == 'DEBUG':
                LOGGER.warning(f"ë°ì´í„° ë²”ìœ„ ì´ˆê³¼ë¡œ ì—í”¼ì†Œë“œ ì¡°ê¸° ì¢…ë£Œ: step {self.current_step}")
            done = True
            current_portfolio_value = self.balance + self.shares_held * current_price_before
            current_price_after = current_price_before
        else:
            # ğŸ“ˆ 6. ì‹œì¥ ë³€ë™ í›„ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜
            current_price_after = self._get_current_price()
            current_portfolio_value = self._get_portfolio_value()
            
            # âœ… 7. ê¸°ì¡´ ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            done = self.current_step >= self.data_length - 1
        
        # ğŸ 8. ë³´ìƒ ê³„ì‚°
        reward = self._calculate_reward(prev_portfolio_value, portfolio_after_trade, current_portfolio_value)
        
        # ğŸ“ˆ 9. íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.portfolio_values_history.append(current_portfolio_value)
        
        if isinstance(reward, dict):
            self.rewards_history.append(reward['total'])
            self.trade_effects_history.append(reward['trade'])
            self.market_effects_history.append(reward['market'])
        else:
            self.rewards_history.append(reward)
            # ê°„ë‹¨í•œ íš¨ê³¼ ê³„ì‚°í•´ì„œ ì €ì¥
            trade_effect = (portfolio_after_trade - prev_portfolio_value) / prev_portfolio_value if prev_portfolio_value > 0 else 0
            market_effect = (current_portfolio_value - portfolio_after_trade) / portfolio_after_trade if portfolio_after_trade > 0 else 0
            self.trade_effects_history.append(trade_effect)
            self.market_effects_history.append(market_effect)
        
        # ğŸ‘€ 10. ê´€ì¸¡ê°’ ë° ì¶”ê°€ ì •ë³´
        observation = self._get_observation()
        info = self._get_info()
        
        # ğŸ“Š ì‹œê°„ ì •ë³´ë¥¼ infoì— ì¶”ê°€
        if self.current_step < len(self.raw_data):
            current_timestamp = self.raw_data.index[self.current_step]
        else:
            current_timestamp = "ë²”ìœ„ì´ˆê³¼"
            
        info.update({
            'reward_mode': self.reward_mode,
            'current_timestamp': current_timestamp,
            'price_before': current_price_before,
            'price_after': current_price_after,
            'portfolio_before': prev_portfolio_value,
            'portfolio_after_trade': portfolio_after_trade,
            'portfolio_after_market': current_portfolio_value
        })
        
        # ê°„ë‹¨í•œ ì§„í–‰ ë¡œê·¸ (ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ì—ì„œë§Œ)
        if self.debug_level == 'INFO' and (self.current_step % 1000 == 0 or done):
            LOGGER.info(f"Step {self.current_step}: í¬íŠ¸í´ë¦¬ì˜¤=${current_portfolio_value:.2f}, "
                       f"ë³´ìƒ={reward if not isinstance(reward, dict) else reward['total']:.4f}, "
                       f"ê±°ë˜={self.position}")
        
        return observation, reward, done, info

    def _get_info(self) -> Dict[str, Any]:
        """
        ğŸ“Š ì¶”ê°€ ì •ë³´ ë°˜í™˜
        """
        current_price = self._get_current_price()
        portfolio_value = self._get_portfolio_value()
        
        total_return = ((portfolio_value - self.initial_balance) / self.initial_balance) if self.initial_balance > 0 else 0
        
        # ğŸ“Š í¬ì§€ì…˜ ê²°ì •
        if self.trade_executed:
            position = self.position
        else:
            if self.shares_held < self.previous_shares_held:
                position = "ë§¤ë„"
            elif self.shares_held > self.previous_shares_held:
                position = "ë§¤ìˆ˜"
            else:
                position = "í™€ë“œ"
        
        self.previous_shares_held = self.shares_held
        
        # â° í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        current_timestamp = None
        if self.current_step < len(self.timestamps):
            current_timestamp = self.timestamps[self.current_step]
        
        timestamps_info = None
        if self.current_step < len(self.raw_data):
            timestamps_info = self.raw_data.index[self.current_step]
        
        return {
            'step': self.current_step,
            'timestamp': current_timestamp,
            'timestamps': timestamps_info,
            'balance': self.balance,
            'shares_held': self.shares_held,
            'position': position, 
            "previous_shares_held": self.previous_shares_held,
            'current_price': current_price,
            'portfolio_value': portfolio_value,
            'total_return': total_return,
            'cost_basis': self.cost_basis,
            'total_shares_purchased': self.total_shares_purchased,
            'total_shares_sold': self.total_shares_sold,
            'total_sales_value': self.total_sales_value,
            'total_commission': self.total_commission,
            'trade_executed': self.trade_executed,
        }
    
    def render(self, mode: str = 'human') -> None:
        """
        ğŸ–¥ï¸ í™˜ê²½ ì‹œê°í™” (ë¡œê·¸ ê°„ì†Œí™”)
        """
        if self.debug_level != 'DEBUG':
            return  # DEBUG ë ˆë²¨ì—ì„œë§Œ ì¶œë ¥
            
        info = self._get_info()
        
        LOGGER.debug(f"Step: {info['step']} | ì”ê³ : ${info['balance']:.2f} | "
                    f"ë³´ìœ ëŸ‰: {info['shares_held']:.4f} | í˜„ì¬ê°€: ${info['current_price']:.2f} | "
                    f"í¬íŠ¸í´ë¦¬ì˜¤: ${info['portfolio_value']:.2f} | ìˆ˜ìµë¥ : {info['total_return'] * 100:.2f}%")
    
    def get_episode_data(self) -> Dict[str, List]:
        """
        ğŸ“ˆ ì—í”¼ì†Œë“œ ë°ì´í„° ë°˜í™˜
        """
        return {
            'actions': self.actions_history,
            'rewards': self.rewards_history,
            'portfolio_values': self.portfolio_values_history
        }
    
    def get_final_portfolio_value(self) -> float:
        """
        ğŸ’ ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë°˜í™˜
        """
        return self._get_portfolio_value()
    
    def get_total_reward(self) -> float:
        """
        ğŸ ì´ ë³´ìƒ ë°˜í™˜
        """
        return sum(self.rewards_history)
    
    def get_reward_analysis(self) -> Dict[str, Any]:
        """
        ğŸ“Š ë³´ìƒ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
        """
        if not self.trade_effects_history:
            return {}
        
        trade_effects = np.array(self.trade_effects_history)
        market_effects = np.array(self.market_effects_history)
        
        return {
            'trade_effects': {
                'mean': np.mean(trade_effects),
                'std': np.std(trade_effects),
                'sum': np.sum(trade_effects),
                'positive_ratio': np.mean(trade_effects > 0)
            },
            'market_effects': {
                'mean': np.mean(market_effects),
                'std': np.std(market_effects),
                'sum': np.sum(market_effects),
                'positive_ratio': np.mean(market_effects > 0)
            },
            'correlation': np.corrcoef(trade_effects, market_effects)[0, 1] if len(trade_effects) > 1 else 0
        }

def create_environment_from_results(results: Dict[str, Dict[str, Any]], symbol: str, data_type: str = 'test', **kwargs) -> TradingEnvironment:
    """
    ğŸ­ DataProcessor ê²°ê³¼ë¡œë¶€í„° í™˜ê²½ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    
    Args:
        results: DataProcessor.process_all_symbols()ì˜ ê²°ê³¼
        symbol: ì£¼ì‹ ì‹¬ë³¼
        data_type: 'train', 'valid', 'test' ì¤‘ í•˜ë‚˜
        **kwargs: TradingEnvironment ì¶”ê°€ ì¸ì
        
    Returns:
        TradingEnvironment ì¸ìŠ¤í„´ìŠ¤
    """
    if symbol not in results:
        raise ValueError(f"ì‹¬ë³¼ {symbol}ì„ ê²°ê³¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    result = results[symbol]
    
    # ğŸ“Š ì •ê·œí™”ëœ ë°ì´í„° (ì—ì´ì „íŠ¸ ê´€ì¸¡ìš©)
    if data_type not in result:
        raise ValueError(f"ë°ì´í„° íƒ€ì… {data_type}ì„ ì‹¬ë³¼ {symbol}ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    normalized_data = result[data_type]
    
    # ğŸ“ˆ ì›ë³¸ ë°ì´í„° (ì‹¤ì œ ê±°ë˜ìš©) - featured_dataì—ì„œ í•´ë‹¹ êµ¬ê°„ ì¶”ì¶œ
    featured_data = result['featured_data']
    
    if data_type == 'train':
        raw_data = featured_data.iloc[:len(normalized_data)]
    elif data_type == 'valid':
        train_len = len(result['train'])
        raw_data = featured_data.iloc[train_len:train_len + len(normalized_data)]
    else:  # test
        train_len = len(result['train'])
        valid_len = len(result['valid'])
        raw_data = featured_data.iloc[train_len + valid_len:train_len + valid_len + len(normalized_data)]
    
    # ê¸°ë³¸ ë¡œê·¸ëŠ” ê°„ì†Œí™”
    LOGGER.info(f"í™˜ê²½ìƒì„±: {symbol}-{data_type} | ì •ê·œí™”:{normalized_data.shape} | ì›ë³¸:{raw_data.shape}")
    
    # âœ… í™˜ê²½ ìƒì„± (ê¸°ë³¸ê°’ìœ¼ë¡œ detailed_logging=False)
    env = TradingEnvironment(
        data=normalized_data,
        raw_data=raw_data,
        symbol=symbol,
        train_data=(data_type == 'train'),
        detailed_logging=kwargs.get('detailed_logging', False),  # ê¸°ë³¸ê°’ False
        debug_level=kwargs.get('debug_level', 'INFO'),  # ê¸°ë³¸ê°’ INFO
        **{k: v for k, v in kwargs.items() if k not in ['detailed_logging', 'debug_level']}
    )
    return env    

class MultiAssetTradingEnvironment:
    """ğŸ¢ ë‹¤ì¤‘ ìì‚° íŠ¸ë ˆì´ë”© í™˜ê²½ í´ë˜ìŠ¤ (ë¡œê·¸ ìµœì í™” ë²„ì „)"""
    
    def __init__(
        self,
        results: Dict[str, Dict[str, Any]],
        symbols: List[str],
        data_type: str = 'test',
        window_size: int = WINDOW_SIZE,
        initial_balance: float = INITIAL_BALANCE,
        max_trading_units: int = MAX_TRADING_UNITS,
        transaction_fee_percent: float = TRANSACTION_FEE_PERCENT,
        reward_mode: str = 'combined',
        detailed_logging: bool = False,  # ê¸°ë³¸ê°’ Falseë¡œ ë³€ê²½
        debug_level: str = 'INFO'
    ):
        """
        MultiAssetTradingEnvironment í´ë˜ìŠ¤ ì´ˆê¸°í™”
        """
        self.symbols = symbols
        self.n_assets = len(symbols)
        self.data_type = data_type
        self.initial_balance = initial_balance
        
        LOGGER.info(f"ë‹¤ì¤‘ìì‚°í™˜ê²½ ì´ˆê¸°í™”: {self.n_assets}ê°œ ìì‚° | {data_type} ë°ì´í„°")
        
        # ğŸ­ ê°œë³„ í™˜ê²½ ìƒì„±
        self.envs = {}
        for symbol in symbols:
            self.envs[symbol] = create_environment_from_results(
                results=results,
                symbol=symbol,
                data_type=data_type,
                window_size=window_size,
                initial_balance=initial_balance / self.n_assets,  # ìì‚°ë³„ ê· ë“± ë°°ë¶„
                max_trading_units=max_trading_units,
                transaction_fee_percent=transaction_fee_percent,
                reward_mode=reward_mode,
                detailed_logging=detailed_logging,
                debug_level=debug_level
            )
        
        # ğŸ® í–‰ë™ ê³µê°„: ê° ìì‚°ì— ëŒ€í•œ ì—°ì† í–‰ë™
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(self.n_assets,), dtype=np.float32
        )
        
        # ğŸ‘€ ê´€ì¸¡ ê³µê°„
        self.observation_space = spaces.Dict({
            symbol: env.observation_space for symbol, env in self.envs.items()
        })
        
        LOGGER.info(f"ë‹¤ì¤‘ìì‚°í™˜ê²½ ì´ˆê¸°í™” ì™„ë£Œ: {self.n_assets}ê°œ ìì‚°")

    def reset(self) -> Dict[str, Dict[str, np.ndarray]]:
        """
        ğŸ”„ í™˜ê²½ ì´ˆê¸°í™”
        """
        LOGGER.info("ë‹¤ì¤‘ìì‚°í™˜ê²½ ë¦¬ì…‹")
        observations = {}
        for symbol, env in self.envs.items():
            observations[symbol] = env.reset()
        
        return observations
    
    def step(self, actions: Dict[str, float]) -> Tuple[Dict[str, Dict[str, np.ndarray]], float, bool, Dict[str, Any]]:
        """
        ğŸš€ í™˜ê²½ì—ì„œ í•œ ìŠ¤í… ì§„í–‰ (ë¡œê·¸ ê°„ì†Œí™”)
        
        Args:
            actions: ì‹¬ë³¼ì„ í‚¤ë¡œ í•˜ê³  í–‰ë™ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
            
        Returns:
            (ê´€ì¸¡ê°’, ë³´ìƒ, ì¢…ë£Œ ì—¬ë¶€, ì¶”ê°€ ì •ë³´) íŠœí”Œ
        """
        observations = {}
        rewards = {}
        dones = {}
        infos = {}
        
        # ğŸ“Š ê° ìì‚°ì— ëŒ€í•œ í–‰ë™ ì‹¤í–‰
        for symbol, env in self.envs.items():
            action = actions.get(symbol, 0.0)  # í–‰ë™ì´ ì—†ëŠ” ê²½ìš° í™€ë“œ
            obs, rew, done, info = env.step(action)
            
            observations[symbol] = obs
            rewards[symbol] = rew
            dones[symbol] = done
            infos[symbol] = info
        
        # ğŸ ì „ì²´ ë³´ìƒì€ ê° ìì‚°ì˜ ë³´ìƒ í‰ê· 
        total_reward = sum(rewards.values()) / self.n_assets
        
        # ğŸ ëª¨ë“  ìì‚°ì˜ ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë˜ë©´ ì „ì²´ ì—í”¼ì†Œë“œ ì¢…ë£Œ
        done = all(dones.values())
        
        # ğŸ’ ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚°
        total_portfolio_value = sum(info['portfolio_value'] for info in infos.values())
        trade_executed_any = any(info.get('trade_executed', False) for info in infos.values())
        
        # ğŸ“Š ì¶”ê°€ ì •ë³´ì— ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ í¬í•¨
        infos['total'] = {
            'portfolio_value': total_portfolio_value,
            'total_return': (total_portfolio_value - self.initial_balance) / self.initial_balance,
            'trade_executed': trade_executed_any
        }

        return observations, total_reward, done, infos
    
    def render(self, mode: str = 'human') -> None:
        """
        ğŸ–¥ï¸ í™˜ê²½ ì‹œê°í™” (ë¡œê·¸ ê°„ì†Œí™”)
        """
        total_portfolio_value = sum(env.get_final_portfolio_value() for env in self.envs.values())
        total_return = (total_portfolio_value - self.initial_balance) / self.initial_balance
        
        LOGGER.info(f"ë‹¤ì¤‘ìì‚°ìƒíƒœ: ì´í¬íŠ¸í´ë¦¬ì˜¤=${total_portfolio_value:.2f} | ìˆ˜ìµë¥ ={total_return*100:.2f}%")
    
    def get_final_portfolio_value(self) -> float:
        """
        ğŸ’ ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë°˜í™˜
        """
        return sum(env.get_final_portfolio_value() for env in self.envs.values())
    
    def get_total_reward(self) -> float:
        """
        ğŸ ì´ ë³´ìƒ ë°˜í™˜
        """
        return sum(env.get_total_reward() for env in self.envs.values()) / self.n_assets


def create_flexible_environment(results, symbol, reward_mode='combined', detailed_logging=False, data_type='train', debug_level='INFO'):
    """
    ğŸ”§ ìœ ì—°í•œ ë³´ìƒ ì‹œìŠ¤í…œì„ ê°€ì§„ í™˜ê²½ ìƒì„± (ë¡œê·¸ ìµœì í™”)
    
    Args:
        results: DataProcessor ê²°ê³¼
        symbol: ì£¼ì‹ ì‹¬ë³¼
        reward_mode: 'combined', 'trade_only', 'market_only', 'separated'
        detailed_logging: ìƒì„¸ ë¡œê¹… ì—¬ë¶€ (ê¸°ë³¸ê°’ Falseë¡œ ë³€ê²½)
        data_type: 'train', 'valid', 'test'
        debug_level: 'INFO', 'DEBUG'
    """
    LOGGER.info(f"ìœ ì—°í™˜ê²½ìƒì„±: {symbol} | ëª¨ë“œ:{reward_mode} | ë ˆë²¨:{debug_level}")
    env = create_environment_from_results(
        results=results,
        symbol=symbol,
        data_type=data_type,
        reward_mode=reward_mode,
        detailed_logging=detailed_logging,
        debug_level=debug_level
    )
    return env


if __name__ == "__main__":
    # ğŸ§ª ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì½”ë“œ (ë¡œê·¸ ê°„ì†Œí™”)
    import matplotlib.pyplot as plt
    from src.data_collection.data_collector import DataCollector
    from src.preprocessing.data_processor import DataProcessor
    from src.config.config import config
    import argparse
    
    parser = argparse.ArgumentParser(description="í…ŒìŠ¤íŠ¸í•  ì‹¬ë³¼")
    parser.add_argument("--symbols", nargs="+", help="í…ŒìŠ¤íŠ¸í•  ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸", default=config.trading_symbols)
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    args = parser.parse_args()
    
    # ğŸ“Š ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ í• ë‹¹
    symbols = args.symbols
    TARGET_SYMBOL = symbols[0] if symbols else config.trading_symbols
    debug_level = 'DEBUG' if args.debug else 'INFO'
    
    LOGGER.info(f'í…ŒìŠ¤íŠ¸ ì‹œì‘: {TARGET_SYMBOL} | ë””ë²„ê·¸ë ˆë²¨: {debug_level}')
    
    # ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
    collector = DataCollector(symbols=symbols)
    data = collector.load_all_data()
    
    try:
        if data:
            processor = DataProcessor()
            results = processor.process_all_symbols(data)
            
            if TARGET_SYMBOL in results:
                LOGGER.info(f"ìµœì í™”ëœ TradingEnvironment í…ŒìŠ¤íŠ¸ ì‹œì‘: {TARGET_SYMBOL}")
                
                # ğŸ¯ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (ë¡œê·¸ ìµœì†Œí™”)
                env = create_flexible_environment(
                    results=results,
                    symbol=TARGET_SYMBOL,
                    reward_mode='combined',
                    detailed_logging=False,
                    data_type='train',
                    debug_level=debug_level
                )
                
                LOGGER.info(f"í™˜ê²½ ìƒì„± ì™„ë£Œ: {env.symbol}")
                
                # ğŸ§ª 5ìŠ¤í…ë§Œ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸
                obs = env.reset()
                total_reward = 0
                
                for i in range(5):
                    action = np.random.uniform(-1.0, 1.0)
                    obs, reward, done, info = env.step(action)
                    
                    reward_val = reward if not isinstance(reward, dict) else reward['total']
                    total_reward += reward_val
                    
                    if debug_level == 'DEBUG':
                        LOGGER.debug(f"Step {i+1}: í–‰ë™={action:.4f}, ë³´ìƒ={reward_val:.6f}")
                    
                    if done:
                        break
                
                LOGGER.info(f"í…ŒìŠ¤íŠ¸ ì™„ë£Œ: ì´ë³´ìƒ={total_reward:.6f}")
                
            else:
                LOGGER.error(f"{TARGET_SYMBOL} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            LOGGER.error("ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        LOGGER.error(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()