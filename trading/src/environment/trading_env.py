"""
ê°•í™”í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë”© í™˜ê²½ ëª¨ë“ˆ (íš¨ìœ¨ì ì¸ ë¡œê¹… ë²„ì „)
"""
import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Union, Optional, Any
import gym
from gym import spaces
from src.data_collection.data_collector import DataCollector
from decimal import Decimal, ROUND_DOWN, getcontext

# ì €ì¥ ê²½ë¡œ í´ë” ìƒì„±
save_dir = './results'
os.makedirs(save_dir, exist_ok=True)

from src.config.ea_teb_config import (
    INITIAL_BALANCE,
    MAX_TRADING_UNITS,
    TRANSACTION_FEE_PERCENT,
    WINDOW_SIZE,
    LOGGER,
)

class TradingEnvironment:
    """
    ê°•í™”í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë”© í™˜ê²½ í´ë˜ìŠ¤ (íš¨ìœ¨ì ì¸ ë¡œê¹… ë²„ì „)
    """
    
    def __init__(
        self,
        data: pd.DataFrame,
        raw_data: pd.DataFrame = None,
        window_size: int = WINDOW_SIZE,
        initial_balance: float = INITIAL_BALANCE,
        max_trading_units: int = MAX_TRADING_UNITS,
        transaction_fee_percent: float = TRANSACTION_FEE_PERCENT,
        symbol: str = None,
        train_data: bool = True,
        reward_mode: str = 'combined',
        detailed_logging: bool = True,
        log_level: str = 'normal'
    ):
        """
        TradingEnvironment í´ë˜ìŠ¤ ì´ˆê¸°í™”
        """
        # ğŸ“Š ê¸°ë³¸ ë°ì´í„° ì„¤ì •
        self.data = data
        if raw_data is not None:
            self.raw_data = raw_data
        else:
            LOGGER.warning("âš ï¸ raw_dataê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.raw_data = data
        
        print(f'ğŸ“ˆ ì •ê·œí™”ëœ ë°ì´í„° í˜•íƒœ: {data.shape}')
        print(f'ğŸ“Š ì›ë³¸ ë°ì´í„° í˜•íƒœ: {self.raw_data.shape}')
        
        # â° íƒ€ì„ìŠ¤íƒ¬í”„ ì´ˆê¸°í™”
        if hasattr(self.raw_data.index, 'values'):
            self.timestamps = self.raw_data.index.values
        else:
            self.timestamps = np.array([None] * len(self.raw_data))

        # ğŸ¯ í™˜ê²½ ì„¤ì • íŒŒë¼ë¯¸í„°
        self.window_size = window_size
        self.initial_balance = initial_balance
        self.max_trading_units = max_trading_units
        self.transaction_fee_percent = transaction_fee_percent
        self.symbol = symbol if symbol else "UNKNOWN"
        self.train_data = train_data
        self.reward_mode = reward_mode
        self.detailed_logging = detailed_logging
        self.log_level = log_level  # ë¡œê·¸ ë ˆë²¨ ì¶”ê°€
        
        # ğŸ“ ë°ì´í„° ê´€ë ¨ ë³€ìˆ˜
        self.feature_dim = data.shape[1]
        self.data_length = len(data)
        
        # ğŸ’° í™˜ê²½ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
        self._init_state_variables()
        
        # ğŸ® í–‰ë™ ê³µê°„: [-1.0, 1.0] ë²”ìœ„ì˜ ì—°ì†ì ì¸ ê°’
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)
        
        # ğŸ‘€ ê´€ì¸¡ ê³µê°„: ê°€ê²© ë°ì´í„° + í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ
        self.observation_space = spaces.Dict({
            'market_data': spaces.Box(
                low=-np.inf, high=np.inf, shape=(self.window_size, self.feature_dim), dtype=np.float32
            ),
            'portfolio_state': spaces.Box(
                low=0, high=np.inf, shape=(2,), dtype=np.float32
            )
        })
        
        # ğŸ” ì›ë³¸ ë°ì´í„° ê²€ì¦
        if 'close' not in self.raw_data.columns:
            LOGGER.warning("âš ï¸ raw_dataì— 'close' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ì»¬ëŸ¼ì„ ì¢…ê°€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # âœ… ìœ íš¨ì„± ê²€ì‚¬
        valid_modes = ['combined', 'trade_only', 'market_only', 'separated']
        if reward_mode not in valid_modes:
            raise ValueError(f"reward_modeëŠ” {valid_modes} ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
        
    def _init_state_variables(self):
        """í™˜ê²½ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”"""
        # ëœë¤ ì‹œì‘ì  ì„¤ì • (window_size ì´í›„ë¶€í„° ì‹œì‘ ê°€ëŠ¥)
        max_start = self.data_length - self.window_size - 1
        if max_start > self.window_size:
            self.current_step = np.random.randint(self.window_size, max_start)
        else:
            self.current_step = self.window_size
            
        self.balance = self.initial_balance
        self.shares_held = 0
        self.cost_basis = 0
        self.total_shares_purchased = 0
        self.total_shares_sold = 0
        self.total_sales_value = 0
        self.total_commission = 0
        self.position = "í™€ë“œ"
        self.trade_executed = False 
        self.previous_shares_held = 0
        self.invalid_sell_penalty = False
        
        # ğŸ“ˆ ì—í”¼ì†Œë“œ íˆìŠ¤í† ë¦¬
        self.states_history = []
        self.actions_history = []
        self.rewards_history = []
        self.portfolio_values_history = []
        self.trade_effects_history = []
        self.market_effects_history = []
        self.combined_effects_history = []
    
    def reset(self) -> Dict[str, np.ndarray]:
        """ğŸ”„ í™˜ê²½ ì´ˆê¸°í™” (ëœë¤ ì‹œì‘ì )"""
        if self.log_level != 'minimal':
            print(f"\nğŸ”„ í™˜ê²½ ë¦¬ì…‹ ì‹œì‘ - {self.symbol}")
            print(f"ğŸ’° ì´ˆê¸° ì”ê³ : ${self.initial_balance:.2f}")
        
        self._init_state_variables()
        
        if self.log_level != 'minimal':
            print(f"ğŸ“ ì‹œì‘ì : {self.current_step} (ëœë¤)")
            
        return self._get_observation()

    def _get_current_price(self) -> float:
        """ğŸ’µ í˜„ì¬ ì£¼ê°€ ë°˜í™˜ (ìµœì í™”ëœ ë¡œê¹…)"""
        if self.current_step >= len(self.raw_data):
            last_valid_index = len(self.raw_data) - 1
            if self.log_level == 'detailed':
                LOGGER.warning(f"âš ï¸ ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼: {self.current_step} >= {len(self.raw_data)}")
            
            if 'close' in self.raw_data.columns:
                price = float(self.raw_data.iloc[last_valid_index]['close'])
            else:
                price = float(self.raw_data.iloc[last_valid_index][-1])
        else:
            if 'close' in self.raw_data.columns:
                price = float(self.raw_data.iloc[self.current_step]['close'])
            else:
                price = float(self.raw_data.iloc[self.current_step][-1])
        
        # ğŸš¨ ê°€ê²©ì´ 0 ì´í•˜ì¸ ê²½ìš° ì²˜ë¦¬
        if price <= 0:
            if self.log_level == 'detailed':
                LOGGER.warning(f"âš ï¸ í˜„ì¬ ê°€ê²©ì´ 0 ì´í•˜ì…ë‹ˆë‹¤: {price}, ìµœì†Œê°’ìœ¼ë¡œ ì¡°ì •")
            price = 0.01
            
        return price

    def _get_observation(self) -> Dict[str, np.ndarray]:
        """ğŸ‘€ í˜„ì¬ ê´€ì¸¡ê°’ ë°˜í™˜"""
        # ğŸ“Š ìœˆë„ìš° í¬ê¸°ë§Œí¼ì˜ ì •ê·œí™”ëœ ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        start_idx = max(0, self.current_step - self.window_size + 1)
        end_idx = self.current_step + 1
        
        # ğŸ”§ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš° íŒ¨ë”© ì²˜ë¦¬
        if start_idx == 0 and end_idx - start_idx < self.window_size:
            market_data = np.zeros((self.window_size, self.feature_dim), dtype=np.float32)
            actual_data = self.data.iloc[start_idx:end_idx].values
            market_data[-len(actual_data):] = actual_data
        else:
            market_data = self.data.iloc[start_idx:end_idx].values
            if len(market_data) < self.window_size:
                padding = np.zeros((self.window_size - len(market_data), self.feature_dim), dtype=np.float32)
                market_data = np.vstack([padding, market_data])
        
        # ğŸ’ í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ê³„ì‚°
        portfolio_value = self._get_portfolio_value()
        stock_value = self.shares_held * self._get_current_price()
        
        portfolio_state = np.array([
            self.balance / portfolio_value,  # í˜„ê¸ˆ ë¹„ìœ¨
            stock_value / portfolio_value    # ì£¼ì‹ ë¹„ìœ¨
        ], dtype=np.float32)
        
        observation = {
            'market_data': market_data.astype(np.float32),
            'portfolio_state': portfolio_state
        }
        
        self.states_history.append(observation)
        return observation

    def _get_portfolio_value(self) -> float:
        """ğŸ’ í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚°"""
        current_price = self._get_current_price()
        stock_value = self.shares_held * current_price
        total_value = self.balance + stock_value
        
        if total_value <= 0:
            return max(self.balance, 1.0)
            
        return total_value

    def _calculate_reward(self, prev_portfolio_value: float, portfolio_after_trade: float, 
                         current_portfolio_value: float) -> Union[float, Dict[str, float]]:
        """ğŸ ê°œì„ ëœ ë³´ìƒ ê³„ì‚° (ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© ìµœì í™”)"""
        if prev_portfolio_value <= 0:
            prev_portfolio_value = max(self.balance, 1.0)
        
        # ê° íš¨ê³¼ ê³„ì‚° (ì •ê·œí™”)
        if prev_portfolio_value > 0:
            trade_effect = (portfolio_after_trade - prev_portfolio_value) / prev_portfolio_value
        else:
            trade_effect = 0
            
        if portfolio_after_trade > 0:
            market_effect = (current_portfolio_value - portfolio_after_trade) / portfolio_after_trade
        else:
            market_effect = 0
            
        if prev_portfolio_value > 0:
            total_effect = (current_portfolio_value - prev_portfolio_value) / prev_portfolio_value
        else:
            total_effect = 0
        
        # âœ… ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© ìµœì í™”ëœ ë³´ìƒ
        if self.reward_mode == 'trade_only':
            # ê±°ë˜ íš¨ê³¼ë§Œ ê³ ë ¤ (ë” ë³´ìˆ˜ì ì¸ ìŠ¤ì¼€ì¼ë§)
            reward = np.tanh(trade_effect * 50)  # [-1, 1] ë²”ìœ„ë¡œ ë¶€ë“œëŸ½ê²Œ ì œí•œ
        elif self.reward_mode == 'market_only':
            # ì‹œì¥ íš¨ê³¼ë§Œ ê³ ë ¤
            reward = np.tanh(market_effect * 30)
        elif self.reward_mode == 'separated':
            trade_reward = np.tanh(trade_effect * 50)
            market_reward = np.tanh(market_effect * 30)
            reward = {
                'trade': trade_reward,
                'market': market_reward,
                'total': trade_reward + market_reward
            }
        else:  # 'combined' - ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© ìµœì í™”
            # âœ… í¬íŠ¸í´ë¦¬ì˜¤ ì´ ìˆ˜ìµë¥  ê¸°ë°˜ (ë” ë¶€ë“œëŸ¬ìš´ ë³´ìƒ)
            base_reward = np.tanh(total_effect * 20)  # ê¸°ë³¸ ìˆ˜ìµë¥  ë³´ìƒ
            
            # âœ… í¬ì§€ì…˜ ë‹¤ì–‘ì„± ë³´ìƒ (ë¬´ê±°ë˜ ë°©ì§€)
            current_stock_ratio = self._get_current_stock_ratio()
            # ì ì ˆí•œ í¬ì§€ì…˜ ìœ ì§€ ë³´ìƒ (0.2~0.8 ì‚¬ì´ê°€ ì¢‹ìŒ)
            if 0.2 <= current_stock_ratio <= 0.8:
                position_bonus = 0.1
            elif current_stock_ratio == 0 or current_stock_ratio == 1:
                position_bonus = -0.2  # ê·¹ë‹¨ì  í¬ì§€ì…˜ íŒ¨ë„í‹°
            else:
                position_bonus = 0
            
            # âœ… ë³€ë™ì„± ì¡°ì • (ê³¼ë„í•œ ê±°ë˜ ë°©ì§€)
            if abs(trade_effect) > 0.05:  # 5% ì´ìƒ ë³€í™”
                volatility_penalty = -0.05
            else:
                volatility_penalty = 0
            
            reward = base_reward + position_bonus + volatility_penalty
        
        # âœ… ê°œì„ ëœ íŒ¨ë„í‹° ì‹œìŠ¤í…œ (ë” ë¶€ë“œëŸ½ê²Œ)
        if self.invalid_sell_penalty:
            penalty = -0.3  # -1.0ì—ì„œ -0.3ìœ¼ë¡œ ì™„í™”
            if isinstance(reward, dict):
                reward['trade'] += penalty
                reward['total'] += penalty
            else:
                reward += penalty
        
        # âœ… ìµœì¢… ë³´ìƒ í´ë¦¬í•‘ (ê·¹ë‹¨ê°’ ë°©ì§€)
        if isinstance(reward, dict):
            for key in reward:
                reward[key] = np.clip(reward[key], -2.0, 2.0)
        else:
            reward = np.clip(reward, -2.0, 2.0)
        
        return reward

    def step(self, action: float) -> Tuple[Dict[str, np.ndarray], Union[float, Dict[str, float]], bool, Dict[str, Any]]:
        """
        í™˜ê²½ì—ì„œ í•œ ìŠ¤í… ì§„í–‰ (íš¨ìœ¨ì ì¸ ë¡œê¹… ë²„ì „)
        """
        # ğŸš€ Step ì‹œì‘ ë¡œê¹…
        if self.log_level != 'minimal':
            print(f"\nâœ… Step {self.current_step}")
            if self.log_level == 'detailed':
                print("=" * 60)
        
        # ì´ˆê¸°í™”
        self.invalid_sell_penalty = False
        self.actions_history.append(action)
        
        # ğŸ“Š ê±°ë˜ ì „ ìƒíƒœ
        current_price_before = self._get_current_price()
        prev_portfolio_value = self._get_portfolio_value()
        
        # ì‹œê°„ ì •ë³´ (í•„ìš”í•œ ê²½ìš°ë§Œ)
        if self.current_step < len(self.raw_data):
            before_timestamp = self.raw_data.index[self.current_step]
        else:
            before_timestamp = "ë²”ìœ„ ì´ˆê³¼"
        
        if self.log_level != 'minimal':
            action_value = action[0] if isinstance(action, np.ndarray) else action
            print(f"â° {before_timestamp} | âœ… ê±°ë˜ ì „ ê°€ê²© : ${current_price_before:.2f} | âœ… í–‰ë™ ê°’ : {action_value:.3f}")
            print(f"âœ…  í¬íŠ¸í´ë¦¬ì˜¤: ${prev_portfolio_value:.2f} (${self.balance:.2f} + {self.shares_held:.3f}ì£¼)")
        
        # ê±°ë˜ ì‹¤í–‰
        self._execute_trade_action(action)
        
        # ê±°ë˜ í›„ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚°
        portfolio_after_trade = self.balance + self.shares_held * current_price_before
        
        # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
        self.current_step += 1
        
        # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        if self.current_step >= len(self.raw_data):
            if self.log_level == 'detailed':
                LOGGER.warning(f"âš ï¸ ë°ì´í„° ë²”ìœ„ ì´ˆê³¼ë¡œ ì—í”¼ì†Œë“œ ì¡°ê¸° ì¢…ë£Œ")
            done = True
            current_portfolio_value = portfolio_after_trade
            current_price_after = current_price_before
            after_timestamp = before_timestamp
        else:
            # ì‹œì¥ ë³€ë™ í›„ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜
            current_price_after = self._get_current_price()
            current_portfolio_value = self._get_portfolio_value()
            after_timestamp = self.raw_data.index[self.current_step]
            done = self.current_step >= self.data_length - 1
        
        # ê°€ê²© ë° í¬íŠ¸í´ë¦¬ì˜¤ ë³€í™” ë¡œê¹…
        if self.log_level != 'minimal':
            price_change = current_price_after - current_price_before
            portfolio_change = current_portfolio_value - prev_portfolio_value
            
            print(f"âœ… ê°€ê²© ë³€í™”: ${price_change:+.2f} | âœ… í¬íŠ¸í´ë¦¬ì˜¤ ë³€í™”: ${portfolio_change:+.2f}")
            
            if self.log_level == 'detailed':
                print(f"âœ… ê°€ê²©: ${current_price_before:.2f} â†’ ${current_price_after:.2f}")
                print(f"âœ… í¬íŠ¸í´ë¦¬ì˜¤: ${prev_portfolio_value:.2f} â†’ ${current_portfolio_value:.2f}")
        
        # ë³´ìƒ ê³„ì‚°
        reward = self._calculate_reward(prev_portfolio_value, portfolio_after_trade, current_portfolio_value)
        
        # ë³´ìƒ ë¡œê¹…
        if self.log_level != 'minimal':
            if isinstance(reward, dict):
                print(f"âœ… ë³´ìƒ - ê±°ë˜:{reward['trade']:.4f} | ì‹œì¥:{reward['market']:.4f} | ì´:{reward['total']:.4f}")
            else:
                print(f"âœ… ë³´ìƒ: {reward:.4f}")
        
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.portfolio_values_history.append(current_portfolio_value)
        
        if isinstance(reward, dict):
            self.rewards_history.append(reward['total'])
            self.trade_effects_history.append(reward['trade'])
            self.market_effects_history.append(reward['market'])
        else:
            self.rewards_history.append(reward)
            trade_effect = (portfolio_after_trade - prev_portfolio_value) / prev_portfolio_value if prev_portfolio_value > 0 else 0
            market_effect = (current_portfolio_value - portfolio_after_trade) / portfolio_after_trade if portfolio_after_trade > 0 else 0
            self.trade_effects_history.append(trade_effect)
            self.market_effects_history.append(market_effect)
        
        # ê´€ì¸¡ê°’ ë° ì¶”ê°€ ì •ë³´
        observation = self._get_observation()
        info = self._get_info()
        
        # ì‹œê°„ ì •ë³´ë¥¼ infoì— ì¶”ê°€
        info.update({
            'reward_mode': self.reward_mode,
            'current_timestamp': after_timestamp,
            'previous_timestamp': before_timestamp,
            'price_before': current_price_before,
            'price_after': current_price_after,
            'portfolio_before': prev_portfolio_value,
            'portfolio_after_trade': portfolio_after_trade,
            'portfolio_after_market': current_portfolio_value,
            # ğŸ†• í¬ì§€ì…˜ ë³€í™” ì •ë³´ ì¶”ê°€ (ì—”íŠ¸ë¡œí”¼ ì¡°ì •ìš©)
            'position_change': abs(info['stock_ratio'] - (self.previous_shares_held * current_price_before / prev_portfolio_value) if prev_portfolio_value > 0 else 0),
            'target_position_ratio': info['stock_ratio'],  # í˜„ì¬ê°€ ëª©í‘œê°€ ë¨
        })
        
        if self.log_level == 'detailed':
            print("=" * 60)
        
        return observation, reward, done, info

    def _get_info(self) -> Dict[str, Any]:
        """ì¶”ê°€ ì •ë³´ ë°˜í™˜"""
        current_price = self._get_current_price()
        portfolio_value = self._get_portfolio_value()
        
        # ğŸ†• í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ìœ¨ ì •ë³´ ì¶”ê°€
        stock_ratio = self._get_current_stock_ratio()
        cash_ratio = 1.0 - stock_ratio
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ ì¶”ê°€
        if self.current_step < len(self.raw_data):
            current_timestamp = self.raw_data.index[self.current_step]
        else:
            current_timestamp = "ë²”ìœ„ ì´ˆê³¼"
        
        timestamps_info = {
            'current': current_timestamp,
            'total_length': len(self.raw_data)
        }
        
        total_return = (portfolio_value - self.initial_balance) / self.initial_balance
        position = self.position
        
        return {
            # ê¸°ì¡´ í•„ë“œë“¤...
            'step': self.current_step,
            'timestamp': current_timestamp,
            'timestamps': timestamps_info,
            'balance': self.balance,
            'shares_held': self.shares_held,
            'position': position, 
            "previous_shares_held": self.previous_shares_held,
            'current_price': current_price,
            'portfolio_value': portfolio_value,
            'total_return': total_return,
            'cost_basis': self.cost_basis,
            'total_shares_purchased': self.total_shares_purchased,
            'total_shares_sold': self.total_shares_sold,
            'total_sales_value': self.total_sales_value,
            'total_commission': self.total_commission,
            'trade_executed': self.trade_executed,
            'stock_ratio': stock_ratio,
            'cash_ratio': cash_ratio,
        }
    
    def _execute_trade_action(self, action: float) -> None:
        """ê±°ë˜ í–‰ë™ ì‹¤í–‰ (ê°œì„ ëœ ë²„ì „)"""
        self.trade_executed = False
        self.previous_shares_held = self.shares_held
        self.invalid_sell_penalty = False
        
        target_ratio = action
        current_ratio = self._get_current_stock_ratio()
        
        current_price = self._get_current_price()
        portfolio_value = self._get_portfolio_value()
        
        ratio_diff = target_ratio - current_ratio
        
        # ì„ê³„ê°’ì„ ë” ë‚®ì¶°ì„œ evaluationì—ì„œë„ ë” ë¯¼ê°í•œ ê±°ë˜ ì‹¤í–‰
        threshold = 0.0005 if self.log_level == 'minimal' else 0.001  # evaluation: 0.05%, training: 0.1%
        
        if abs(ratio_diff) > threshold:
            if self.log_level == 'detailed':
                print(f"   ğŸ¯ ê±°ë˜ ê²°ì •: ëª©í‘œë¹„ìœ¨ {target_ratio:.4f}, í˜„ì¬ë¹„ìœ¨ {current_ratio:.4f}, ì°¨ì´ {ratio_diff:.4f}")
            
            if ratio_diff > 0:
                self._execute_buy_by_ratio(ratio_diff, current_price, portfolio_value)
            else:
                self._execute_sell_by_ratio(-ratio_diff, current_price, portfolio_value)
        else:
            if self.log_level == 'detailed':
                print(f"   ğŸ’¤ ê±°ë˜ ì—†ìŒ: ë¹„ìœ¨ ì°¨ì´ {abs(ratio_diff):.6f} < ì„ê³„ê°’ {threshold:.6f}")

    def _execute_buy_by_ratio(self, target_increase_ratio: float, current_price: float, portfolio_value: float) -> None:
        """ë¹„ìœ¨ ê¸°ë°˜ ë§¤ìˆ˜ ì‹¤í–‰"""
        # ëª©í‘œ ë§¤ìˆ˜ ê¸ˆì•¡ ê³„ì‚°
        target_buy_amount = portfolio_value * target_increase_ratio
        
        # ê±°ë˜ ë¹„ìš© ê³ ë ¤í•œ ì‹¤ì œ í•„ìš” ê¸ˆì•¡
        total_cost_needed = target_buy_amount * (1 + self.transaction_fee_percent)
        
        if self.balance < total_cost_needed:
            # ì”ê³  ë¶€ì¡± ì‹œ ê°€ëŠ¥í•œ ìµœëŒ€ ê¸ˆì•¡ìœ¼ë¡œ ì¡°ì •
            available_for_stocks = self.balance / (1 + self.transaction_fee_percent)
            if available_for_stocks > 0:
                target_buy_amount = available_for_stocks
                total_cost_needed = self.balance
            else:
                if self.log_level == 'detailed':
                    print(f"   âŒ ì”ê³  ë¶€ì¡±: í•„ìš” ${total_cost_needed:.2f}, ë³´ìœ  ${self.balance:.2f}")
                return
        
        # ë§¤ìˆ˜í•  ì£¼ì‹ ìˆ˜ ê³„ì‚°
        shares_to_buy = target_buy_amount / current_price
        
        if shares_to_buy > 0:
            # ê±°ë˜ ì‹¤í–‰
            actual_cost = shares_to_buy * current_price
            commission = actual_cost * self.transaction_fee_percent
            total_cost = actual_cost + commission
            
            self.balance -= total_cost
            self.shares_held += shares_to_buy
            self.total_shares_purchased += shares_to_buy
            self.total_commission += commission
            
            # í‰ê·  ë§¤ìˆ˜ ë‹¨ê°€ ì—…ë°ì´íŠ¸
            if self.shares_held > 0:
                self.cost_basis = ((self.cost_basis * (self.shares_held - shares_to_buy)) + actual_cost) / self.shares_held
            
            self.trade_executed = True
            self.position = "ë§¤ìˆ˜"
            
            if self.log_level == 'detailed':
                print(f"   âœ… ë§¤ìˆ˜ ì„±ê³µ: {shares_to_buy:.3f}ì£¼ @ ${current_price:.2f} (ëª©í‘œê¸ˆì•¡: ${target_buy_amount:.2f})")

    def _execute_sell_by_ratio(self, target_decrease_ratio: float, current_price: float, portfolio_value: float) -> None:
        """ë¹„ìœ¨ ê¸°ë°˜ ë§¤ë„ ì‹¤í–‰"""
        # ëª©í‘œ ë§¤ë„ ê¸ˆì•¡ ê³„ì‚°
        target_sell_amount = portfolio_value * target_decrease_ratio
        
        # ë§¤ë„í•  ì£¼ì‹ ìˆ˜ ê³„ì‚°
        shares_to_sell = min(target_sell_amount / current_price, self.shares_held)
        
        if shares_to_sell <= 0:
            if self.log_level == 'detailed':
                print(f"   âŒ ë§¤ë„í•  ì£¼ì‹ì´ ì—†ìŒ")
            return
        
        # ê±°ë˜ ì‹¤í–‰
        gross_sell_value = shares_to_sell * current_price
        commission = gross_sell_value * self.transaction_fee_percent
        net_value = gross_sell_value - commission
        
        self.balance += net_value
        self.shares_held -= shares_to_sell
        self.total_shares_sold += shares_to_sell
        self.total_sales_value += gross_sell_value
        self.total_commission += commission
        
        self.trade_executed = True
        self.position = "ë§¤ë„"
        
        if self.log_level == 'detailed':
            print(f"   âœ… ë§¤ë„ ì„±ê³µ: {shares_to_sell:.3f}ì£¼ @ ${current_price:.2f} (ëª©í‘œê¸ˆì•¡: ${target_sell_amount:.2f})")

    def _get_current_stock_ratio(self) -> float:
        """í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ì—ì„œ ì£¼ì‹ì´ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ ê³„ì‚°"""
        current_price = self._get_current_price()
        stock_value = self.shares_held * current_price
        total_portfolio = self.balance + stock_value
        
        if total_portfolio <= 0:
            return 0.0
        
        return stock_value / total_portfolio

    def render(self, mode: str = 'human') -> None:
        """í™˜ê²½ ì‹œê°í™”"""
        info = self._get_info()
        
        print(f"âœ… Step: {info['step']}")
        print(f"âœ… ì”ê³ : ${info['balance']:.2f}")
        print(f"âœ… ë³´ìœ ëŸ‰: {info['shares_held']}")
        print(f"âœ… í˜„ì¬ê°€: ${info['current_price']:.2f}")
        print(f"âœ… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: ${info['portfolio_value']:.2f}")
        print(f"âœ… ì´ ìˆ˜ìµë¥ : {info['total_return'] * 100:.2f}%")
        print(f"âœ… ì´ ìˆ˜ìˆ˜ë£Œ: ${info['total_commission']:.2f}")
        # ğŸ†• ë¹„ìœ¨ ì •ë³´ ì¶”ê°€
        print(f"âœ… ì£¼ì‹ ë¹„ìœ¨: {info['stock_ratio']:.1%}")
        print(f"âœ… í˜„ê¸ˆ ë¹„ìœ¨: {info['cash_ratio']:.1%}")
        print("-" * 50)
    
    def get_episode_data(self) -> Dict[str, List]:
        """ì—í”¼ì†Œë“œ ë°ì´í„° ë°˜í™˜"""
        return {
            'actions': self.actions_history,
            'rewards': self.rewards_history,
            'portfolio_values': self.portfolio_values_history
        }
    
    def get_final_portfolio_value(self) -> float:
        """ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë°˜í™˜"""
        return self._get_portfolio_value()
    
    def get_total_reward(self) -> float:
        """ì´ ë³´ìƒ ë°˜í™˜"""
        return sum(self.rewards_history)
    
    def get_reward_analysis(self) -> Dict[str, Any]:
        """ë³´ìƒ ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
        if not self.trade_effects_history:
            return {}
        
        trade_effects = np.array(self.trade_effects_history)
        market_effects = np.array(self.market_effects_history)
        
        return {
            'trade_effects': {
                'mean': np.mean(trade_effects),
                'std': np.std(trade_effects),
                'sum': np.sum(trade_effects),
                'positive_ratio': np.mean(trade_effects > 0)
            },
            'market_effects': {
                'mean': np.mean(market_effects),
                'std': np.std(market_effects),
                'sum': np.sum(market_effects),
                'positive_ratio': np.mean(market_effects > 0)
            },
            'correlation': np.corrcoef(trade_effects, market_effects)[0, 1] if len(trade_effects) > 1 else 0
        }

    def set_log_level(self, level: str):
        """ë¡œê·¸ ë ˆë²¨ ë³€ê²½"""
        if level in ['minimal', 'normal', 'detailed']:
            self.log_level = level
            if level != 'minimal':
                print(f"ë¡œê·¸ ë ˆë²¨ ë³€ê²½: {level}")


def create_environment_from_results(results: Dict[str, Dict[str, Any]], symbol: str, data_type: str = 'test', 
                                  log_level: str = 'normal', **kwargs) -> TradingEnvironment:
    """
    DataProcessor ê²°ê³¼ë¡œë¶€í„° í™˜ê²½ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    """
    if symbol not in results:
        raise ValueError(f"ì‹¬ë³¼ {symbol}ì„ ê²°ê³¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    result = results[symbol]
    
    if data_type not in result:
        raise ValueError(f"ë°ì´í„° íƒ€ì… {data_type}ì„ ì‹¬ë³¼ {symbol}ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    normalized_data = result[data_type]
    featured_data = result['featured_data']
    
    if data_type == 'train':
        raw_data = featured_data.iloc[:len(normalized_data)]
    elif data_type == 'valid':
        train_len = len(result['train'])
        raw_data = featured_data.iloc[train_len:train_len + len(normalized_data)]
    else:  # test
        train_len = len(result['train'])
        valid_len = len(result['valid'])
        raw_data = featured_data.iloc[train_len + valid_len:train_len + valid_len + len(normalized_data)]
    
    print(f"âœ… í™˜ê²½ ìƒì„±: {symbol} - {data_type} (ë¡œê·¸ ë ˆë²¨: {log_level})")
    print(f"âœ… ì •ê·œí™” ë°ì´í„°: {normalized_data.shape}")
    print(f"âœ… ì›ë³¸ ë°ì´í„°: {raw_data.shape}")
    
    env = TradingEnvironment(
        data=normalized_data,
        raw_data=raw_data,
        symbol=symbol,
        train_data=(data_type == 'train'),
        log_level=log_level,
        **kwargs
    )
    return env    

class MultiAssetTradingEnvironment:
    """ğŸ¢ ë‹¤ì¤‘ ìì‚° íŠ¸ë ˆì´ë”© í™˜ê²½ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        results: Dict[str, Dict[str, Any]],
        symbols: List[str],
        data_type: str = 'test',
        window_size: int = WINDOW_SIZE,
        initial_balance: float = INITIAL_BALANCE,
        max_trading_units: int = MAX_TRADING_UNITS,
        transaction_fee_percent: float = TRANSACTION_FEE_PERCENT,
        reward_mode: str = 'combined',
        log_level: str = 'normal'
    ):
        """MultiAssetTradingEnvironment í´ë˜ìŠ¤ ì´ˆê¸°í™”"""
        self.symbols = symbols
        self.n_assets = len(symbols)
        self.data_type = data_type
        self.initial_balance = initial_balance
        
        print(f"ğŸ¢ ë‹¤ì¤‘ ìì‚° í™˜ê²½ ì´ˆê¸°í™”: {self.n_assets}ê°œ ìì‚°")
        
        # ğŸ­ ê°œë³„ í™˜ê²½ ìƒì„±
        self.envs = {}
        for symbol in symbols:
            print(f"ğŸ“Š {symbol} í™˜ê²½ ìƒì„± ì¤‘...")
            self.envs[symbol] = create_environment_from_results(
                results=results,
                symbol=symbol,
                data_type=data_type,
                window_size=window_size,
                initial_balance=initial_balance / self.n_assets,
                max_trading_units=max_trading_units,
                transaction_fee_percent=transaction_fee_percent,
                reward_mode=reward_mode,
                log_level=log_level
            )
        
        # ğŸ® í–‰ë™ ê³µê°„: ê° ìì‚°ì— ëŒ€í•œ ì—°ì† í–‰ë™
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(self.n_assets,), dtype=np.float32
        )
        
        # ğŸ‘€ ê´€ì¸¡ ê³µê°„
        self.observation_space = spaces.Dict({
            symbol: env.observation_space for symbol, env in self.envs.items()
        })
        
        LOGGER.info(f"âœ… ë‹¤ì¤‘ ìì‚° í™˜ê²½ ì´ˆê¸°í™” ì™„ë£Œ: {self.n_assets}ê°œ ìì‚°, {data_type} ë°ì´í„°")

    def reset(self) -> Dict[str, Dict[str, np.ndarray]]:
        """ğŸ”„ í™˜ê²½ ì´ˆê¸°í™”"""
        print(f"ğŸ”„ ë‹¤ì¤‘ ìì‚° í™˜ê²½ ë¦¬ì…‹")
        observations = {}
        for symbol, env in self.envs.items():
            observations[symbol] = env.reset()
        return observations
    
    def step(self, actions: Dict[str, float]) -> Tuple[Dict[str, Dict[str, np.ndarray]], float, bool, Dict[str, Any]]:
        """ğŸš€ í™˜ê²½ì—ì„œ í•œ ìŠ¤í… ì§„í–‰"""
        observations = {}
        rewards = {}
        dones = {}
        infos = {}
        
        print(f"\nğŸš€ ë‹¤ì¤‘ ìì‚° Step ì§„í–‰")
        
        for symbol, env in self.envs.items():
            action = actions.get(symbol, 0.0)
            print(f"ğŸ“ˆ {symbol} ì²˜ë¦¬ ì¤‘...")
            obs, rew, done, info = env.step(action)
            
            observations[symbol] = obs
            rewards[symbol] = rew
            dones[symbol] = done
            infos[symbol] = info
        
        # ì „ì²´ ë³´ìƒì€ ê° ìì‚°ì˜ ë³´ìƒ í‰ê· 
        total_reward = sum(rewards.values()) / self.n_assets
        
        # ëª¨ë“  ìì‚°ì˜ ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë˜ë©´ ì „ì²´ ì—í”¼ì†Œë“œ ì¢…ë£Œ
        done = all(dones.values())
        
        # ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚°
        total_portfolio_value = sum(info['portfolio_value'] for info in infos.values())
        trade_executed_any = any(info.get('trade_executed', False) for info in infos.values())
        
        infos['total'] = {
            'portfolio_value': total_portfolio_value,
            'total_return': (total_portfolio_value - self.initial_balance) / self.initial_balance,
            'trade_executed': trade_executed_any
        }

        return observations, total_reward, done, infos
    
    def set_log_level(self, level: str):
        """ëª¨ë“  í™˜ê²½ì˜ ë¡œê·¸ ë ˆë²¨ ë³€ê²½"""
        for env in self.envs.values():
            env.set_log_level(level)
    
    def render(self, mode: str = 'human') -> None:
        """ğŸ–¥ï¸ í™˜ê²½ ì‹œê°í™”"""
        total_portfolio_value = 0
        
        print("=" * 50)
        print("âœ… ë‹¤ì¤‘ ìì‚° íŠ¸ë ˆì´ë”© í™˜ê²½ ìƒíƒœ")
        print("=" * 50)
        
        for symbol, env in self.envs.items():
            info = env._get_info()
            total_portfolio_value += info['portfolio_value']
            
            print(f"âœ… ìì‚°: {symbol}")
            print(f"  âœ… ê°€ê²©: ${info['current_price']:.2f}")
            print(f"  âœ… ë³´ìœ ëŸ‰: {info['shares_held']:.3f}")
            print(f"  âœ… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: ${info['portfolio_value']:.2f}")
            print(f"  âœ… ìˆ˜ìµë¥ : {info['total_return'] * 100:.2f}%")
            print("-" * 50)
        
        total_return = (total_portfolio_value - self.initial_balance) / self.initial_balance
        print(f"âœ… ì´ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: ${total_portfolio_value:.2f}")
        print(f"âœ… ì´ ìˆ˜ìµë¥ : {total_return * 100:.2f}%")
        print("=" * 50)
    
    def get_final_portfolio_value(self) -> float:
        """ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë°˜í™˜"""
        return sum(env.get_final_portfolio_value() for env in self.envs.values())
    
    def get_total_reward(self) -> float:
        """ì´ ë³´ìƒ ë°˜í™˜"""
        return sum(env.get_total_reward() for env in self.envs.values()) / self.n_assets


def create_flexible_environment(results, symbol, reward_mode='combined', log_level='normal', 
                              detailed_logging=True, data_type='train'):
    """
    ğŸ”§ ìœ ì—°í•œ ë³´ìƒ ì‹œìŠ¤í…œì„ ê°€ì§„ í™˜ê²½ ìƒì„±
    """
    print(f" ìœ ì—°í•œ í™˜ê²½ ìƒì„±: {symbol} - {reward_mode} (ë¡œê·¸: {log_level})")
    env = create_environment_from_results(
        results=results,
        symbol=symbol,
        data_type=data_type,
        reward_mode=reward_mode,
        detailed_logging=detailed_logging,
        log_level=log_level
    )
    return env


if __name__ == "__main__":
    # ğŸ§ª ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import matplotlib.pyplot as plt
    from src.data_collection.data_collector import DataCollector
    from src.preprocessing.data_processor import DataProcessor
    from src.config.config import config
    import argparse
    
    parser = argparse.ArgumentParser(description="í…ŒìŠ¤íŠ¸í•  ì‹¬ë³¼")
    parser.add_argument("--symbols", nargs="+", help="í…ŒìŠ¤íŠ¸í•  ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸", default=config.trading_symbols)
    parser.add_argument("--log_level", choices=['minimal', 'normal', 'detailed'], default='normal', help="ë¡œê·¸ ë ˆë²¨")
    args = parser.parse_args()
    
    symbols = args.symbols
    TARGET_SYMBOL = symbols[0] if symbols else config.trading_symbols
    print(f'ğŸ“ˆ í…ŒìŠ¤íŠ¸ ì‹¬ë³¼ë“¤: {symbols}')
    print(f'ğŸ¯ ì£¼ìš” í…ŒìŠ¤íŠ¸ ì‹¬ë³¼: {TARGET_SYMBOL}')
    print(f'ğŸ“ ë¡œê·¸ ë ˆë²¨: {args.log_level}')
    
    # ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
    print("ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    collector = DataCollector(symbols=symbols)
    data = collector.load_all_data()
    
    try:
        if data:
            print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
            processor = DataProcessor()
            results = processor.process_all_symbols(data)
            
            if TARGET_SYMBOL in results:
                print("=" * 60)
                print(f"ğŸ§ª íš¨ìœ¨ì ì¸ ë¡œê¹… TradingEnvironment í…ŒìŠ¤íŠ¸ ì‹œì‘ - {TARGET_SYMBOL}")
                print("=" * 60)
                
                # ğŸ¯ ë‹¤ì–‘í•œ ë¡œê·¸ ë ˆë²¨ë¡œ í…ŒìŠ¤íŠ¸
                for log_level in ['minimal', 'normal', 'detailed']:
                    print(f"\nğŸ”„ ë¡œê·¸ ë ˆë²¨ í…ŒìŠ¤íŠ¸: {log_level}")
                    print("-" * 40)
                    
                    env = create_flexible_environment(
                        results=results,
                        symbol=TARGET_SYMBOL,
                        reward_mode='combined',
                        log_level=log_level,
                        detailed_logging=True,
                        data_type='train'
                    )
                    
                    obs = env.reset()
                    
                    # ğŸš€ 3ìŠ¤í… ì‹¤í–‰
                    for i in range(3):
                        action = np.random.uniform(-1.0, 1.0)
                        obs, reward, done, info = env.step(action)
                        
                        if done:
                            break
                    
                    print(f"âœ… {log_level} ë ˆë²¨ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                
                print("\n" + "=" * 60)
                print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
                print("=" * 60)
                
            else:
                print(f"âŒ {TARGET_SYMBOL} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¬ë³¼: {list(results.keys())}")
        else:
            print("âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"ğŸš¨ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()