"""
ê· í˜•ì¡íŒ íŠ¸ë ˆì´ë”© í™˜ê²½ ëª¨ë“ˆ (ë§¤ìˆ˜ í¸í–¥ ë¬¸ì œ í•´ê²°)
"""
import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Union, Optional, Any
import gym
from gym import spaces
from src.environment.trading_env import TradingEnvironment

from src.config.ea_teb_config import (
    INITIAL_BALANCE,
    MAX_TRADING_UNITS,
    TRANSACTION_FEE_PERCENT,
    WINDOW_SIZE,
    LOGGER,
)


class BalancedTradingEnvironment(TradingEnvironment):
    """
    ê· í˜•ì¡íŒ íŠ¸ë ˆì´ë”© í™˜ê²½ í´ë˜ìŠ¤ (ë§¤ìˆ˜ í¸í–¥ ë¬¸ì œ í•´ê²°)
    """

    def __init__(self, *args, **kwargs):
        """ì´ˆê¸°í™”"""
        # ê· í˜•ì¡íŒ ì´ˆê¸° ìƒíƒœ ì„¤ì • (ë¶€ëª¨ í´ë˜ìŠ¤ì— ì „ë‹¬í•˜ì§€ ì•ŠìŒ)
        self.balanced_initialization = kwargs.pop('balanced_initialization', True)
        
        super().__init__(*args, **kwargs)
        
        LOGGER.info(f"ğŸ¯ ê· í˜•ì¡íŒ íŠ¸ë ˆì´ë”© í™˜ê²½ ì´ˆê¸°í™” (ê· í˜• ì‹œì‘: {self.balanced_initialization})")

    def reset(self) -> Dict[str, np.ndarray]:
        """ğŸ”„ í™˜ê²½ ì´ˆê¸°í™” (ê· í˜•ì¡íŒ ì‹œì‘ì )"""
        if self.log_level != 'minimal':
            print(f"\nğŸ”„ ê· í˜•ì¡íŒ í™˜ê²½ ë¦¬ì…‹ ì‹œì‘ - {self.symbol}")
        
        # ê¸°ë³¸ ì´ˆê¸°í™”
        self._init_state_variables()
        
        # ê· í˜•ì¡íŒ ì´ˆê¸° í¬ì§€ì…˜ ì„¤ì •
        if self.balanced_initialization:
            current_price = self._get_current_price()
            
            # 50% í˜„ê¸ˆ, 50% ì£¼ì‹ìœ¼ë¡œ ì‹œì‘
            target_stock_value = self.initial_balance * 0.5
            shares_to_buy = target_stock_value / current_price
            cost = shares_to_buy * current_price
            
            self.balance = self.initial_balance - cost
            self.shares_held = shares_to_buy
            self.cost_basis = current_price
            
            if self.log_level != 'minimal':
                print(f"ğŸ’° ê· í˜•ì¡íŒ ì‹œì‘: í˜„ê¸ˆ ${self.balance:.2f}, ì£¼ì‹ ${shares_to_buy:.3f}ì£¼")
        
        return self._get_observation()

    def _execute_trade_action(self, action: float) -> None:
        """ê°œì„ ëœ ê±°ë˜ í–‰ë™ ì‹¤í–‰ (ê· í˜•ì¡íŒ í•´ì„)"""
        self.trade_executed = False
        self.previous_shares_held = self.shares_held
        self.invalid_sell_penalty = False
        
        # ğŸ†• ê°œì„ ëœ í–‰ë™ í•´ì„: actionì„ ìƒëŒ€ì  ë³€í™”ë¡œ í•´ì„
        current_ratio = self._get_current_stock_ratio()
        
        # actionì„ [-1, 1]ì—ì„œ [0, 1]ë¡œ ë§¤í•‘ (ì‹œê·¸ëª¨ì´ë“œ ë³€í™˜)
        target_ratio = 1 / (1 + np.exp(-action * 2))  # ì‹œê·¸ëª¨ì´ë“œë¡œ [0, 1] ë§¤í•‘
        
        # ë˜ëŠ” ë” ì§ê´€ì ì¸ ì„ í˜• ë§¤í•‘ë„ ê°€ëŠ¥
        # target_ratio = (action + 1) / 2  # [-1, 1] -> [0, 1]
        
        current_price = self._get_current_price()
        portfolio_value = self._get_portfolio_value()
        
        ratio_diff = target_ratio - current_ratio
        
        # ë” ì—„ê²©í•œ ì„ê³„ê°’ (ë” ì„ ë³„ì ì¸ ê±°ë˜)
        threshold = 0.02  # 2% ì´ìƒ ì°¨ì´ë‚  ë•Œë§Œ ê±°ë˜
        
        if abs(ratio_diff) > threshold:
            if self.log_level == 'detailed':
                print(f"   ğŸ¯ ê· í˜•ì¡íŒ ê±°ë˜: action={action:.3f} -> ëª©í‘œë¹„ìœ¨ {target_ratio:.3f}, í˜„ì¬ {current_ratio:.3f}")
            
            if ratio_diff > 0:
                self._execute_buy_by_ratio(ratio_diff, current_price, portfolio_value)
            else:
                self._execute_sell_by_ratio(-ratio_diff, current_price, portfolio_value)
        else:
            if self.log_level == 'detailed':
                print(f"   ğŸ’¤ ê±°ë˜ ì—†ìŒ: ë¹„ìœ¨ ì°¨ì´ {abs(ratio_diff):.4f} < ì„ê³„ê°’ {threshold:.4f}")

    def _calculate_reward(self, prev_portfolio_value: float, portfolio_after_trade: float, 
                         current_portfolio_value: float) -> Union[float, Dict[str, float]]:
        """ğŸ ê· í˜•ì¡íŒ ë³´ìƒ ê³„ì‚° (ë§¤ìˆ˜ í¸í–¥ ì œê±°)"""
        if prev_portfolio_value <= 0:
            prev_portfolio_value = max(self.balance, 1.0)
        
        # ê¸°ë³¸ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  (ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œ)
        if prev_portfolio_value > 0:
            portfolio_return = (current_portfolio_value - prev_portfolio_value) / prev_portfolio_value
        else:
            portfolio_return = 0
        
        # ğŸ†• ê· í˜•ì¡íŒ í¬ì§€ì…˜ ë³´ìƒ (ë§¤ìˆ˜ í¸í–¥ ì œê±°)
        current_stock_ratio = self._get_current_stock_ratio()
        
        # ì´ìƒì ì¸ ê· í˜•: 40-60% ì‚¬ì´ê°€ ì¢‹ìŒ (ë” ë„“ì€ ë²”ìœ„)
        if 0.4 <= current_stock_ratio <= 0.6:
            balance_bonus = 0.02  # ì‘ì€ ë³´ë„ˆìŠ¤
        elif 0.2 <= current_stock_ratio <= 0.8:
            balance_bonus = 0.01  # ë§¤ìš° ì‘ì€ ë³´ë„ˆìŠ¤
        elif current_stock_ratio < 0.1 or current_stock_ratio > 0.9:
            balance_bonus = -0.05  # ê·¹ë‹¨ì  í¬ì§€ì…˜ì— íŒ¨ë„í‹°
        else:
            balance_bonus = 0  # ì¤‘ë¦½
        
        # ğŸ†• ê±°ë˜ ë¹ˆë„ ì¡°ì ˆ (ê³¼ë„í•œ ê±°ë˜ ë°©ì§€)
        trade_penalty = 0
        if self.trade_executed:
            # ê±°ë˜í•  ë•Œë§ˆë‹¤ ì‘ì€ ë¹„ìš© (í˜„ì‹¤ì ì¸ ìŠ¬ë¦¬í”¼ì§€ ë°˜ì˜)
            trade_penalty = -0.001
        
        # ğŸ†• ë³€ë™ì„± ê¸°ë°˜ ë³´ìƒ ì¡°ì •
        volatility_adjustment = 0
        if hasattr(self, 'portfolio_values_history') and len(self.portfolio_values_history) > 10:
            recent_values = self.portfolio_values_history[-10:]
            volatility = np.std(recent_values) / np.mean(recent_values) if np.mean(recent_values) > 0 else 0
            
            # ì ë‹¹í•œ ë³€ë™ì„±ì€ ì¢‹ì§€ë§Œ, ë„ˆë¬´ ë†’ìœ¼ë©´ íŒ¨ë„í‹°
            if volatility > 0.05:  # 5% ì´ìƒ ë³€ë™ì„±
                volatility_adjustment = -0.02
        
        # ìµœì¢… ë³´ìƒ ê³„ì‚° (í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ ì´ ê°€ì¥ ì¤‘ìš”)
        base_reward = np.tanh(portfolio_return * 100)  # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê¸°ë°˜
        
        total_reward = base_reward + balance_bonus + trade_penalty + volatility_adjustment
        
        # íŒ¨ë„í‹° ì ìš©
        if self.invalid_sell_penalty:
            total_reward -= 0.1  # ë” ê°€ë²¼ìš´ íŒ¨ë„í‹°
        
        # í´ë¦¬í•‘
        total_reward = np.clip(total_reward, -1.0, 1.0)
        
        if self.log_level == 'detailed':
            print(f"   ğŸ“Š ë³´ìƒ êµ¬ì„±: ê¸°ë³¸={base_reward:.4f}, ê· í˜•={balance_bonus:.4f}, "
                  f"ê±°ë˜={trade_penalty:.4f}, ë³€ë™ì„±={volatility_adjustment:.4f}")
        
        return total_reward


def create_balanced_environment_from_results(results: Dict[str, Dict[str, Any]], symbol: str, 
                                           data_type: str = 'test', **kwargs) -> BalancedTradingEnvironment:
    """
    ê· í˜•ì¡íŒ í™˜ê²½ ìƒì„± í—¬í¼ í•¨ìˆ˜
    """
    if symbol not in results:
        raise ValueError(f"ì‹¬ë³¼ {symbol}ì„ ê²°ê³¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    result = results[symbol]
    
    if data_type not in result:
        raise ValueError(f"ë°ì´í„° íƒ€ì… {data_type}ì„ ì‹¬ë³¼ {symbol}ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    normalized_data = result[data_type]
    featured_data = result['featured_data']
    
    if data_type == 'train':
        raw_data = featured_data
    elif data_type == 'valid':
        raw_data = featured_data
    else:  # test
        raw_data = featured_data
    
    # í™˜ê²½ ìƒì„±
    env = BalancedTradingEnvironment(
        data=normalized_data,
        raw_data=raw_data,
        symbol=symbol,
        train_data=(data_type == 'train'),
        **kwargs
    )
    
    LOGGER.info(f"âœ… ê· í˜•ì¡íŒ {symbol} í™˜ê²½ ìƒì„± ì™„ë£Œ ({data_type} ë°ì´í„°)")
    return env


# ê¸°ì¡´ í™˜ê²½ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼
def create_environment_from_results_balanced(results: Dict[str, Dict[str, Any]], symbol: str, 
                                           data_type: str = 'test', use_balanced: bool = True, 
                                           **kwargs):
    """
    ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ë˜ëŠ” í™˜ê²½ ìƒì„± í•¨ìˆ˜
    """
    if use_balanced:
        return create_balanced_environment_from_results(results, symbol, data_type, **kwargs)
    else:
        # ê¸°ì¡´ í™˜ê²½ ì‚¬ìš©
        from src.environment.trading_env import create_environment_from_results
        return create_environment_from_results(results, symbol, data_type, **kwargs) 