"""
SAC ì•Œê³ ë¦¬ì¦˜ì˜ Actorì™€ Critic ë„¤íŠ¸ì›Œí¬ êµ¬í˜„ ëª¨ë“ˆ (LSTM + Mamba ì¶”ê°€ ë²„ì „)
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Dict, List, Union, Optional
import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

from src.config.ea_teb_config import (
    HIDDEN_DIM,
    DEVICE,
    LOGGER,
    WINDOW_SIZE
)

class ActorNetwork(nn.Module):
    """
    SAC ì•Œê³ ë¦¬ì¦˜ì˜ Actor ë„¤íŠ¸ì›Œí¬ (ì •ì±… ë„¤íŠ¸ì›Œí¬)
    ì—°ì†ì ì¸ í–‰ë™ ê³µê°„ì— ëŒ€í•œ í™•ë¥ ì  ì •ì±…ì„ ëª¨ë¸ë§
    """
    
    def __init__(
        self,
        state_dim: int,
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        log_std_min: float = -2.0,
        log_std_max: float = 2.0,
        device: torch.device = DEVICE
    ):
        """
        ActorNetwork í´ëž˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            state_dim: ìƒíƒœ ê³µê°„ì˜ ì°¨ì›
            action_dim: í–‰ë™ ê³µê°„ì˜ ì°¨ì›
            hidden_dim: ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜
            log_std_min: ë¡œê·¸ í‘œì¤€íŽ¸ì°¨ì˜ ìµœì†Œê°’
            log_std_max: ë¡œê·¸ í‘œì¤€íŽ¸ì°¨ì˜ ìµœëŒ€ê°’
            device: ëª¨ë¸ì´ ì‹¤í–‰ë  ìž¥ì¹˜
        """
        super(ActorNetwork, self).__init__()
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.device = device
        
        # ê³µí†µ íŠ¹ì„± ì¶”ì¶œ ë ˆì´ì–´
        print(f'state_dim : {state_dim}')
        print(f'hidden_dim : {hidden_dim}')
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        
        # í‰ê· ê°’ ì¶œë ¥ ë ˆì´ì–´
        self.mean = nn.Linear(hidden_dim, action_dim)
        
        # ë¡œê·¸ í‘œì¤€íŽ¸ì°¨ ì¶œë ¥ ë ˆì´ì–´
        self.log_std = nn.Linear(hidden_dim, action_dim)
        
        # ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        self.to(device)
        
        LOGGER.info(f"Actor ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ: ìƒíƒœ ì°¨ì› {state_dim}, í–‰ë™ ì°¨ì› {action_dim}")
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ìˆœì „íŒŒ í•¨ìˆ˜
        
        Args:
            state: ìƒíƒœ í…ì„œ
            
        Returns:
            í‰ê· ê³¼ ë¡œê·¸ í‘œì¤€íŽ¸ì°¨ í…ì„œì˜ íŠœí”Œ
        """
        # ìž…ë ¥ ì°¨ì› ê²€ì‚¬ ë° ì¡°ì •
        if state.size(1) != self.state_dim:
            print(f"ê²½ê³ : ìž…ë ¥ ìƒíƒœ ì°¨ì› ({state.size(1)})ì´ ëª¨ë¸ì˜ ìƒíƒœ ì°¨ì› ({self.state_dim})ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            # ì°¨ì›ì´ ë” í° ê²½ìš° ìžë¥´ê¸°
            if state.size(1) > self.state_dim:
                state = state[:, :self.state_dim]
                print(f"ìž…ë ¥ì„ {self.state_dim} ì°¨ì›ìœ¼ë¡œ ìž˜ëžìŠµë‹ˆë‹¤.")
            # ì°¨ì›ì´ ë” ìž‘ì€ ê²½ìš° íŒ¨ë”©
            else:
                padding = torch.zeros(state.size(0), self.state_dim - state.size(1), device=state.device)
                state = torch.cat([state, padding], dim=1)
                print(f"ìž…ë ¥ì„ {self.state_dim} ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©í–ˆìŠµë‹ˆë‹¤.")
        
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        
        # í‰ê· ê°’ ê³„ì‚°
        mean = self.mean(x)
        
        # ë¡œê·¸ í‘œì¤€íŽ¸ì°¨ ê³„ì‚° ë° í´ë¦¬í•‘
        log_std = self.log_std(x)
        log_std = torch.clamp(log_std, min=self.log_std_min, max=self.log_std_max)
        
        return mean, log_std
    
    def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ìƒíƒœì—ì„œ í–‰ë™ì„ ìƒ˜í”Œë§
        
        Args:
            state: ìƒíƒœ í…ì„œ
            
        Returns:
            (í–‰ë™, ë¡œê·¸ í™•ë¥ , í‰ê· ) íŠœí”Œ
        """
        mean, log_std = self.forward(state)
        std = log_std.exp()
        
        # ìž¬ë§¤ê°œë³€ìˆ˜í™” íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ ì •ê·œ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()  # ìž¬ë§¤ê°œë³€ìˆ˜í™”ëœ ìƒ˜í”Œ
        
        # Tanh ë³€í™˜ì„ í†µí•´ í–‰ë™ ë²”ìœ„ ì œí•œ (-1, 1)
        y_t = torch.tanh(x_t)
        
        # ì •ì±…ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚°
        log_prob = normal.log_prob(x_t) - torch.log(1 - y_t.pow(2) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        
        return y_t, log_prob, mean
    
    def to(self, device: torch.device) -> 'ActorNetwork':
        """
        ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        
        Args:
            device: ì´ë™í•  ìž¥ì¹˜
            
        Returns:
            ìž¥ì¹˜ë¡œ ì´ë™ëœ ëª¨ë¸
        """
        self.device = device
        return super(ActorNetwork, self).to(device)


class CriticNetwork(nn.Module):
    """
    SAC ì•Œê³ ë¦¬ì¦˜ì˜ Critic ë„¤íŠ¸ì›Œí¬ (Q-í•¨ìˆ˜)
    ìƒíƒœ-í–‰ë™ ìŒì˜ ê°€ì¹˜ë¥¼ í‰ê°€
    """
    
    def __init__(
        self,
        state_dim: int,
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        device: torch.device = DEVICE
    ):
        """
        CriticNetwork í´ëž˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            state_dim: ìƒíƒœ ê³µê°„ì˜ ì°¨ì›
            action_dim: í–‰ë™ ê³µê°„ì˜ ì°¨ì›
            hidden_dim: ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜
            device: ëª¨ë¸ì´ ì‹¤í–‰ë  ìž¥ì¹˜
        """
        super(CriticNetwork, self).__init__()
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.device = device
        
        # Q1 ë„¤íŠ¸ì›Œí¬
        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.q1 = nn.Linear(hidden_dim, 1)
        
        # Q2 ë„¤íŠ¸ì›Œí¬ (Double Q-learning)
        self.fc4 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc5 = nn.Linear(hidden_dim, hidden_dim)
        self.fc6 = nn.Linear(hidden_dim, hidden_dim)
        self.q2 = nn.Linear(hidden_dim, 1)
        
        # ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        self.to(device)
        
        LOGGER.info(f"Critic ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ: ìƒíƒœ ì°¨ì› {state_dim}, í–‰ë™ ì°¨ì› {action_dim}")
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ìˆœì „íŒŒ í•¨ìˆ˜
        
        Args:
            state: ìƒíƒœ í…ì„œ
            action: í–‰ë™ í…ì„œ
            
        Returns:
            ë‘ Q ê°’ì˜ íŠœí”Œ
        """
        # ìž…ë ¥ ì°¨ì› ê²€ì‚¬ ë° ì¡°ì •
        if state.size(1) != self.state_dim:
            print(f"ê²½ê³ : ìž…ë ¥ ìƒíƒœ ì°¨ì› ({state.size(1)})ì´ ëª¨ë¸ì˜ ìƒíƒœ ì°¨ì› ({self.state_dim})ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            # ì°¨ì›ì´ ë” í° ê²½ìš° ìžë¥´ê¸°
            if state.size(1) > self.state_dim:
                state = state[:, :self.state_dim]
                print(f"ìž…ë ¥ì„ {self.state_dim} ì°¨ì›ìœ¼ë¡œ ìž˜ëžìŠµë‹ˆë‹¤.")
            # ì°¨ì›ì´ ë” ìž‘ì€ ê²½ìš° íŒ¨ë”©
            else:
                padding = torch.zeros(state.size(0), self.state_dim - state.size(1), device=state.device)
                state = torch.cat([state, padding], dim=1)
                print(f"ìž…ë ¥ì„ {self.state_dim} ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©í–ˆìŠµë‹ˆë‹¤.")
        
        # ìƒíƒœì™€ í–‰ë™ì„ ì—°ê²°
        sa = torch.cat([state, action], 1)
        
        # Q1 ê°’ ê³„ì‚°
        q1 = F.relu(self.fc1(sa))
        q1 = F.relu(self.fc2(q1))
        q1 = F.relu(self.fc3(q1))
        q1 = self.q1(q1)
        
        # Q2 ê°’ ê³„ì‚°
        q2 = F.relu(self.fc4(sa))
        q2 = F.relu(self.fc5(q2))
        q2 = F.relu(self.fc6(q2))
        q2 = self.q2(q2)
        
        return q1, q2
    
    def to(self, device: torch.device) -> 'CriticNetwork':
        """
        ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        
        Args:
            device: ì´ë™í•  ìž¥ì¹˜
            
        Returns:
            ìž¥ì¹˜ë¡œ ì´ë™ëœ ëª¨ë¸
        """
        self.device = device
        return super(CriticNetwork, self).to(device)


class CNNActorNetwork(nn.Module):
    """
    ê°œì„ ëœ CNN ê¸°ë°˜ Actor ë„¤íŠ¸ì›Œí¬
    - ë” ê¹Šì€ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
    - Batch Normalizationê³¼ Dropout ì¶”ê°€
    - Residual Connection ì ìš©
    """
    def __init__(
        self,
        input_shape: Tuple[int, int],  # (window_size, feature_dim)
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        log_std_min: float = -2.0,
        log_std_max: float = 2.0,
        dropout_rate: float = 0.1,
        device: torch.device = DEVICE
    ):
        super(CNNActorNetwork, self).__init__()
        
        self.window_size, self.feature_dim = input_shape
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.device = device

        # ðŸ†• ë™ì  ì°¨ì› ì¡°ì • ì„¤ì •
        self.expected_feature_dim = self.feature_dim  # ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì°¨ì›
        self.adaptive_input = True  # ë™ì  ìž…ë ¥ ì¡°ì • í™œì„±í™”
        
        LOGGER.info(f"ðŸ–¼ï¸ CNN Actor ì´ˆê¸°í™”: ê¸°ëŒ€ íŠ¹ì„± ì°¨ì› {self.expected_feature_dim}")
        
        # ê°œì„ ëœ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ë“¤
        self.conv_block1 = self._make_conv_block(self.feature_dim, 64, 3, 1, dropout_rate)
        self.conv_block2 = self._make_conv_block(64, 128, 3, 1, dropout_rate)
        self.conv_block3 = self._make_conv_block(128, 256, 3, 1, dropout_rate)
        
        # âœ… ë™ì  Adaptive pooling í¬ê¸° ê³„ì‚°
        # MaxPool stride=2ê°€ 3ë²ˆ ì ìš©ë˜ë¯€ë¡œ ê¸¸ì´ê°€ 1/8ë¡œ ê°ì†Œ
        pooled_length = max(4, self.window_size // 8)  # ìµœì†Œ 4, ìµœëŒ€ window_size//8
        self.adaptive_pool = nn.AdaptiveAvgPool1d(pooled_length)
        
        # âœ… ë™ì  ì»¨ë³¼ë£¨ì…˜ ì¶œë ¥ í¬ê¸° ê³„ì‚°
        self.conv_output_size = 256 * pooled_length
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì²˜ë¦¬
        self.portfolio_fc = nn.Sequential(
            nn.Linear(2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # âœ… ê°œì„ ëœ íŠ¹ì„± ìœµí•© ë„¤íŠ¸ì›Œí¬
        fusion_input_size = self.conv_output_size + hidden_dim // 4  # ë‹¨ìˆœ concat
        self.fusion_network = nn.Sequential(
            nn.Linear(fusion_input_size, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
        self.to(device)
        LOGGER.info(f"CNN Actor ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ: ìž…ë ¥ í˜•íƒœ {input_shape}, í’€ë§ í¬ê¸° {pooled_length}")
    
    def _make_conv_block(self, in_channels, out_channels, kernel_size, stride, dropout_rate):
        """ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ ìƒì„±"""
        return nn.Sequential(
            nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.MaxPool1d(2, stride=2, padding=0)  # ê²½ë¯¸í•œ ë‹¤ìš´ìƒ˜í”Œë§
        )
    
    def _initialize_weights(self):
        """Xavier ì´ˆê¸°í™” ì ìš©"""
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """ê°œì„ ëœ ìˆœì „íŒŒ"""
        market_data = state['market_data']
        portfolio_state = state['portfolio_state']
        
        # ðŸ†• ë™ì  ìž…ë ¥ ì°¨ì› ì¡°ì •
        if self.adaptive_input:
            market_data = self._adjust_input_dimensions(market_data)
        
        # ì°¨ì› ì²˜ë¦¬
        if len(market_data.shape) == 3:  # (B, W, F)
            market_data = market_data.permute(0, 2, 1)  # (B, F, W)
        elif len(market_data.shape) == 2:  # (W, F)
            market_data = market_data.unsqueeze(0).permute(0, 2, 1)  # (1, F, W)
        
        if len(portfolio_state.shape) == 1:
            portfolio_state = portfolio_state.unsqueeze(0)
        
        # ì»¨ë³¼ë£¨ì…˜ ì²˜ë¦¬
        x = self.conv_block1(market_data)
        x = self.conv_block2(x)
        x = self.conv_block3(x)
        
        # Adaptive pooling
        x = self.adaptive_pool(x)
        market_features = x.view(x.size(0), -1)  # í‰íƒ„í™”
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì²˜ë¦¬
        portfolio_features = self.portfolio_fc(portfolio_state)
        
        # ë‹¨ìˆœí•œ íŠ¹ì„± ìœµí•© (concat)
        combined_features = torch.cat([market_features, portfolio_features], dim=1)
        
        # ìµœì¢… íŠ¹ì„± ì²˜ë¦¬
        fused_features = self.fusion_network(combined_features)
        
        # ì¶œë ¥ ê³„ì‚°
        mean = self.mean(fused_features)
        log_std = self.log_std(fused_features)
        log_std = torch.clamp(log_std, min=self.log_std_min, max=self.log_std_max)
        
        return mean, log_std
    
    def sample(self, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """í–‰ë™ ìƒ˜í”Œë§"""
        mean, log_std = self.forward(state)
        std = log_std.exp()
        
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()
        y_t = torch.tanh(x_t)
        
        log_prob = normal.log_prob(x_t) - torch.log(1 - y_t.pow(2) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        
        return y_t, log_prob, mean
    
    def _adjust_input_dimensions(self, market_data: torch.Tensor) -> torch.Tensor:
        """
        ìž…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì„ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ë™ì  ì¡°ì •
        
        Args:
            market_data: ìž…ë ¥ ë§ˆì¼“ ë°ì´í„° (B, W, F) ë˜ëŠ” (W, F)
            
        Returns:
            ì¡°ì •ëœ ë§ˆì¼“ ë°ì´í„°
        """
        # í˜„ìž¬ ìž…ë ¥ì˜ íŠ¹ì„± ì°¨ì› í™•ì¸
        if len(market_data.shape) == 3:  # (B, W, F)
            current_feature_dim = market_data.shape[2]
            batch_size, window_size = market_data.shape[0], market_data.shape[1]
        elif len(market_data.shape) == 2:  # (W, F)
            current_feature_dim = market_data.shape[1]
            batch_size, window_size = 1, market_data.shape[0]
            market_data = market_data.unsqueeze(0)  # (1, W, F)ë¡œ ë³€í™˜
        else:
            LOGGER.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ìž…ë ¥ í˜•íƒœ: {market_data.shape}")
            return market_data
        
        # ì°¨ì›ì´ ì¼ì¹˜í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if current_feature_dim == self.expected_feature_dim:
            return market_data
        
        # ì°¨ì› ì¡°ì • í•„ìš”
        if current_feature_dim > self.expected_feature_dim:
            # íŠ¹ì„±ì´ ë” ë§Žì€ ê²½ìš°: ì•žì—ì„œë¶€í„° ìžë¥´ê¸°
            adjusted_data = market_data[:, :, :self.expected_feature_dim]
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› ì¶•ì†Œ: {current_feature_dim} â†’ {self.expected_feature_dim}")
            
        else:
            # íŠ¹ì„±ì´ ë” ì ì€ ê²½ìš°: ì œë¡œ íŒ¨ë”©
            padding_size = self.expected_feature_dim - current_feature_dim
            padding = torch.zeros(batch_size, window_size, padding_size, 
                                device=market_data.device, dtype=market_data.dtype)
            adjusted_data = torch.cat([market_data, padding], dim=2)
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› í™•ìž¥: {current_feature_dim} â†’ {self.expected_feature_dim} (ì œë¡œ íŒ¨ë”©)")
        
        return adjusted_data

    def get_expected_input_shape(self) -> Tuple[int, int]:
        """ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ í˜•íƒœ ë°˜í™˜"""
        return (self.window_size, self.expected_feature_dim)

    def set_adaptive_input(self, enabled: bool):
        """ë™ì  ìž…ë ¥ ì¡°ì • ê¸°ëŠ¥ í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.adaptive_input = enabled
        LOGGER.info(f"ðŸ”§ CNN ë™ì  ìž…ë ¥ ì¡°ì •: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")

    def update_expected_dimensions(self, new_feature_dim: int, new_window_size: int = None):
        """ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ ì°¨ì› ì—…ë°ì´íŠ¸ (ëª¨ë¸ ë¡œë“œ ì‹œ ì‚¬ìš©)"""
        old_feature_dim = self.expected_feature_dim
        self.expected_feature_dim = new_feature_dim
        
        if new_window_size:
            self.window_size = new_window_size
        
        LOGGER.info(f"ðŸ”§ CNN ê¸°ëŒ€ ì°¨ì› ì—…ë°ì´íŠ¸: íŠ¹ì„± {old_feature_dim} â†’ {new_feature_dim}")
        
        # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ìž¬êµ¬ì„±ì´ í•„ìš”í•œ ê²½ìš° ê²½ê³ 
        if old_feature_dim != new_feature_dim:
            LOGGER.warning("âš ï¸ íŠ¹ì„± ì°¨ì› ë³€ê²½ìœ¼ë¡œ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ìž¬ì´ˆê¸°í™” í•„ìš”")
    
    def to(self, device: torch.device) -> 'CNNActorNetwork':
        """
        ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        
        Args:
            device: ì´ë™í•  ìž¥ì¹˜
            
        Returns:
            ìž¥ì¹˜ë¡œ ì´ë™ëœ ëª¨ë¸
        """
        self.device = device
        return super(CNNActorNetwork, self).to(device)


class CNNCriticNetwork(nn.Module):
    """
    ê°œì„ ëœ CNN ê¸°ë°˜ Critic ë„¤íŠ¸ì›Œí¬
    """
    def __init__(
        self,
        input_shape: Tuple[int, int],
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        dropout_rate: float = 0.1,
        device: torch.device = DEVICE
    ):
        super(CNNCriticNetwork, self).__init__()
        
        self.window_size, self.feature_dim = input_shape
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.device = device
        
        # ðŸ†• ë™ì  ì°¨ì› ì¡°ì • ì„¤ì •
        self.expected_feature_dim = self.feature_dim  # ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì°¨ì›
        self.adaptive_input = True  # ë™ì  ìž…ë ¥ ì¡°ì • í™œì„±í™”
        
        LOGGER.info(f"ðŸ–¼ï¸ CNN Actor ì´ˆê¸°í™”: ê¸°ëŒ€ íŠ¹ì„± ì°¨ì› {self.expected_feature_dim}")
        
        # Q1, Q2 ë„¤íŠ¸ì›Œí¬ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ë“¤ (ë™ì¼í•œ _make_conv_block ì‚¬ìš©)
        self.q1_conv_block1 = self._make_conv_block(self.feature_dim, 64, 3, 1, dropout_rate)
        self.q1_conv_block2 = self._make_conv_block(64, 128, 3, 1, dropout_rate)
        self.q1_conv_block3 = self._make_conv_block(128, 256, 3, 1, dropout_rate)
        
        self.q2_conv_block1 = self._make_conv_block(self.feature_dim, 64, 3, 1, dropout_rate)
        self.q2_conv_block2 = self._make_conv_block(64, 128, 3, 1, dropout_rate)
        self.q2_conv_block3 = self._make_conv_block(128, 256, 3, 1, dropout_rate)
        
        # âœ… ë™ì  Adaptive pooling
        pooled_length = max(4, self.window_size // 8)
        self.adaptive_pool = nn.AdaptiveAvgPool1d(pooled_length)
        self.conv_output_size = 256 * pooled_length
        
        # Q1, Q2 í¬íŠ¸í´ë¦¬ì˜¤/í–‰ë™ ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
        self.q1_portfolio_fc = nn.Sequential(
            nn.Linear(2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        self.q1_action_fc = nn.Sequential(
            nn.Linear(action_dim, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # Q2ë„ ë™ì¼í•˜ê²Œ...
        self.q2_portfolio_fc = nn.Sequential(
            nn.Linear(2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        self.q2_action_fc = nn.Sequential(
            nn.Linear(action_dim, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # âœ… ê°œì„ ëœ Q1, Q2 ìœµí•© ë„¤íŠ¸ì›Œí¬
        fusion_input_size = self.conv_output_size + hidden_dim // 2  # market + portfolio + action
        self.q1_fusion = nn.Sequential(
            nn.Linear(fusion_input_size, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, 1)
        )
        
        self.q2_fusion = nn.Sequential(
            nn.Linear(fusion_input_size, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, 1)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
        self.to(device)
        LOGGER.info(f"CNN Critic ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ")

    def _make_conv_block(self, in_channels, out_channels, kernel_size, stride, dropout_rate):
        """ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ ìƒì„±"""
        return nn.Sequential(
            nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.MaxPool1d(2, stride=2, padding=0)
        )
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, state: Dict[str, torch.Tensor], action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """ê°œì„ ëœ ìˆœì „íŒŒ"""
        market_data = state['market_data']
        portfolio_state = state['portfolio_state']
        
        # ðŸ†• ë™ì  ìž…ë ¥ ì°¨ì› ì¡°ì •
        if self.adaptive_input:
            market_data = self._adjust_input_dimensions(market_data)
        
        # ì°¨ì› ì²˜ë¦¬
        if len(market_data.shape) == 3:
            market_data = market_data.permute(0, 2, 1)
        elif len(market_data.shape) == 2:
            market_data = market_data.unsqueeze(0).permute(0, 2, 1)
        
        if len(portfolio_state.shape) == 1:
            portfolio_state = portfolio_state.unsqueeze(0)
        
        if len(action.shape) == 1:
            action = action.unsqueeze(0)
        
        # Q1 ê³„ì‚°
        q1_x = self.q1_conv_block1(market_data)
        q1_x = self.q1_conv_block2(q1_x)
        q1_x = self.q1_conv_block3(q1_x)
        q1_x = self.adaptive_pool(q1_x)
        q1_market_features = q1_x.view(q1_x.size(0), -1)  # âœ… ë³€ìˆ˜ëª… ìˆ˜ì •
        
        q1_p = self.q1_portfolio_fc(portfolio_state)
        q1_a = self.q1_action_fc(action)
        q1_portfolio_action = torch.cat([q1_p, q1_a], dim=1)  # âœ… ë³€ìˆ˜ëª… ìˆ˜ì •
        
        # ë‹¨ìˆœí•œ íŠ¹ì„± ê²°í•©
        q1_combined = torch.cat([q1_market_features, q1_portfolio_action], dim=1)
        q1 = self.q1_fusion(q1_combined)
        
        # Q2 ê³„ì‚°
        q2_x = self.q2_conv_block1(market_data)
        q2_x = self.q2_conv_block2(q2_x)
        q2_x = self.q2_conv_block3(q2_x)
        q2_x = self.adaptive_pool(q2_x)
        q2_market_features = q2_x.view(q2_x.size(0), -1)  # âœ… ë³€ìˆ˜ëª… ìˆ˜ì •
        
        q2_p = self.q2_portfolio_fc(portfolio_state)
        q2_a = self.q2_action_fc(action)
        q2_portfolio_action = torch.cat([q2_p, q2_a], dim=1)  # âœ… ë³€ìˆ˜ëª… ìˆ˜ì •
        
        # ë‹¨ìˆœí•œ íŠ¹ì„± ê²°í•©
        q2_combined = torch.cat([q2_market_features, q2_portfolio_action], dim=1)
        q2 = self.q2_fusion(q2_combined)
        
        return q1, q2
    
    def _adjust_input_dimensions(self, market_data: torch.Tensor) -> torch.Tensor:
        """
        ìž…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì„ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ë™ì  ì¡°ì •
        
        Args:
            market_data: ìž…ë ¥ ë§ˆì¼“ ë°ì´í„° (B, W, F) ë˜ëŠ” (W, F)
            
        Returns:
            ì¡°ì •ëœ ë§ˆì¼“ ë°ì´í„°
        """
        # í˜„ìž¬ ìž…ë ¥ì˜ íŠ¹ì„± ì°¨ì› í™•ì¸
        if len(market_data.shape) == 3:  # (B, W, F)
            current_feature_dim = market_data.shape[2]
            batch_size, window_size = market_data.shape[0], market_data.shape[1]
        elif len(market_data.shape) == 2:  # (W, F)
            current_feature_dim = market_data.shape[1]
            batch_size, window_size = 1, market_data.shape[0]
            market_data = market_data.unsqueeze(0)  # (1, W, F)ë¡œ ë³€í™˜
        else:
            LOGGER.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ìž…ë ¥ í˜•íƒœ: {market_data.shape}")
            return market_data
        
        # ì°¨ì›ì´ ì¼ì¹˜í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if current_feature_dim == self.expected_feature_dim:
            return market_data
        
        # ì°¨ì› ì¡°ì • í•„ìš”
        if current_feature_dim > self.expected_feature_dim:
            # íŠ¹ì„±ì´ ë” ë§Žì€ ê²½ìš°: ì•žì—ì„œë¶€í„° ìžë¥´ê¸°
            adjusted_data = market_data[:, :, :self.expected_feature_dim]
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› ì¶•ì†Œ: {current_feature_dim} â†’ {self.expected_feature_dim}")
            
        else:
            # íŠ¹ì„±ì´ ë” ì ì€ ê²½ìš°: ì œë¡œ íŒ¨ë”©
            padding_size = self.expected_feature_dim - current_feature_dim
            padding = torch.zeros(batch_size, window_size, padding_size, 
                                device=market_data.device, dtype=market_data.dtype)
            adjusted_data = torch.cat([market_data, padding], dim=2)
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› í™•ìž¥: {current_feature_dim} â†’ {self.expected_feature_dim} (ì œë¡œ íŒ¨ë”©)")
        
        return adjusted_data

    def get_expected_input_shape(self) -> Tuple[int, int]:
        """ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ í˜•íƒœ ë°˜í™˜"""
        return (self.window_size, self.expected_feature_dim)

    def set_adaptive_input(self, enabled: bool):
        """ë™ì  ìž…ë ¥ ì¡°ì • ê¸°ëŠ¥ í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.adaptive_input = enabled
        LOGGER.info(f"ðŸ”§ CNN ë™ì  ìž…ë ¥ ì¡°ì •: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")

    def update_expected_dimensions(self, new_feature_dim: int, new_window_size: int = None):
        """ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ ì°¨ì› ì—…ë°ì´íŠ¸ (ëª¨ë¸ ë¡œë“œ ì‹œ ì‚¬ìš©)"""
        old_feature_dim = self.expected_feature_dim
        self.expected_feature_dim = new_feature_dim
        
        if new_window_size:
            self.window_size = new_window_size
        
        LOGGER.info(f"ðŸ”§ CNN ê¸°ëŒ€ ì°¨ì› ì—…ë°ì´íŠ¸: íŠ¹ì„± {old_feature_dim} â†’ {new_feature_dim}")
        
        # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ìž¬êµ¬ì„±ì´ í•„ìš”í•œ ê²½ìš° ê²½ê³ 
        if old_feature_dim != new_feature_dim:
            LOGGER.warning("âš ï¸ íŠ¹ì„± ì°¨ì› ë³€ê²½ìœ¼ë¡œ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ìž¬ì´ˆê¸°í™” í•„ìš”")
    
    def to(self, device: torch.device) -> 'CNNCriticNetwork':
        """
        ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        
        Args:
            device: ì´ë™í•  ìž¥ì¹˜
            
        Returns:
            ìž¥ì¹˜ë¡œ ì´ë™ëœ ëª¨ë¸
        """
        self.device = device
        return super(CNNCriticNetwork, self).to(device)


class LSTMActorNetwork(nn.Module):
    """
    LSTM ê¸°ë°˜ Actor ë„¤íŠ¸ì›Œí¬
    ì‹œê³„ì—´ ë°ì´í„°ì˜ ìˆœì°¨ì  íŠ¹ì„±ì„ ëª¨ë¸ë§í•˜ê¸° ìœ„í•œ LSTM ë ˆì´ì–´ ì‚¬ìš©
    """
    
    def __init__(
        self,
        input_shape: Tuple[int, int],  # (window_size, feature_dim)
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        lstm_hidden_dim: int = 128,
        num_lstm_layers: int = 2,
        dropout: float = 0.2,
        log_std_min: float = -2.0,
        log_std_max: float = 2.0,
        device: torch.device = DEVICE
    ):
        """
        LSTMActorNetwork í´ëž˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            input_shape: ìž…ë ¥ ë°ì´í„°ì˜ í˜•íƒœ (window_size, feature_dim)
            action_dim: í–‰ë™ ê³µê°„ì˜ ì°¨ì›
            hidden_dim: FC ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜
            lstm_hidden_dim: LSTM ì€ë‹‰ì¸µì˜ ì°¨ì›
            num_lstm_layers: LSTM ë ˆì´ì–´ ìˆ˜
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            log_std_min: ë¡œê·¸ í‘œì¤€íŽ¸ì°¨ì˜ ìµœì†Œê°’
            log_std_max: ë¡œê·¸ í‘œì¤€íŽ¸ì°¨ì˜ ìµœëŒ€ê°’
            device: ëª¨ë¸ì´ ì‹¤í–‰ë  ìž¥ì¹˜
        """
        super(LSTMActorNetwork, self).__init__()
        
        self.window_size, self.feature_dim = input_shape
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.lstm_hidden_dim = lstm_hidden_dim
        self.num_lstm_layers = num_lstm_layers
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.device = device
        
        # ðŸ†• ë™ì  ìž…ë ¥ ì¡°ì • ì„¤ì • ì¶”ê°€
        self.expected_feature_dim = self.feature_dim
        self.adaptive_input = True
        
        LOGGER.info(f"ðŸ§  LSTM Actor ì´ˆê¸°í™”: ê¸°ëŒ€ íŠ¹ì„± ì°¨ì› {self.expected_feature_dim}")
        
        # LSTM ë ˆì´ì–´ (market_data ì²˜ë¦¬ìš©)
        self.lstm = nn.LSTM(
            input_size=self.feature_dim,
            hidden_size=lstm_hidden_dim,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=dropout if num_lstm_layers > 1 else 0,
            bidirectional=False
        )
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì²˜ë¦¬ìš© FC ë ˆì´ì–´
        self.portfolio_fc = nn.Linear(2, hidden_dim // 4)
        
        # LSTM ì¶œë ¥ê³¼ í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœë¥¼ ê²°í•©í•˜ëŠ” ë ˆì´ì–´  
        combined_dim = lstm_hidden_dim + hidden_dim // 4
        self.fc1 = nn.Linear(combined_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
        
        # í‰ê· ê°’ ì¶œë ¥ ë ˆì´ì–´
        self.mean = nn.Linear(hidden_dim, action_dim)
        
        # ë¡œê·¸ í‘œì¤€íŽ¸ì°¨ ì¶œë ¥ ë ˆì´ì–´
        self.log_std = nn.Linear(hidden_dim, action_dim)
        
        # ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        self.to(device)
        
        LOGGER.info(f"LSTM Actor ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ: ìž…ë ¥ í˜•íƒœ {input_shape}, LSTM ì°¨ì› {lstm_hidden_dim}, ë ˆì´ì–´ ìˆ˜ {num_lstm_layers}")
    
    def forward(self, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ìˆœì „íŒŒ í•¨ìˆ˜
        
        Args:
            state: ìƒíƒœ ë”•ì…”ë„ˆë¦¬ {'market_data': ì‹œìž¥ ë°ì´í„°, 'portfolio_state': í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ}
            
        Returns:
            í‰ê· ê³¼ ë¡œê·¸ í‘œì¤€íŽ¸ì°¨ í…ì„œì˜ íŠœí”Œ
        """
        # ì‹œìž¥ ë°ì´í„° ì²˜ë¦¬ (B, W, F) í˜•íƒœ í™•ì¸
        market_data = state['market_data']
        
        # ðŸ†• ë™ì  ìž…ë ¥ ì°¨ì› ì¡°ì •
        if self.adaptive_input:
            market_data = self._adjust_input_dimensions(market_data)
        
        # ì°¨ì› í™•ì¸ ë° ì²˜ë¦¬
        if len(market_data.shape) == 2:  # (W, F) í˜•íƒœ - ë°°ì¹˜ ì°¨ì› ì¶”ê°€ í•„ìš”
            market_data = market_data.unsqueeze(0)  # (1, W, F) í˜•íƒœë¡œ ë³€í™˜
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì²˜ë¦¬
        portfolio_state = state['portfolio_state']
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì°¨ì› í™•ì¸ ë° ì²˜ë¦¬
        if len(portfolio_state.shape) == 1:  # (2,) í˜•íƒœ - ë°°ì¹˜ ì°¨ì› ì¶”ê°€ í•„ìš”
            portfolio_state = portfolio_state.unsqueeze(0)  # (1, 2) í˜•íƒœë¡œ ë³€í™˜
        
        # LSTMì„ í†µí•œ ì‹œìž¥ ë°ì´í„° ì²˜ë¦¬
        # market_data: (batch_size, sequence_length, input_size)
        lstm_out, (h_n, c_n) = self.lstm(market_data)
        
        # ë§ˆì§€ë§‰ íƒ€ìž„ìŠ¤í…ì˜ ì¶œë ¥ ì‚¬ìš©
        lstm_features = lstm_out[:, -1, :]  # (batch_size, lstm_hidden_dim)
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì²˜ë¦¬
        portfolio_features = F.relu(self.portfolio_fc(portfolio_state))
        
        # íŠ¹ì„± ê²°í•©
        combined = torch.cat([lstm_features, portfolio_features], dim=1)
        
        # FC ë ˆì´ì–´ë¥¼ í†µí•œ ì²˜ë¦¬
        x = F.relu(self.fc1(combined))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        
        # í‰ê· ê°’ ê³„ì‚°
        mean = self.mean(x)
        
        # ë¡œê·¸ í‘œì¤€íŽ¸ì°¨ ê³„ì‚° ë° í´ë¦¬í•‘
        log_std = self.log_std(x)
        log_std = torch.clamp(log_std, min=self.log_std_min, max=self.log_std_max)
        
        return mean, log_std
    
    def sample(self, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ìƒíƒœì—ì„œ í–‰ë™ì„ ìƒ˜í”Œë§
        
        Args:
            state: ìƒíƒœ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            (í–‰ë™, ë¡œê·¸ í™•ë¥ , í‰ê· ) íŠœí”Œ
        """
        mean, log_std = self.forward(state)
        std = log_std.exp()
        
        # ìž¬ë§¤ê°œë³€ìˆ˜í™” íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ ì •ê·œ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()
        y_t = torch.tanh(x_t)
        
        # ì •ì±…ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚°
        log_prob = normal.log_prob(x_t) - torch.log(1 - y_t.pow(2) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        
        return y_t, log_prob, mean
    
    def _adjust_input_dimensions(self, market_data: torch.Tensor) -> torch.Tensor:
        """
        ìž…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì„ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ë™ì  ì¡°ì •
        
        Args:
            market_data: ìž…ë ¥ ë§ˆì¼“ ë°ì´í„° (B, W, F) ë˜ëŠ” (W, F)
            
        Returns:
            ì¡°ì •ëœ ë§ˆì¼“ ë°ì´í„°
        """
        # í˜„ìž¬ ìž…ë ¥ì˜ íŠ¹ì„± ì°¨ì› í™•ì¸
        if len(market_data.shape) == 3:  # (B, W, F)
            current_feature_dim = market_data.shape[2]
            batch_size, window_size = market_data.shape[0], market_data.shape[1]
        elif len(market_data.shape) == 2:  # (W, F)
            current_feature_dim = market_data.shape[1]
            batch_size, window_size = 1, market_data.shape[0]
            market_data = market_data.unsqueeze(0)  # (1, W, F)ë¡œ ë³€í™˜
        else:
            LOGGER.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ìž…ë ¥ í˜•íƒœ: {market_data.shape}")
            return market_data
        
        # ì°¨ì›ì´ ì¼ì¹˜í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if current_feature_dim == self.expected_feature_dim:
            return market_data
        
        # ì°¨ì› ì¡°ì • í•„ìš”
        if current_feature_dim > self.expected_feature_dim:
            # íŠ¹ì„±ì´ ë” ë§Žì€ ê²½ìš°: ì•žì—ì„œë¶€í„° ìžë¥´ê¸°
            adjusted_data = market_data[:, :, :self.expected_feature_dim]
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› ì¶•ì†Œ: {current_feature_dim} â†’ {self.expected_feature_dim}")
            
        else:
            # íŠ¹ì„±ì´ ë” ì ì€ ê²½ìš°: ì œë¡œ íŒ¨ë”©
            padding_size = self.expected_feature_dim - current_feature_dim
            padding = torch.zeros(batch_size, window_size, padding_size, 
                                device=market_data.device, dtype=market_data.dtype)
            adjusted_data = torch.cat([market_data, padding], dim=2)
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› í™•ìž¥: {current_feature_dim} â†’ {self.expected_feature_dim} (ì œë¡œ íŒ¨ë”©)")
        
        return adjusted_data

    def get_expected_input_shape(self) -> Tuple[int, int]:
        """ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ í˜•íƒœ ë°˜í™˜"""
        return (self.window_size, self.expected_feature_dim)

    def set_adaptive_input(self, enabled: bool):
        """ë™ì  ìž…ë ¥ ì¡°ì • ê¸°ëŠ¥ í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.adaptive_input = enabled
        LOGGER.info(f"ðŸ”§ LSTM ë™ì  ìž…ë ¥ ì¡°ì •: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")

    def update_expected_dimensions(self, new_feature_dim: int, new_window_size: int = None):
        """ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ ì°¨ì› ì—…ë°ì´íŠ¸ (ëª¨ë¸ ë¡œë“œ ì‹œ ì‚¬ìš©)"""
        old_feature_dim = self.expected_feature_dim
        self.expected_feature_dim = new_feature_dim
        
        if new_window_size:
            self.window_size = new_window_size
        
        LOGGER.info(f"ðŸ”§ LSTM ê¸°ëŒ€ ì°¨ì› ì—…ë°ì´íŠ¸: íŠ¹ì„± {old_feature_dim} â†’ {new_feature_dim}")
        
        # LSTM ë ˆì´ì–´ ìž¬êµ¬ì„±ì´ í•„ìš”í•œ ê²½ìš° ê²½ê³ 
        if old_feature_dim != new_feature_dim:
            LOGGER.warning("âš ï¸ íŠ¹ì„± ì°¨ì› ë³€ê²½ìœ¼ë¡œ LSTM ë ˆì´ì–´ ìž¬ì´ˆê¸°í™” í•„ìš”")
    
    def to(self, device: torch.device) -> 'LSTMActorNetwork':
        """
        ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        
        Args:
            device: ì´ë™í•  ìž¥ì¹˜
            
        Returns:
            ìž¥ì¹˜ë¡œ ì´ë™ëœ ëª¨ë¸
        """
        self.device = device
        return super(LSTMActorNetwork, self).to(device)


class LSTMCriticNetwork(nn.Module):
    """
    LSTM ê¸°ë°˜ Critic ë„¤íŠ¸ì›Œí¬
    ì‹œê³„ì—´ ë°ì´í„°ì˜ ìˆœì°¨ì  íŠ¹ì„±ì„ ëª¨ë¸ë§í•˜ê¸° ìœ„í•œ LSTM ë ˆì´ì–´ ì‚¬ìš©
    """
    
    def __init__(
        self,
        input_shape: Tuple[int, int],  # (window_size, feature_dim)
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        lstm_hidden_dim: int = 128,
        num_lstm_layers: int = 2,
        dropout: float = 0.2,
        device: torch.device = DEVICE
    ):
        """
        LSTMCriticNetwork í´ëž˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            input_shape: ìž…ë ¥ ë°ì´í„°ì˜ í˜•íƒœ (window_size, feature_dim)
            action_dim: í–‰ë™ ê³µê°„ì˜ ì°¨ì›
            hidden_dim: FC ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜
            lstm_hidden_dim: LSTM ì€ë‹‰ì¸µì˜ ì°¨ì›
            num_lstm_layers: LSTM ë ˆì´ì–´ ìˆ˜
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            device: ëª¨ë¸ì´ ì‹¤í–‰ë  ìž¥ì¹˜
        """
        super(LSTMCriticNetwork, self).__init__()
        
        self.window_size, self.feature_dim = input_shape
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.lstm_hidden_dim = lstm_hidden_dim
        self.num_lstm_layers = num_lstm_layers
        self.device = device
        
        # ðŸ†• ë™ì  ìž…ë ¥ ì¡°ì • ì„¤ì • ì¶”ê°€ (ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìžˆì—ˆìŒ)
        self.expected_feature_dim = self.feature_dim
        self.adaptive_input = True
        
        LOGGER.info(f"ðŸ§  LSTM Critic ì´ˆê¸°í™”: ê¸°ëŒ€ íŠ¹ì„± ì°¨ì› {self.expected_feature_dim}")
        
        # Q1 ë„¤íŠ¸ì›Œí¬ì˜ LSTM
        self.q1_lstm = nn.LSTM(
            input_size=self.feature_dim,
            hidden_size=lstm_hidden_dim,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=dropout if num_lstm_layers > 1 else 0,
            bidirectional=False
        )
        
        # Q2 ë„¤íŠ¸ì›Œí¬ì˜ LSTM
        self.q2_lstm = nn.LSTM(
            input_size=self.feature_dim,
            hidden_size=lstm_hidden_dim,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=dropout if num_lstm_layers > 1 else 0,
            bidirectional=False
        )
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì²˜ë¦¬ìš© FC ë ˆì´ì–´
        self.q1_portfolio_fc = nn.Linear(2, hidden_dim // 4)
        self.q2_portfolio_fc = nn.Linear(2, hidden_dim // 4)
        
        # í–‰ë™ ì²˜ë¦¬ìš© FC ë ˆì´ì–´
        self.q1_action_fc = nn.Linear(action_dim, hidden_dim // 4)
        self.q2_action_fc = nn.Linear(action_dim, hidden_dim // 4)
        
        # LSTM ì¶œë ¥, í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ, í–‰ë™ì„ ê²°í•©í•˜ëŠ” ë ˆì´ì–´
        self.q1_fc1 = nn.Linear(lstm_hidden_dim + hidden_dim // 2, hidden_dim)
        self.q1_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q1 = nn.Linear(hidden_dim, 1)
        
        self.q2_fc1 = nn.Linear(lstm_hidden_dim + hidden_dim // 2, hidden_dim)
        self.q2_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q2 = nn.Linear(hidden_dim, 1)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
        
        # ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        self.to(device)
        
        # ë””ë²„ê¹… í”Œëž˜ê·¸
        self._debug_first_forward = False
        
        LOGGER.info(f"LSTM Critic ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ: ìž…ë ¥ í˜•íƒœ {input_shape}, LSTM ì°¨ì› {lstm_hidden_dim}, ë ˆì´ì–´ ìˆ˜ {num_lstm_layers}")
    
    def forward(self, state: Dict[str, torch.Tensor], action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ìˆœì „íŒŒ í•¨ìˆ˜
        
        Args:
            state: ìƒíƒœ ë”•ì…”ë„ˆë¦¬ {'market_data': ì‹œìž¥ ë°ì´í„°, 'portfolio_state': í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ}
            action: í–‰ë™ í…ì„œ
            
        Returns:
            ë‘ Q ê°’ì˜ íŠœí”Œ
        """
        # ì‹œìž¥ ë°ì´í„° ì²˜ë¦¬ (B, W, F) í˜•íƒœ í™•ì¸
        market_data = state['market_data']
        
        # ðŸ†• ë™ì  ìž…ë ¥ ì°¨ì› ì¡°ì •
        if self.adaptive_input:
            market_data = self._adjust_input_dimensions(market_data)
        
        # ì°¨ì› í™•ì¸ ë° ì²˜ë¦¬
        if len(market_data.shape) == 2:
            market_data = market_data.unsqueeze(0)
        if len(action.shape) == 1:
            action = action.unsqueeze(0)
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì°¨ì› í™•ì¸
        portfolio_state = state['portfolio_state']
        if len(portfolio_state.shape) == 1:
            portfolio_state = portfolio_state.unsqueeze(0)
        
        # ë””ë²„ê¹…: ì°¨ì› ì •ë³´ ì¶œë ¥
        if hasattr(self, '_debug_first_forward') and not self._debug_first_forward:
            self._debug_first_forward = True
            LOGGER.debug(f"ðŸ” LSTM Critic ì°¨ì› ì •ë³´:")
            LOGGER.debug(f"   market_data: {market_data.shape}")
            LOGGER.debug(f"   portfolio_state: {portfolio_state.shape}")
            LOGGER.debug(f"   action: {action.shape}")
            LOGGER.debug(f"   lstm_hidden_dim: {self.lstm_hidden_dim}")
            LOGGER.debug(f"   hidden_dim: {self.hidden_dim}")
        
        # Q1 ê³„ì‚°
        q1_lstm_out, _ = self.q1_lstm(market_data)
        q1_lstm_features = q1_lstm_out[:, -1, :]
        
        q1_p = F.relu(self.q1_portfolio_fc(portfolio_state))
        q1_a = F.relu(self.q1_action_fc(action))
        q1_combined = torch.cat([q1_lstm_features, q1_p, q1_a], dim=1)
        
        q1_x = F.relu(self.q1_fc1(q1_combined))
        q1_x = self.dropout(q1_x)
        q1_x = F.relu(self.q1_fc2(q1_x))
        q1_x = self.dropout(q1_x)
        q1 = self.q1(q1_x)
        
        # Q2 ê³„ì‚°
        q2_lstm_out, _ = self.q2_lstm(market_data)
        q2_lstm_features = q2_lstm_out[:, -1, :]
        
        q2_p = F.relu(self.q2_portfolio_fc(portfolio_state))
        q2_a = F.relu(self.q2_action_fc(action))
        q2_combined = torch.cat([q2_lstm_features, q2_p, q2_a], dim=1)
        
        q2_x = F.relu(self.q2_fc1(q2_combined))
        q2_x = self.dropout(q2_x)
        q2_x = F.relu(self.q2_fc2(q2_x))
        q2_x = self.dropout(q2_x)
        q2 = self.q2(q2_x)
        
        return q1, q2
    
    def _adjust_input_dimensions(self, market_data: torch.Tensor) -> torch.Tensor:
        """
        ìž…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì„ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ë™ì  ì¡°ì •
        
        Args:
            market_data: ìž…ë ¥ ë§ˆì¼“ ë°ì´í„° (B, W, F) ë˜ëŠ” (W, F)
            
        Returns:
            ì¡°ì •ëœ ë§ˆì¼“ ë°ì´í„°
        """
        # í˜„ìž¬ ìž…ë ¥ì˜ íŠ¹ì„± ì°¨ì› í™•ì¸
        if len(market_data.shape) == 3:  # (B, W, F)
            current_feature_dim = market_data.shape[2]
            batch_size, window_size = market_data.shape[0], market_data.shape[1]
        elif len(market_data.shape) == 2:  # (W, F)
            current_feature_dim = market_data.shape[1]
            batch_size, window_size = 1, market_data.shape[0]
            market_data = market_data.unsqueeze(0)  # (1, W, F)ë¡œ ë³€í™˜
        else:
            LOGGER.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ìž…ë ¥ í˜•íƒœ: {market_data.shape}")
            return market_data
        
        # ì°¨ì›ì´ ì¼ì¹˜í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if current_feature_dim == self.expected_feature_dim:
            return market_data
        
        # ì°¨ì› ì¡°ì • í•„ìš”
        if current_feature_dim > self.expected_feature_dim:
            # íŠ¹ì„±ì´ ë” ë§Žì€ ê²½ìš°: ì•žì—ì„œë¶€í„° ìžë¥´ê¸°
            adjusted_data = market_data[:, :, :self.expected_feature_dim]
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› ì¶•ì†Œ: {current_feature_dim} â†’ {self.expected_feature_dim}")
            
        else:
            # íŠ¹ì„±ì´ ë” ì ì€ ê²½ìš°: ì œë¡œ íŒ¨ë”©
            padding_size = self.expected_feature_dim - current_feature_dim
            padding = torch.zeros(batch_size, window_size, padding_size, 
                                device=market_data.device, dtype=market_data.dtype)
            adjusted_data = torch.cat([market_data, padding], dim=2)
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› í™•ìž¥: {current_feature_dim} â†’ {self.expected_feature_dim} (ì œë¡œ íŒ¨ë”©)")
        
        return adjusted_data

    def get_expected_input_shape(self) -> Tuple[int, int]:
        """ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ í˜•íƒœ ë°˜í™˜"""
        return (self.window_size, self.expected_feature_dim)

    def set_adaptive_input(self, enabled: bool):
        """ë™ì  ìž…ë ¥ ì¡°ì • ê¸°ëŠ¥ í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.adaptive_input = enabled
        LOGGER.info(f"ðŸ”§ LSTM ë™ì  ìž…ë ¥ ì¡°ì •: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")

    def update_expected_dimensions(self, new_feature_dim: int, new_window_size: int = None):
        """ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ ì°¨ì› ì—…ë°ì´íŠ¸ (ëª¨ë¸ ë¡œë“œ ì‹œ ì‚¬ìš©)"""
        old_feature_dim = self.expected_feature_dim
        self.expected_feature_dim = new_feature_dim
        
        if new_window_size:
            self.window_size = new_window_size
        
        LOGGER.info(f"ðŸ”§ LSTM ê¸°ëŒ€ ì°¨ì› ì—…ë°ì´íŠ¸: íŠ¹ì„± {old_feature_dim} â†’ {new_feature_dim}")
        
        # LSTM ë ˆì´ì–´ ìž¬êµ¬ì„±ì´ í•„ìš”í•œ ê²½ìš° ê²½ê³ 
        if old_feature_dim != new_feature_dim:
            LOGGER.warning("âš ï¸ íŠ¹ì„± ì°¨ì› ë³€ê²½ìœ¼ë¡œ LSTM ë ˆì´ì–´ ìž¬ì´ˆê¸°í™” í•„ìš”")
    
    def to(self, device: torch.device) -> 'LSTMCriticNetwork':
        """
        ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        
        Args:
            device: ì´ë™í•  ìž¥ì¹˜
            
        Returns:
            ìž¥ì¹˜ë¡œ ì´ë™ëœ ëª¨ë¸
        """
        self.device = device
        return super(LSTMCriticNetwork, self).to(device)


class MambaBlock(nn.Module):
    """
    Simplified Mamba Block for Trading
    State Space Model with selective mechanism
    """
    def __init__(
        self, 
        d_model: int, 
        d_state: int = 16, 
        d_conv: int = 4,
        expand_factor: int = 2,
        device: torch.device = DEVICE
    ):
        super(MambaBlock, self).__init__()
        
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.d_inner = expand_factor * d_model
        self.device = device
        
        # Input projection
        self.in_proj = nn.Linear(d_model, self.d_inner * 2)
        
        # Convolution for local interactions
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            kernel_size=d_conv,
            padding=d_conv - 1,
            groups=self.d_inner  # Depthwise convolution
        )
        
        # SSM parameters
        self.x_proj = nn.Linear(self.d_inner, d_state * 2)  # B, C projections
        self.dt_proj = nn.Linear(self.d_inner, self.d_inner)
        
        # A parameter (learnable)
        A = torch.arange(1, d_state + 1, dtype=torch.float32).unsqueeze(0).repeat(self.d_inner, 1)
        self.A_log = nn.Parameter(torch.log(A))
        
        # D parameter (skip connection)
        self.D = nn.Parameter(torch.ones(self.d_inner))
        
        # Output projection
        self.out_proj = nn.Linear(self.d_inner, d_model)
        
        # Layer norm
        self.norm = nn.LayerNorm(d_model)
        
        self.to(device)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch_size, seq_len, d_model)
        Returns:
            output: (batch_size, seq_len, d_model)
        """
        batch_size, seq_len, d_model = x.shape
        
        # Residual connection
        residual = x
        
        # Input projection
        x_and_res = self.in_proj(x)  # (B, L, 2*d_inner)
        x, res = x_and_res.split([self.d_inner, self.d_inner], dim=-1)
        
        # Apply SiLU activation
        x = F.silu(x)
        
        # Convolution (need to transpose for conv1d)
        x = x.transpose(1, 2)  # (B, d_inner, L)
        x = self.conv1d(x)[:, :, :seq_len]  # Trim to original length
        x = x.transpose(1, 2)  # (B, L, d_inner)
        
        # SSM
        x = F.silu(x)
        
        # Get B, C, and dt
        x_proj_out = self.x_proj(x)  # (B, L, 2*d_state)
        B, C = x_proj_out.split([self.d_state, self.d_state], dim=-1)
        
        dt = F.softplus(self.dt_proj(x))  # (B, L, d_inner)
        
        # Apply SSM
        A = -torch.exp(self.A_log)  # (d_inner, d_state)
        y = self._ssm(x, A, B, C, dt)
        
        # Skip connection and gating
        y = y * F.silu(res)
        
        # Output projection
        output = self.out_proj(y)
        
        # Residual connection and layer norm
        return self.norm(output + residual)
    
    def _ssm(self, x, A, B, C, dt):
        """Simplified SSM computation"""
        batch_size, seq_len, d_inner = x.shape
        d_state = A.shape[1]
        
        # Discretize A and B
        dt = dt.unsqueeze(-1)  # (B, L, d_inner, 1)
        A_disc = torch.exp(A.unsqueeze(0).unsqueeze(0) * dt)  # (B, L, d_inner, d_state)
        B_disc = dt * B.unsqueeze(2)  # (B, L, d_inner, d_state)
        
        # Initialize state
        h = torch.zeros(batch_size, d_inner, d_state, device=x.device)
        
        outputs = []
        for i in range(seq_len):
            # Update state
            h = A_disc[:, i] * h + B_disc[:, i] * x[:, i:i+1]  # Broadcasting
            
            # Compute output
            y = torch.sum(h * C[:, i:i+1].unsqueeze(1), dim=-1)  # (B, d_inner)
            outputs.append(y)
        
        y = torch.stack(outputs, dim=1)  # (B, L, d_inner)
        
        # Add skip connection
        y = y + x * self.D.unsqueeze(0).unsqueeze(0)
        
        return y


class MambaActorNetwork(nn.Module):
    """
    Mamba-based Actor Network for Trading
    Efficient state space model for long-range dependencies
    """
    def __init__(
        self,
        input_shape: Tuple[int, int],  # (window_size, feature_dim)
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        mamba_layers: int = 4,
        d_state: int = 16,
        log_std_min: float = -2.0,
        log_std_max: float = 2.0,
        dropout_rate: float = 0.1,
        device: torch.device = DEVICE
    ):
        super(MambaActorNetwork, self).__init__()
        
        self.window_size, self.feature_dim = input_shape
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.device = device
        
        # ðŸ†• ë™ì  ì°¨ì› ì¡°ì • ì„¤ì •
        self.expected_feature_dim = self.feature_dim
        self.adaptive_input = True
        
        LOGGER.info(f"ðŸŒŠ Mamba Actor ì´ˆê¸°í™”: ê¸°ëŒ€ íŠ¹ì„± ì°¨ì› {self.expected_feature_dim}")
        
        # Input embedding
        self.input_embedding = nn.Linear(self.feature_dim, hidden_dim)
        
        # Mamba blocks
        self.mamba_blocks = nn.ModuleList([
            MambaBlock(
                d_model=hidden_dim,
                d_state=d_state,
                device=device
            ) for _ in range(mamba_layers)
        ])
        
        # Portfolio state processing
        self.portfolio_fc = nn.Sequential(
            nn.Linear(2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # Feature fusion
        self.fusion_network = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim // 4, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # Output layers
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)
        
        self._initialize_weights()
        self.to(device)
        
        LOGGER.info(f"Mamba Actor ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ: {mamba_layers}ê°œ Mamba ë¸”ë¡")
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """ìˆœì „íŒŒ"""
        market_data = state['market_data']
        portfolio_state = state['portfolio_state']
        
        # ë™ì  ìž…ë ¥ ì°¨ì› ì¡°ì •
        if self.adaptive_input:
            market_data = self._adjust_input_dimensions(market_data)
        
        # ì°¨ì› ì²˜ë¦¬
        if len(market_data.shape) == 2:
            market_data = market_data.unsqueeze(0)
        if len(portfolio_state.shape) == 1:
            portfolio_state = portfolio_state.unsqueeze(0)
        
        # Market data embedding
        x = self.input_embedding(market_data)  # (B, W, hidden_dim)
        
        # Mamba blocks
        for mamba_block in self.mamba_blocks:
            x = mamba_block(x)
        
        # Global average pooling for sequence aggregation
        market_features = torch.mean(x, dim=1)  # (B, hidden_dim)
        
        # Portfolio features
        portfolio_features = self.portfolio_fc(portfolio_state)
        
        # Feature fusion
        combined_features = torch.cat([market_features, portfolio_features], dim=1)
        fused_features = self.fusion_network(combined_features)
        
        # Output
        mean = self.mean(fused_features)
        log_std = self.log_std(fused_features)
        log_std = torch.clamp(log_std, min=self.log_std_min, max=self.log_std_max)
        
        return mean, log_std
    
    def sample(self, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """í–‰ë™ ìƒ˜í”Œë§"""
        mean, log_std = self.forward(state)
        std = log_std.exp()
        
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()
        y_t = torch.tanh(x_t)
        
        log_prob = normal.log_prob(x_t) - torch.log(1 - y_t.pow(2) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        
        return y_t, log_prob, mean
    
    def _adjust_input_dimensions(self, market_data: torch.Tensor) -> torch.Tensor:
        """ìž…ë ¥ ì°¨ì› ë™ì  ì¡°ì •"""
        if len(market_data.shape) == 3:
            current_feature_dim = market_data.shape[2]
            batch_size, window_size = market_data.shape[0], market_data.shape[1]
        elif len(market_data.shape) == 2:
            current_feature_dim = market_data.shape[1]
            batch_size, window_size = 1, market_data.shape[0]
            market_data = market_data.unsqueeze(0)
        else:
            LOGGER.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ìž…ë ¥ í˜•íƒœ: {market_data.shape}")
            return market_data
        
        # ì°¨ì›ì´ ì¼ì¹˜í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if current_feature_dim == self.expected_feature_dim:
            return market_data
        
        # ì°¨ì› ì¡°ì • í•„ìš”
        if current_feature_dim > self.expected_feature_dim:
            # íŠ¹ì„±ì´ ë” ë§Žì€ ê²½ìš°: ì•žì—ì„œë¶€í„° ìžë¥´ê¸°
            adjusted_data = market_data[:, :, :self.expected_feature_dim]
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› ì¶•ì†Œ: {current_feature_dim} â†’ {self.expected_feature_dim}")
            
        else:
            # íŠ¹ì„±ì´ ë” ì ì€ ê²½ìš°: ì œë¡œ íŒ¨ë”©
            padding_size = self.expected_feature_dim - current_feature_dim
            padding = torch.zeros(batch_size, window_size, padding_size, 
                                device=market_data.device, dtype=market_data.dtype)
            adjusted_data = torch.cat([market_data, padding], dim=2)
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› í™•ìž¥: {current_feature_dim} â†’ {self.expected_feature_dim} (ì œë¡œ íŒ¨ë”©)")
        
        return adjusted_data
    
    def get_expected_input_shape(self) -> Tuple[int, int]:
        """ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ í˜•íƒœ ë°˜í™˜"""
        return (self.window_size, self.expected_feature_dim)
    
    def set_adaptive_input(self, enabled: bool):
        """ë™ì  ìž…ë ¥ ì¡°ì • ê¸°ëŠ¥ í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.adaptive_input = enabled
        LOGGER.info(f"ðŸ”§ Mamba ë™ì  ìž…ë ¥ ì¡°ì •: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
    
    def update_expected_dimensions(self, new_feature_dim: int, new_window_size: int = None):
        """ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ ì°¨ì› ì—…ë°ì´íŠ¸ (ëª¨ë¸ ë¡œë“œ ì‹œ ì‚¬ìš©)"""
        old_feature_dim = self.expected_feature_dim
        self.expected_feature_dim = new_feature_dim
        
        if new_window_size:
            self.window_size = new_window_size
        
        LOGGER.info(f"ðŸ”§ Mamba ê¸°ëŒ€ ì°¨ì› ì—…ë°ì´íŠ¸: íŠ¹ì„± {old_feature_dim} â†’ {new_feature_dim}")
    
    def to(self, device: torch.device) -> 'MambaActorNetwork':
        """
        ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        
        Args:
            device: ì´ë™í•  ìž¥ì¹˜
            
        Returns:
            ìž¥ì¹˜ë¡œ ì´ë™ëœ ëª¨ë¸
        """
        self.device = device
        return super(MambaActorNetwork, self).to(device)


class MambaCriticNetwork(nn.Module):
    """
    Mamba-based Critic Network for Trading
    """
    def __init__(
        self,
        input_shape: Tuple[int, int],
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        mamba_layers: int = 4,
        d_state: int = 16,
        dropout_rate: float = 0.1,
        device: torch.device = DEVICE
    ):
        super(MambaCriticNetwork, self).__init__()
        
        self.window_size, self.feature_dim = input_shape
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.device = device
        
        # ðŸ†• ë™ì  ì°¨ì› ì¡°ì • ì„¤ì •
        self.expected_feature_dim = self.feature_dim
        self.adaptive_input = True
        
        LOGGER.info(f"ðŸ§  LSTM Critic ì´ˆê¸°í™”: ê¸°ëŒ€ íŠ¹ì„± ì°¨ì› {self.expected_feature_dim}")
        
        # Q1 ë„¤íŠ¸ì›Œí¬ì˜ LSTM
        self.q1_lstm = nn.LSTM(
            input_size=self.feature_dim,
            hidden_size=hidden_dim,
            num_layers=mamba_layers,
            batch_first=True,
            dropout=dropout_rate if mamba_layers > 1 else 0,
            bidirectional=False
        )
        
        # Q2 ë„¤íŠ¸ì›Œí¬ì˜ LSTM
        self.q2_lstm = nn.LSTM(
            input_size=self.feature_dim,
            hidden_size=hidden_dim,
            num_layers=mamba_layers,
            batch_first=True,
            dropout=dropout_rate if mamba_layers > 1 else 0,
            bidirectional=False
        )
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì²˜ë¦¬ìš© FC ë ˆì´ì–´
        self.q1_portfolio_fc = nn.Linear(2, hidden_dim // 4)
        self.q2_portfolio_fc = nn.Linear(2, hidden_dim // 4)
        
        # í–‰ë™ ì²˜ë¦¬ìš© FC ë ˆì´ì–´
        self.q1_action_fc = nn.Linear(action_dim, hidden_dim // 4)
        self.q2_action_fc = nn.Linear(action_dim, hidden_dim // 4)
        
        # LSTM ì¶œë ¥, í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ, í–‰ë™ì„ ê²°í•©í•˜ëŠ” ë ˆì´ì–´
        self.q1_fc1 = nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim)
        self.q1_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q1 = nn.Linear(hidden_dim, 1)
        
        self.q2_fc1 = nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim)
        self.q2_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q2 = nn.Linear(hidden_dim, 1)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout_rate)
        
        # ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        self.to(device)
        
        LOGGER.info(f"LSTM Critic ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ: ìž…ë ¥ í˜•íƒœ {input_shape}, LSTM ì°¨ì› {hidden_dim}, ë ˆì´ì–´ ìˆ˜ {mamba_layers}")
    
    def forward(self, state: Dict[str, torch.Tensor], action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ìˆœì „íŒŒ í•¨ìˆ˜
        
        Args:
            state: ìƒíƒœ ë”•ì…”ë„ˆë¦¬ {'market_data': ì‹œìž¥ ë°ì´í„°, 'portfolio_state': í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ}
            action: í–‰ë™ í…ì„œ
            
        Returns:
            ë‘ Q ê°’ì˜ íŠœí”Œ
        """
        # ì‹œìž¥ ë°ì´í„° ì²˜ë¦¬ (B, W, F) í˜•íƒœ í™•ì¸
        market_data = state['market_data']
        
        # ðŸ†• ë™ì  ìž…ë ¥ ì°¨ì› ì¡°ì •
        if self.adaptive_input:
            market_data = self._adjust_input_dimensions(market_data)
        
        # ì°¨ì› í™•ì¸ ë° ì²˜ë¦¬
        if len(market_data.shape) == 2:
            market_data = market_data.unsqueeze(0)
        if len(action.shape) == 1:
            action = action.unsqueeze(0)
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì°¨ì› í™•ì¸
        portfolio_state = state['portfolio_state']
        if len(portfolio_state.shape) == 1:
            portfolio_state = portfolio_state.unsqueeze(0)
        
        # ë””ë²„ê¹…: ì°¨ì› ì •ë³´ ì¶œë ¥
        if hasattr(self, '_debug_first_forward') and not self._debug_first_forward:
            self._debug_first_forward = True
            LOGGER.debug(f"ðŸ” LSTM Critic ì°¨ì› ì •ë³´:")
            LOGGER.debug(f"   market_data: {market_data.shape}")
            LOGGER.debug(f"   portfolio_state: {portfolio_state.shape}")
            LOGGER.debug(f"   action: {action.shape}")
            LOGGER.debug(f"   lstm_hidden_dim: {self.lstm_hidden_dim}")
            LOGGER.debug(f"   hidden_dim: {self.hidden_dim}")
        
        # Q1 ê³„ì‚°
        q1_lstm_out, _ = self.q1_lstm(market_data)
        q1_lstm_features = q1_lstm_out[:, -1, :]
        
        q1_p = F.relu(self.q1_portfolio_fc(portfolio_state))
        q1_a = F.relu(self.q1_action_fc(action))
        q1_combined = torch.cat([q1_lstm_features, q1_p, q1_a], dim=1)
        
        q1_x = F.relu(self.q1_fc1(q1_combined))
        q1_x = self.dropout(q1_x)
        q1_x = F.relu(self.q1_fc2(q1_x))
        q1_x = self.dropout(q1_x)
        q1 = self.q1(q1_x)
        
        # Q2 ê³„ì‚°
        q2_lstm_out, _ = self.q2_lstm(market_data)
        q2_lstm_features = q2_lstm_out[:, -1, :]
        
        q2_p = F.relu(self.q2_portfolio_fc(portfolio_state))
        q2_a = F.relu(self.q2_action_fc(action))
        q2_combined = torch.cat([q2_lstm_features, q2_p, q2_a], dim=1)
        
        q2_x = F.relu(self.q2_fc1(q2_combined))
        q2_x = self.dropout(q2_x)
        q2_x = F.relu(self.q2_fc2(q2_x))
        q2_x = self.dropout(q2_x)
        q2 = self.q2(q2_x)
        
        return q1, q2
    
    def _adjust_input_dimensions(self, market_data: torch.Tensor) -> torch.Tensor:
        """
        ìž…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì„ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ë™ì  ì¡°ì •
        
        Args:
            market_data: ìž…ë ¥ ë§ˆì¼“ ë°ì´í„° (B, W, F) ë˜ëŠ” (W, F)
            
        Returns:
            ì¡°ì •ëœ ë§ˆì¼“ ë°ì´í„°
        """
        # í˜„ìž¬ ìž…ë ¥ì˜ íŠ¹ì„± ì°¨ì› í™•ì¸
        if len(market_data.shape) == 3:  # (B, W, F)
            current_feature_dim = market_data.shape[2]
            batch_size, window_size = market_data.shape[0], market_data.shape[1]
        elif len(market_data.shape) == 2:  # (W, F)
            current_feature_dim = market_data.shape[1]
            batch_size, window_size = 1, market_data.shape[0]
            market_data = market_data.unsqueeze(0)  # (1, W, F)ë¡œ ë³€í™˜
        else:
            LOGGER.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ìž…ë ¥ í˜•íƒœ: {market_data.shape}")
            return market_data
        
        # ì°¨ì›ì´ ì¼ì¹˜í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if current_feature_dim == self.expected_feature_dim:
            return market_data
        
        # ì°¨ì› ì¡°ì • í•„ìš”
        if current_feature_dim > self.expected_feature_dim:
            # íŠ¹ì„±ì´ ë” ë§Žì€ ê²½ìš°: ì•žì—ì„œë¶€í„° ìžë¥´ê¸°
            adjusted_data = market_data[:, :, :self.expected_feature_dim]
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› ì¶•ì†Œ: {current_feature_dim} â†’ {self.expected_feature_dim}")
            
        else:
            # íŠ¹ì„±ì´ ë” ì ì€ ê²½ìš°: ì œë¡œ íŒ¨ë”©
            padding_size = self.expected_feature_dim - current_feature_dim
            padding = torch.zeros(batch_size, window_size, padding_size, 
                                device=market_data.device, dtype=market_data.dtype)
            adjusted_data = torch.cat([market_data, padding], dim=2)
            LOGGER.debug(f"ðŸ”§ íŠ¹ì„± ì°¨ì› í™•ìž¥: {current_feature_dim} â†’ {self.expected_feature_dim} (ì œë¡œ íŒ¨ë”©)")
        
        return adjusted_data

    def get_expected_input_shape(self) -> Tuple[int, int]:
        """ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ í˜•íƒœ ë°˜í™˜"""
        return (self.window_size, self.expected_feature_dim)

    def set_adaptive_input(self, enabled: bool):
        """ë™ì  ìž…ë ¥ ì¡°ì • ê¸°ëŠ¥ í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.adaptive_input = enabled
        LOGGER.info(f"ðŸ”§ LSTM ë™ì  ìž…ë ¥ ì¡°ì •: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")

    def update_expected_dimensions(self, new_feature_dim: int, new_window_size: int = None):
        """ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ ì°¨ì› ì—…ë°ì´íŠ¸ (ëª¨ë¸ ë¡œë“œ ì‹œ ì‚¬ìš©)"""
        old_feature_dim = self.expected_feature_dim
        self.expected_feature_dim = new_feature_dim
        
        if new_window_size:
            self.window_size = new_window_size
        
        LOGGER.info(f"ðŸ”§ LSTM ê¸°ëŒ€ ì°¨ì› ì—…ë°ì´íŠ¸: íŠ¹ì„± {old_feature_dim} â†’ {new_feature_dim}")
        
        # LSTM ë ˆì´ì–´ ìž¬êµ¬ì„±ì´ í•„ìš”í•œ ê²½ìš° ê²½ê³ 
        if old_feature_dim != new_feature_dim:
            LOGGER.warning("âš ï¸ íŠ¹ì„± ì°¨ì› ë³€ê²½ìœ¼ë¡œ LSTM ë ˆì´ì–´ ìž¬ì´ˆê¸°í™” í•„ìš”")
    
    def to(self, device: torch.device) -> 'MambaCriticNetwork':
        """
        ëª¨ë¸ì„ ì§€ì •ëœ ìž¥ì¹˜ë¡œ ì´ë™
        
        Args:
            device: ì´ë™í•  ìž¥ì¹˜
            
        Returns:
            ìž¥ì¹˜ë¡œ ì´ë™ëœ ëª¨ë¸
        """
        self.device = device
        return super(MambaCriticNetwork, self).to(device)


if __name__ == "__main__":
    # ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì½”ë“œ
    # ì¼ë°˜ ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
    state_dim = 10
    action_dim = 1
    batch_size = 4
    
    actor = ActorNetwork(state_dim, action_dim)
    critic = CriticNetwork(state_dim, action_dim)
    
    # ëžœë¤ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    state = torch.randn(batch_size, state_dim).to(DEVICE)
    action = torch.randn(batch_size, action_dim).to(DEVICE)
    
    # Actor í…ŒìŠ¤íŠ¸
    mean, log_std = actor(state)
    action_sample, log_prob, _ = actor.sample(state)
    
    print(f"Actor ì¶œë ¥ - í‰ê· : {mean.shape}, ë¡œê·¸ í‘œì¤€íŽ¸ì°¨: {log_std.shape}")
    print(f"Actor ìƒ˜í”Œ - í–‰ë™: {action_sample.shape}, ë¡œê·¸ í™•ë¥ : {log_prob.shape}")
    
    # Critic í…ŒìŠ¤íŠ¸
    q1, q2 = critic(state, action)
    
    print(f"Critic ì¶œë ¥ - Q1: {q1.shape}, Q2: {q2.shape}")
    
    # CNN ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
    window_size = WINDOW_SIZE
    feature_dim = 5
    
    cnn_actor = CNNActorNetwork((window_size, feature_dim), action_dim)
    cnn_critic = CNNCriticNetwork((window_size, feature_dim), action_dim)
    
    # ëžœë¤ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    market_data = torch.randn(batch_size, window_size, feature_dim).to(DEVICE)
    portfolio_state = torch.randn(batch_size, 2).to(DEVICE)
    state_dict = {
        'market_data': market_data,
        'portfolio_state': portfolio_state
    }
    
    # CNN Actor í…ŒìŠ¤íŠ¸
    mean, log_std = cnn_actor(state_dict)
    action_sample, log_prob, _ = cnn_actor.sample(state_dict)
    
    print(f"CNN Actor ì¶œë ¥ - í‰ê· : {mean.shape}, ë¡œê·¸ í‘œì¤€íŽ¸ì°¨: {log_std.shape}")
    print(f"CNN Actor ìƒ˜í”Œ - í–‰ë™: {action_sample.shape}, ë¡œê·¸ í™•ë¥ : {log_prob.shape}")
    
    # CNN Critic í…ŒìŠ¤íŠ¸
    q1, q2 = cnn_critic(state_dict, action_sample)
    
    print(f"CNN Critic ì¶œë ¥ - Q1: {q1.shape}, Q2: {q2.shape}")
    
    # LSTM ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
    print("\n=== LSTM ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ ===")
    
    lstm_actor = LSTMActorNetwork((window_size, feature_dim), action_dim)
    lstm_critic = LSTMCriticNetwork((window_size, feature_dim), action_dim)
    
    # LSTM Actor í…ŒìŠ¤íŠ¸
    mean, log_std = lstm_actor(state_dict)
    action_sample, log_prob, _ = lstm_actor.sample(state_dict)
    
    print(f"LSTM Actor ì¶œë ¥ - í‰ê· : {mean.shape}, ë¡œê·¸ í‘œì¤€íŽ¸ì°¨: {log_std.shape}")
    print(f"LSTM Actor ìƒ˜í”Œ - í–‰ë™: {action_sample.shape}, ë¡œê·¸ í™•ë¥ : {log_prob.shape}")
    
    # LSTM Critic í…ŒìŠ¤íŠ¸
    q1, q2 = lstm_critic(state_dict, action_sample)
    
    print(f"LSTM Critic ì¶œë ¥ - Q1: {q1.shape}, Q2: {q2.shape}")
    
    print("\nëª¨ë“  ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")