"""
SAC (Soft Actor-Critic) ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ëª¨ë“ˆ (Config í†µí•© ë²„ì „)
"""
import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
import torch

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from typing import Dict, List, Tuple, Union, Optional, Any
from collections import deque
import random
import pickle
import time
from pathlib import Path
import sys
import os

from src.config.ea_teb_config import (
    DEVICE,
    HIDDEN_DIM,
    GAMMA,
    TAU,
    LEARNING_RATE_ACTOR,
    LEARNING_RATE_CRITIC,
    LEARNING_RATE_ALPHA,
    ALPHA_INIT,
    REPLAY_BUFFER_SIZE,
    TARGET_UPDATE_INTERVAL,
    BATCH_SIZE,
    MODELS_DIR,
    LOGGER,
    MAX_STEPS_PER_EPISODE,
    WINDOW_SIZE,
    WEIGHT_DECAY,
    TARGET_ENTROPY_FACTOR
)

from src.models.networks import (
    ActorNetwork, 
    CriticNetwork, 
    CNNActorNetwork, 
    CNNCriticNetwork,
    LSTMActorNetwork,
    LSTMCriticNetwork
)
from src.models.mamba_networks import (
    MambaActorNetwork,
    MambaCriticNetwork
)
from src.models.tiny_transformer_networks import (
    TinyTransformerActorNetwork,
    TinyTransformerCriticNetwork
)

from src.utils.utils import soft_update, create_directory

    
class ReplayBuffer:
    """
    ë¦¬í”Œë ˆì´ ë²„í¼ (ì •ë¦¬ëœ ë²„ì „ - í•µì‹¬ ê¸°ëŠ¥ë§Œ ìœ ì§€)
    """
    
    def __init__(self, capacity: int = REPLAY_BUFFER_SIZE):
        """
        ë¦¬í”Œë ˆì´ ë²„í¼ ì´ˆê¸°í™”
        
        Args:
            capacity: ë²„í¼ ìµœëŒ€ ìš©ëŸ‰
        """
        self.capacity = capacity
        self.buffer: List[Tuple] = []
        self.position = 0
    
    def push(self, state: Any, action: Any, reward: float, next_state: Any, done: bool) -> None:
        """
        ê²½í—˜ì„ ë²„í¼ì— ì €ì¥
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
            action: ìˆ˜í–‰í•œ ì•¡ì…˜
            reward: ë°›ì€ ë³´ìƒ
            next_state: ë‹¤ìŒ ìƒíƒœ  
            done: ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€
        """
        experience = (state, action, reward, next_state, done)
        
        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
        else:
            self.buffer[self.position] = experience
            
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int) -> Tuple:
        """
        ë°°ì¹˜ ìƒ˜í”Œë§
        
        Args:
            batch_size: ìƒ˜í”Œë§í•  ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ìƒ˜í”Œë§ëœ ê²½í—˜ë“¤ì˜ íŠœí”Œ
        """
        if len(self.buffer) < batch_size:
            batch_size = len(self.buffer)
            
        experiences = random.sample(self.buffer, batch_size)
        
        # ê° ìš”ì†Œë³„ë¡œ ë¶„ë¦¬
        states, actions, rewards, next_states, dones = zip(*experiences)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self) -> int:
        """
        ë²„í¼ í¬ê¸° ë°˜í™˜
        
        Returns:
            í˜„ì¬ ë²„í¼ì— ì €ì¥ëœ ê²½í—˜ì˜ ìˆ˜
        """
        return len(self.buffer)





class SACAgent:
    """
    SAC ì•Œê³ ë¦¬ì¦˜ ì—ì´ì „íŠ¸ (ì •ë¦¬ëœ ë²„ì „)
    """
        
    def __init__(
        self,
        state_dim: Optional[int] = None,
        action_dim: Optional[int] = None,
        hidden_dim: int = HIDDEN_DIM,
        actor_lr: float = LEARNING_RATE_ACTOR,
        critic_lr: float = LEARNING_RATE_CRITIC,
        alpha_lr: float = LEARNING_RATE_ALPHA,
        gamma: float = GAMMA,
        tau: float = TAU,
        alpha_init: float = ALPHA_INIT,
        target_update_interval: int = TARGET_UPDATE_INTERVAL,
        use_automatic_entropy_tuning: bool = True,
        device: torch.device = DEVICE,
        buffer_capacity: int = REPLAY_BUFFER_SIZE,
        input_shape: Optional[Tuple[int, int]] = None,
        use_cnn: bool = False,
        use_lstm: bool = False,
        use_mamba: bool = False,
        use_tinytransformer: bool = False,
        model_type: Optional[str] = None,
        lstm_hidden_dim: int = 128,
        num_lstm_layers: int = 2,
        dropout_rate: float = 0.1,
        training_symbols: Optional[List[str]] = None,
        lstm_dropout: float = 0.2,
        symbol: Optional[str] = None,
        gradient_clip_norm: float = 1.0,
        use_gradient_clipping: bool = True,
        adaptive_alpha: bool = True
    ):
        """SACAgent í´ë˜ìŠ¤ ì´ˆê¸°í™” (Config í†µí•© ë²„ì „)"""
        
        # ê¸°ë³¸ ì†ì„± ì„¤ì •
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.gamma = gamma
        self.tau = tau
        self.target_update_interval = target_update_interval
        self.use_automatic_entropy_tuning = use_automatic_entropy_tuning
        self.device = device
        self.use_cnn = use_cnn
        self.use_lstm = use_lstm
        self.use_mamba = use_mamba
        self.use_tinytransformer = use_tinytransformer
        self.input_shape = input_shape
        self.lstm_hidden_dim = lstm_hidden_dim
        self.num_lstm_layers = num_lstm_layers
        self.lstm_dropout = lstm_dropout
        self.symbol = symbol
        
        # model_type ì²˜ë¦¬ (auto-detect if not provided)
        if model_type is None:
            if use_tinytransformer:
                model_type = 'tinytransformer'
            elif use_mamba:
                model_type = 'mamba'
            elif use_lstm:
                model_type = 'lstm'
            elif use_cnn:
                model_type = 'cnn'
            else:
                model_type = 'mlp'
        self.model_type = model_type
        
        # í†µí•©ëœ íŒŒë¼ë¯¸í„° ì„¤ì •
        self.dropout_rate = dropout_rate
        self.training_symbols = training_symbols or []
        
        # âœ… ì „ë‹¬ë°›ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš© (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í˜¸í™˜)
        self.actor_lr = actor_lr
        self.critic_lr = critic_lr
        self.alpha_lr = alpha_lr
        self.alpha_init = alpha_init
        
        LOGGER.info(f"ğŸ“Š {model_type.upper() if model_type else 'SAC'} ì—ì´ì „íŠ¸ íŒŒë¼ë¯¸í„°:")
        LOGGER.info(f"   â””â”€ Actor LR: {self.actor_lr:.6f}")
        LOGGER.info(f"   â””â”€ Critic LR: {self.critic_lr:.6f}")
        LOGGER.info(f"   â””â”€ Dropout: {self.dropout_rate}")
        
        # ì•ˆì •ì„± ê°œì„  ì„¤ì • (í†µí•©)
        self.use_gradient_clipping = use_gradient_clipping
        self.gradient_clip_norm = gradient_clip_norm
        self.adaptive_alpha = adaptive_alpha
        
        # âœ… SAC í‘œì¤€: ìë™ ì—”íŠ¸ë¡œí”¼ íŠœë‹ (ê³ ì • target_entropy)
        if self.adaptive_alpha:
            self.recent_entropies: deque = deque(maxlen=100)
        
        # âœ… SAC í‘œì¤€: ê³ ì • ì—”íŠ¸ë¡œí”¼ íƒ€ê²Ÿ ì„¤ì • (ì„¤ì •ê°’ ë°˜ì˜)
        action_dim_safe = action_dim or 1
        self.target_entropy = action_dim_safe * TARGET_ENTROPY_FACTOR  # configì—ì„œ ì„¤ì •í•œ ê°’ ì‚¬ìš©
        LOGGER.info(f"ğŸ“Š ì—”íŠ¸ë¡œí”¼ íƒ€ê²Ÿ: {self.target_entropy} (Factor: {TARGET_ENTROPY_FACTOR})")

        # í•™ìŠµ ë‹¨ê³„ ì¹´ìš´í„°
        self.train_step_counter = 0
        self.update_counter = 0

        # ëª¨ë¸ íƒ€ì… ê²€ì¦
        model_count = sum([use_cnn, use_lstm, use_mamba, use_tinytransformer])
        if model_count > 1:
            raise ValueError("CNN, LSTM, Mamba, TinyTransformer ëª¨ë¸ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # ëª¨ë¸ íƒ€ì… ë¡œê¹…
        if use_tinytransformer:
            LOGGER.info("ğŸ¤– TinyTransformer ê¸°ë°˜ SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        elif use_mamba:
            LOGGER.info("ğŸŒŠ Mamba ê¸°ë°˜ SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        elif use_lstm:
            LOGGER.info("LSTM ê¸°ë°˜ SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        elif use_cnn:
            LOGGER.info("ê°œì„ ëœ CNN ê¸°ë°˜ SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        else:
            LOGGER.info("MLP ê¸°ë°˜ SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")

        # TradingEnvironmentë¥¼ ìœ„í•œ ìƒíƒœ ì°¨ì› ìë™ ê³„ì‚°
        if not use_cnn and not use_lstm and not use_mamba and not use_tinytransformer and state_dim is None:
            if input_shape is None:
                raise ValueError("input_shapeë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")
            self.state_dim = input_shape[0] * input_shape[1] + 2
            state_dim = self.state_dim
            LOGGER.info(f"ğŸ“ ìƒíƒœ ì°¨ì› ìë™ ê³„ì‚°: {self.state_dim}")

        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self._initialize_networks()

        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(param.data)

        # âœ… ì˜µí‹°ë§ˆì´ì € ì„¤ì • (SAC í‘œì¤€: ê³ ì • í•™ìŠµë¥ )
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr, weight_decay=WEIGHT_DECAY)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr, weight_decay=WEIGHT_DECAY)
        
        # âœ… Alpha ìë™ íŠœë‹ (SAC í•µì‹¬ ê¸°ëŠ¥) - ì´ë¡ ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ì´ˆê¸°í™”
        if self.use_automatic_entropy_tuning:
            # âœ… ì•ˆì •ì ì¸ ì´ˆê¸°í™” (ë” ë³´ìˆ˜ì ì¸ alpha í•™ìŠµë¥ )
            conservative_alpha_lr = alpha_lr * 0.3  # Alpha í•™ìŠµë¥ ì„ ë” ë³´ìˆ˜ì ìœ¼ë¡œ
            
            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
            self.log_alpha.data.fill_(np.log(alpha_init))  # alpha_initì˜ ë¡œê·¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
            self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=conservative_alpha_lr)
            self.alpha = self.log_alpha.exp()
            
            LOGGER.info(f"ğŸ¯ SAC ìë™ ì—”íŠ¸ë¡œí”¼ íŠœë‹ í™œì„±í™”:")
            LOGGER.info(f"   â””â”€ ì´ˆê¸° Alpha: {self.alpha.item():.4f}")
            LOGGER.info(f"   â””â”€ ì´ˆê¸° log_Alpha: {self.log_alpha.item():.4f}")
            LOGGER.info(f"   â””â”€ Alpha í•™ìŠµë¥ : {conservative_alpha_lr:.6f} (ë³´ìˆ˜ì )")
            LOGGER.info(f"   â””â”€ Target Entropy: {self.target_entropy}")
        else:
            self.alpha = torch.tensor(self.alpha_init, device=device)
            LOGGER.info(f"ğŸ¯ ê³ ì • Alpha ì‚¬ìš©: {self.alpha.item():.4f}")
        
        # âœ… ë¦¬í”Œë ˆì´ ë²„í¼
        self.replay_buffer = ReplayBuffer(capacity=buffer_capacity)
        
        # âœ… ë°°ì¹˜ í¬ê¸° ì €ì¥ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ì—ì„œ í•„ìš”)
        self.batch_size = BATCH_SIZE
        
        # í•™ìŠµ í†µê³„
        self.actor_losses: List[float] = []
        self.critic_losses: List[float] = []
        self.alpha_losses: List[float] = []
        self.entropy_values: List[float] = []
        
        # ìµœì¢… ë¡œê·¸
        model_type_display = "LSTM" if use_lstm else ("CNN" if use_cnn else "MLP")
        LOGGER.info(f"ğŸ‰ SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        LOGGER.info(f"   â””â”€ ëª¨ë¸ íƒ€ì…: {model_type_display}")
        LOGGER.info(f"   â””â”€ í–‰ë™ ì°¨ì›: {action_dim}")
        LOGGER.info(f"   â””â”€ ìƒíƒœ ì°¨ì›: {self.state_dim if not (use_cnn or use_lstm) else input_shape}")
        LOGGER.info(f"   â””â”€ ì¥ì¹˜: {device}")
        if use_cnn:
            LOGGER.info(f"   â””â”€ CNN ì„¤ì • ì†ŒìŠ¤: {'ì‹¬ë³¼ë³„ ìµœì í™”' if symbol else 'ê¸°ë³¸ CNN ì„¤ì •'}")
        
    def _initialize_networks(self):
        """ê°œì„ ëœ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”"""
        if self.use_lstm:
            # LSTM ë„¤íŠ¸ì›Œí¬
            if self.input_shape is None:
                raise ValueError("LSTM ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” input_shapeê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            self.actor = LSTMActorNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                lstm_hidden_dim=self.lstm_hidden_dim,
                num_lstm_layers=self.num_lstm_layers,
                dropout=self.lstm_dropout,
                device=self.device
            )
            
            self.critic = LSTMCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                lstm_hidden_dim=self.lstm_hidden_dim,
                num_lstm_layers=self.num_lstm_layers,
                dropout=self.lstm_dropout,
                device=self.device
            )
            
            self.critic_target = LSTMCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                lstm_hidden_dim=self.lstm_hidden_dim,
                num_lstm_layers=self.num_lstm_layers,
                dropout=self.lstm_dropout,
                device=self.device
            )
            
        elif self.use_cnn:
            # ğŸ–¼ï¸ ê°œì„ ëœ CNN ë„¤íŠ¸ì›Œí¬ ì‚¬ìš© (config ê°’ ì ìš©)
            if self.input_shape is None:
                raise ValueError("CNN ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” input_shapeê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            LOGGER.info("ğŸ—ï¸ ê°œì„ ëœ CNN ë„¤íŠ¸ì›Œí¬ ìƒì„± ì¤‘...")
            
            self.actor = CNNActorNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                dropout_rate=self.dropout_rate,  # âœ… ìˆ˜ì •: cnn_dropout_rate â†’ dropout_rate
                device=self.device
            )
            
            self.critic = CNNCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                dropout_rate=self.dropout_rate,  # âœ… ìˆ˜ì •: cnn_dropout_rate â†’ dropout_rate
                device=self.device
            )
            
            self.critic_target = CNNCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                dropout_rate=self.dropout_rate,  # âœ… ìˆ˜ì •: cnn_dropout_rate â†’ dropout_rate
                device=self.device
            )
            
            LOGGER.info("âœ… ê°œì„ ëœ CNN ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            
        elif self.use_mamba:
            # ğŸŒŠ Mamba ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
            if self.input_shape is None:
                raise ValueError("Mamba ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” input_shapeê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            LOGGER.info("ğŸ—ï¸ Mamba ë„¤íŠ¸ì›Œí¬ ìƒì„± ì¤‘...")
            
            self.actor = MambaActorNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                mamba_layers=4,  # Default Mamba layers
                dropout_rate=self.dropout_rate,
                device=self.device
            )
            
            self.critic = MambaCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                mamba_layers=4,  # Default Mamba layers
                dropout_rate=self.dropout_rate,
                device=self.device
            )
            
            self.critic_target = MambaCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                mamba_layers=4,  # Default Mamba layers
                dropout_rate=self.dropout_rate,
                device=self.device
            )
            
            LOGGER.info("âœ… Mamba ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            
        elif self.use_tinytransformer:
            # ğŸ¤– TinyTransformer ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
            if self.input_shape is None:
                raise ValueError("TinyTransformer ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” input_shapeê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            LOGGER.info("ğŸ—ï¸ TinyTransformer ë„¤íŠ¸ì›Œí¬ ìƒì„± ì¤‘...")
            
            self.actor = TinyTransformerActorNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                d_model=128,  # Default transformer dimension
                nhead=4,      # Default number of attention heads
                num_layers=2, # Default number of transformer layers
                dropout_rate=self.dropout_rate,
                device=self.device
            )
            
            self.critic = TinyTransformerCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                d_model=128,
                nhead=4,
                num_layers=2,
                dropout_rate=self.dropout_rate,
                device=self.device
            )
            
            self.critic_target = TinyTransformerCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                d_model=128,
                nhead=4,
                num_layers=2,
                dropout_rate=self.dropout_rate,
                device=self.device
            )
            
            LOGGER.info("âœ… TinyTransformer ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
                
        else:
            # MLP ë„¤íŠ¸ì›Œí¬
            if self.state_dim is None:
                raise ValueError("ì¼ë°˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” state_dimì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            self.actor = ActorNetwork(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                device=self.device
            )
            
            self.critic = CriticNetwork(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                device=self.device
            )
            
            self.critic_target = CriticNetwork(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                device=self.device
            )
    def select_action(self, state: Union[Dict[str, np.ndarray], Dict[str, torch.Tensor], np.ndarray], evaluate: bool = False) -> float:
        """
        ìƒíƒœì— ë”°ë¥¸ í–‰ë™ ì„ íƒ (ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© í˜¸í™˜)
        """
        try:
            # ìƒíƒœë¥¼ ë„¤íŠ¸ì›Œí¬ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
            if isinstance(state, dict):
                state_tensor = self._process_state_for_network(state)
            elif isinstance(state, np.ndarray):
                if self.use_cnn or self.use_lstm or self.use_mamba or self.use_tinytransformer:
                    if self.input_shape and len(state) == (self.input_shape[0] * self.input_shape[1] + 2):
                        market_data_flat = state[:-2]
                        portfolio_state = state[-2:]
                        market_data = market_data_flat.reshape(self.input_shape)
                        state = {
                            'market_data': market_data,
                            'portfolio_state': portfolio_state
                        }
                        state_tensor = self._process_state_for_network(state)
                    else:
                        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                else:
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            elif isinstance(state, torch.Tensor):
                state_tensor = state.to(self.device)
                if state_tensor.dim() == 1:
                    state_tensor = state_tensor.unsqueeze(0)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìƒíƒœ íƒ€ì…: {type(state)}")
            
            # í–‰ë™ ì„ íƒ - BatchNorm ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ eval ëª¨ë“œ ì„¤ì •
            self.actor.eval()
            with torch.no_grad():
                if evaluate:
                    # âœ… Evaluation ëª¨ë“œ: ì•½ê°„ì˜ ë³€ë™ì„±ì„ ê°€ì§„ deterministic í–‰ë™
                    action, log_prob, mean = self.actor.sample(state_tensor)
                    # í‰ê· ê°’ ê¸°ë°˜ì´ì§€ë§Œ ì•½ê°„ì˜ ë³€ë™ì„± ìœ ì§€ (ì‹¤ì œ ìƒ˜í”Œë§ ê°’ ì‚¬ìš©)
                    action_value = float(action.detach().cpu().numpy().flatten()[0])
                    
                    # ì¶”ê°€ ì•ˆì •ì„±: ê·¹ë‹¨ê°’ í´ë¦¬í•‘
                    action_value = np.clip(action_value, -0.99, 0.99)
                else:
                    # âœ… Training ëª¨ë“œ: íƒí—˜ì  ìƒ˜í”Œë§
                    action, log_prob, mean = self.actor.sample(state_tensor)
                    action_value = float(action.detach().cpu().numpy().flatten()[0])
            self.actor.train()
            
            # í•™ìŠµ ì´ˆê¸°ì— ì¶”ê°€ íƒìƒ‰ ë…¸ì´ì¦ˆ ì£¼ì… (í›ˆë ¨ ëª¨ë“œì—ë§Œ)
            if not evaluate and self.train_step_counter < 5000:
                noise_std = max(0.1, 0.5 * (1 - self.train_step_counter / 5000))
                noise = np.random.normal(0, noise_std)
                action_value = np.clip(action_value + noise, -1.0, 1.0)
                
            return action_value
            
        except Exception as e:
            LOGGER.error(f"í–‰ë™ ì„ íƒ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0.0

    def _process_state_for_network(self, state: Dict[str, Union[np.ndarray, torch.Tensor]]) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        ìƒíƒœë¥¼ ë„¤íŠ¸ì›Œí¬ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
        
        Args:
            state: TradingEnvironment ìƒíƒœ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ë„¤íŠ¸ì›Œí¬ ì…ë ¥ìš© í…ì„œ ë˜ëŠ” í…ì„œ ë”•ì…”ë„ˆë¦¬
        """
        try:
            if self.use_cnn or self.use_lstm or self.use_mamba or self.use_tinytransformer:
                # CNN / LSTM / Mamba / TinyTransformer: ìƒíƒœë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ìœ ì§€
                market_data = state["market_data"]
                portfolio_state = state["portfolio_state"]
                
                # numpy ë°°ì—´ì„ í…ì„œë¡œ ë³€í™˜
                if isinstance(market_data, np.ndarray):
                    market_tensor = torch.FloatTensor(market_data).to(self.device)
                else:
                    market_tensor = market_data.to(self.device)
                    
                if isinstance(portfolio_state, np.ndarray):
                    portfolio_tensor = torch.FloatTensor(portfolio_state).to(self.device)
                else:
                    portfolio_tensor = portfolio_state.to(self.device)
                
                # ë°°ì¹˜ ì°¨ì› í™•ì¸ ë° ì¶”ê°€
                if market_tensor.dim() == 2:  # (window_size, feature_dim)
                    market_tensor = market_tensor.unsqueeze(0)  # (1, window_size, feature_dim)
                if portfolio_tensor.dim() == 1:  # (2,)
                    portfolio_tensor = portfolio_tensor.unsqueeze(0)  # (1, 2)
                
                return {
                    "market_data": market_tensor,
                    "portfolio_state": portfolio_tensor
                }
            else:
                # MLP: ìƒíƒœë¥¼ flatten
                market_data = state['market_data']
                portfolio_state = state['portfolio_state']
                
                # numpy ë°°ì—´ì„ í…ì„œë¡œ ë³€í™˜
                if isinstance(market_data, np.ndarray):
                    market_flat = market_data.flatten()
                else:
                    market_flat = market_data.flatten().cpu().numpy()
                    
                if isinstance(portfolio_state, np.ndarray):
                    portfolio_flat = portfolio_state
                else:
                    portfolio_flat = portfolio_state.cpu().numpy()
                
                # ê²°í•©
                combined_state = np.concatenate([market_flat, portfolio_flat])
                
                return torch.FloatTensor(combined_state).unsqueeze(0).to(self.device)
            
        except Exception as e:
            LOGGER.error(f"ìƒíƒœ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ìƒíƒœ ë°˜í™˜
            if self.use_cnn or self.use_lstm or self.use_mamba or self.use_tinytransformer:
                if self.input_shape is not None:
                    return {
                        "market_data": torch.zeros((1, self.input_shape[0], self.input_shape[1]), device=self.device),
                        "portfolio_state": torch.zeros((1, 2), device=self.device)
                    }
                else:
                    return {
                        "market_data": torch.zeros((1, 10, 5), device=self.device),  # fallback dimensions
                        "portfolio_state": torch.zeros((1, 2), device=self.device)
                    }
            else:
                return torch.zeros((1, self.state_dim or 1), device=self.device)

    def _process_batch_states(self, states: List[Dict[str, np.ndarray]]) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        ë°°ì¹˜ ìƒíƒœë“¤ì„ ë„¤íŠ¸ì›Œí¬ ì…ë ¥ìœ¼ë¡œ ë³€í™˜
        """
        # ğŸ”„ GPU ë©”ëª¨ë¦¬ ì²´í¬ë¥¼ ë§¨ ì•ìœ¼ë¡œ ì´ë™
        if self.device.type == 'cuda':
            total_memory_gb = torch.cuda.get_device_properties(self.device).total_memory / 1024**3
            memory_threshold = total_memory_gb * 0.85
            memory_allocated = torch.cuda.memory_allocated() / 1024**3
            
            if memory_allocated > memory_threshold:
                LOGGER.warning(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_allocated:.1f}GB/{total_memory_gb:.1f}GB ({memory_allocated/total_memory_gb*100:.1f}%)")
        
        if self.use_cnn or self.use_lstm or self.use_mamba or self.use_tinytransformer:
            market_batch = []
            portfolio_batch = []
            for state in states:
                market_batch.append(state["market_data"])
                portfolio_batch.append(state["portfolio_state"])
            
            market_tensor = torch.FloatTensor(np.stack(market_batch)).to(self.device)
            portfolio_tensor = torch.FloatTensor(np.stack(portfolio_batch)).to(self.device)

            return {
                "market_data": market_tensor,
                "portfolio_state": portfolio_tensor
            }
        else:
            batch_states = []
            for state in states:
                processed_state = self._process_state_for_network(state)
                batch_states.append(processed_state)
            return torch.cat(batch_states, dim=0).to(self.device)
        
    def add_experience(self, state: Dict[str, np.ndarray], action: float, reward: float, 
                      next_state: Dict[str, np.ndarray], done: bool) -> None:
        """
        ê²½í—˜ì„ ë¦¬í”Œë ˆì´ ë²„í¼ì— ì¶”ê°€
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
            action: ìˆ˜í–‰í•œ í–‰ë™
            reward: ë°›ì€ ë³´ìƒ
            next_state: ë‹¤ìŒ ìƒíƒœ
            done: ì¢…ë£Œ ì—¬ë¶€
        """
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update_parameters(self, batch_size: int = BATCH_SIZE) -> Dict[str, float]:
        """í†µí•©ëœ ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ì•ˆì •ì„± ê°œì„ )"""
        if len(self.replay_buffer) < batch_size:
            return {
                'actor_loss': 0.0,
                'critic_loss': 0.0,
                'alpha_loss': 0.0,
                'entropy': 0.0,
                'alpha': self.alpha.item() if isinstance(self.alpha, torch.Tensor) else self.alpha
            }
        
        # ëœë¤ ìƒ˜í”Œë§
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
        
        # ë°°ì¹˜ í…ì„œ ë³€í™˜
        batched_states = self._process_batch_states(states)
        batched_next_states = self._process_batch_states(next_states)
        
        batched_actions = torch.FloatTensor(actions).unsqueeze(1).to(self.device)
        batched_rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
        batched_dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)
        
        # âœ… í†µí•©ëœ ì—…ë°ì´íŠ¸ (ëª¨ë“  ëª¨ë¸ì— ë™ì¼í•œ ì•ˆì •ì„± ì ìš©)
        critic_loss = self._update_critic(
            batched_states, batched_actions, batched_rewards, batched_next_states, batched_dones
        )
        actor_loss = self._update_actor(batched_states)
        alpha_loss = self._update_alpha(batched_states)
        
        # âœ… ê°œì„ ëœ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.train_step_counter += 1
        self.update_counter += 1  # âœ… update_counter ì¦ê°€ ì¶”ê°€
        if self.train_step_counter % self.target_update_interval == 0:
            soft_update(self.critic_target, self.critic, self.tau)
        
        # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ì˜¬ë°”ë¥¸ ê³µì‹ + ë””ë²„ê¹…)
        entropy_value = 0.0
        if self.update_counter % 10 == 0:
            with torch.no_grad():
                _, log_probs, _ = self.actor.sample(batched_states)
                # âœ… ì •í™•í•œ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°: H = -E[log Ï€(a|s)]
                entropy_value = -log_probs.mean().item()
                
                # ğŸ” ë””ë²„ê¹…: ë§¤ìš° ì‘ì€ ìŒìˆ˜ ì—”íŠ¸ë¡œí”¼ ê°ì§€
                if entropy_value < -0.1:  # ì˜ë¯¸ìˆëŠ” ìŒìˆ˜ ì—”íŠ¸ë¡œí”¼ë§Œ ê²½ê³ 
                    LOGGER.debug(f"ğŸ“Š ì—”íŠ¸ë¡œí”¼ ë””ë²„ê¹… - log_probs í‰ê· : {log_probs.mean().item():.6f}, "
                               f"ì—”íŠ¸ë¡œí”¼: {entropy_value:.6f}")

        # í•™ìŠµ í†µê³„
        stats = {
            'actor_loss': actor_loss,
            'critic_loss': critic_loss,
            'alpha_loss': alpha_loss if self.use_automatic_entropy_tuning else 0.0,
            'entropy': entropy_value,
            'alpha': self.alpha.item()
        }
        
        self.actor_losses.append(stats['actor_loss'])
        self.critic_losses.append(stats['critic_loss'])
        self.alpha_losses.append(stats['alpha_loss'])
        self.entropy_values.append(stats['entropy'])
        
        return stats
    
    def _update_critic(self, states, actions, rewards, next_states, dones):
        """í†µí•©ëœ Critic ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ì•ˆì •ì„± ê°œì„ )"""
        # í˜„ì¬ ì •ì±…ì—ì„œ ë‹¤ìŒ í–‰ë™ ìƒ˜í”Œë§
        with torch.no_grad():
            next_actions, next_log_probs, _ = self.actor.sample(next_states)
            
            # íƒ€ê²Ÿ Q ê°’ ê³„ì‚°
            next_q1_target, next_q2_target = self.critic_target(next_states, next_actions)
            next_q_target = torch.min(next_q1_target, next_q2_target)
            next_q_target = next_q_target - self.alpha * next_log_probs
            expected_q = rewards + (1.0 - dones) * self.gamma * next_q_target
        
        current_q1, current_q2 = self.critic(states, actions)
        
        q1_loss = F.mse_loss(current_q1, expected_q)
        q2_loss = F.mse_loss(current_q2, expected_q)
        
        critic_loss = q1_loss + q2_loss
        
        # âœ… ì•ˆì •ì„± ê°œì„ : ì†ì‹¤ ê°’ ì²´í¬
        if torch.isnan(critic_loss) or torch.isinf(critic_loss):
            LOGGER.warning("Critic ì†ì‹¤ì— NaN/Inf ê°ì§€ë¨. ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€.")
            return 0.0
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        
        # âœ… í†µí•©ëœ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ëª¨ë“  ëª¨ë¸ì— ì ìš©)
        if self.use_gradient_clipping:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.critic.parameters(), 
                max_norm=self.gradient_clip_norm
            )
            
            # ê·¹ë‹¨ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ê°ì§€
            if grad_norm > self.gradient_clip_norm * 5:
                LOGGER.warning(f"Critic í° ê·¸ë˜ë””ì–¸íŠ¸ ê°ì§€: {grad_norm:.6f}")
        
        self.critic_optimizer.step()
        
        return critic_loss.item()

    def _update_actor(self, states):
        """í†µí•©ëœ Actor ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ì•ˆì •ì„± ê°œì„ )"""
        new_actions, log_probs, _ = self.actor.sample(states)
        q1, q2 = self.critic(states, new_actions)
        q = torch.min(q1, q2)
        
        actor_loss = (self.alpha * log_probs - q).mean()
        
        # âœ… ì•ˆì •ì„± ê°œì„ : ì†ì‹¤ ê°’ ì²´í¬
        if torch.isnan(actor_loss) or torch.isinf(actor_loss):
            LOGGER.warning("Actor ì†ì‹¤ì— NaN/Inf ê°ì§€ë¨. ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€.")
            return 0.0
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        
        # âœ… í†µí•©ëœ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        if self.use_gradient_clipping:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.actor.parameters(), 
                max_norm=self.gradient_clip_norm
            )
            
            if grad_norm > self.gradient_clip_norm * 5:
                LOGGER.warning(f"Actor í° ê·¸ë˜ë””ì–¸íŠ¸ ê°ì§€: {grad_norm:.6f}")
        
        self.actor_optimizer.step()
        
        return actor_loss.item()

    def _update_alpha(self, states):
        """í‘œì¤€ SAC Alpha ì—…ë°ì´íŠ¸ (ì´ë¡ ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë°©ë²•)"""
        if not self.use_automatic_entropy_tuning:
            return 0.0
        
        # í˜„ì¬ ì •ì±…ì˜ í–‰ë™ê³¼ log_prob ìƒ˜í”Œë§
        with torch.no_grad():
            _, log_probs, _ = self.actor.sample(states)
            current_entropy = -log_probs.mean().item()
        
        # âœ… SAC í‘œì¤€: ì—”íŠ¸ë¡œí”¼ í†µê³„ ìˆ˜ì§‘
        if self.adaptive_alpha:
            self.recent_entropies.append(current_entropy)
        
        # âœ… SAC í‘œì¤€ Alpha Loss ê³µì‹ (detach() ì˜¬ë°”ë¥¸ ìœ„ì¹˜)
        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
        
        # âœ… ì•ˆì •ì„± ì²´í¬ (NaN/Infë§Œ ì²´í¬)
        if torch.isnan(alpha_loss) or torch.isinf(alpha_loss):
            LOGGER.warning("Alpha ì†ì‹¤ì— NaN/Inf ê°ì§€ë¨. ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€.")
            return 0.0
        
        # âœ… ì ì‘ì  í•™ìŠµë¥  (ê·¹ë‹¨ì  loss ì‹œì—ë§Œ)
        current_alpha_lr = self.alpha_lr
        if abs(alpha_loss.item()) > 5:  # ì„ê³„ê°’ ì™„í™”
            current_alpha_lr = self.alpha_lr * 0.5
            LOGGER.debug(f"Alpha Loss í¼: {alpha_loss.item():.4f}, í•™ìŠµë¥  ì„ì‹œ ê°ì†Œ")
        
        # Alpha ì—…ë°ì´íŠ¸
        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        
        # âœ… ë³´ìˆ˜ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (alphaë§Œ)
        if self.use_gradient_clipping:
            torch.nn.utils.clip_grad_norm_([self.log_alpha], max_norm=1.0)
        
        # âœ… ì ì‘ì  í•™ìŠµë¥  ì ìš©
        for param_group in self.alpha_optimizer.param_groups:
            param_group['lr'] = current_alpha_lr
        
        self.alpha_optimizer.step()
        
        # âœ… í•™ìŠµë¥  ë³µì›
        for param_group in self.alpha_optimizer.param_groups:
            param_group['lr'] = self.alpha_lr
        
        # Alpha ê°’ ì—…ë°ì´íŠ¸ (ì œí•œ ì—†ìŒ - SAC ì´ë¡  ì¤€ìˆ˜)
        self.alpha = self.log_alpha.exp()
        
        # âœ… ê²½ê³ ë§Œ ì¶œë ¥ (ê°’ ì œí•œí•˜ì§€ ì•ŠìŒ)
        if self.alpha.item() > 10:
            LOGGER.warning(f"Alpha ê°’ì´ í¼: {self.alpha.item():.4f} - í•™ìŠµì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ")
        elif self.alpha.item() < 0.001:
            LOGGER.warning(f"Alpha ê°’ì´ ì‘ìŒ: {self.alpha.item():.6f} - íƒí—˜ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŒ")
        
        # âœ… ë””ë²„ê¹… ì •ë³´ (ì£¼ê¸°ì ) alphaê°’ ë””ë²„ê¹… 100ìŠ¤í… ë§ˆë‹¤ ì°ê³  ì‹¶ìœ¼ë©´ % 100
        if hasattr(self, 'update_counter') and self.update_counter % 1000 == 0:
            LOGGER.info(f"ğŸ” Alpha ìƒíƒœ - Loss: {alpha_loss.item():.4f}, Alpha: {self.alpha.item():.4f}, "
                       f"Entropy: {current_entropy:.4f}, Target: {self.target_entropy}")
        
        return alpha_loss.item()
    
    def save_model(self, save_dir: Optional[Union[str, Path]] = None, prefix: str = "",
                model_type: Optional[str] = None, symbol: Optional[str] = None, symbols: Optional[List[str]] = None) -> str:
            """
            ëª¨ë¸ ì €ì¥ (ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© í˜¸í™˜ì„± ê°œì„ )

            Args:
                save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
                prefix: íŒŒì¼ëª… ì ‘ë‘ì‚¬
                model_type: ëª¨ë¸ íƒ€ì… ('mlp', 'cnn', 'lstm')
                symbol: ë‹¨ì¼ ì‹¬ë³¼ (MLPìš©)
                symbols: ë‹¤ì¤‘ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸

            Returns:
                str: ì €ì¥ëœ ëª¨ë¸ì˜ ê²½ë¡œ
            """
            if save_dir is None:
                save_dir = MODELS_DIR

            create_directory(save_dir)

            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            timestamp = time.strftime("%Y%m%d_%H%M%S")

            # ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€ (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
            if model_type is None:
                model_type = getattr(self, 'model_type', None)
                if model_type is None:
                    if self.use_lstm:
                        model_type = 'lstm'
                    elif self.use_cnn:
                        model_type = 'cnn'
                    else:
                        model_type = 'mlp'

            # íŒŒì¼ëª… íŒ¨í„´ ìƒì„±
            if model_type.lower() == 'mlp':
                # MLP: ì‹¬ë³¼ë³„ë¡œ êµ¬ë¶„
                if symbol:
                    model_name = f"final_sac_model_{symbol}_{timestamp}"
                elif symbols and len(symbols) == 1:
                    model_name = f"final_sac_model_{symbols[0]}_{timestamp}"
                else:
                    # ë‹¤ì¤‘ ì‹¬ë³¼ì´ê±°ë‚˜ ì‹¬ë³¼ ì •ë³´ ì—†ìŒ
                    model_name = f"final_sac_model_multi_{timestamp}"
            else:
                # CNN, LSTM: ì‹¬ë³¼ êµ¬ë¶„ ì—†ìŒ
                model_name = f"final_{model_type.lower()}_sac_model_{timestamp}"

            # ì ‘ë‘ì‚¬ ì ìš©
            if prefix:
                model_name = f"{prefix}{model_name}"

            # ì €ì¥ ê²½ë¡œ ìƒì„±
            model_path = Path(save_dir) / model_name
            create_directory(model_path)

            # ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ì €ì¥
            torch.save(self.actor.state_dict(), model_path / "actor.pth")
            torch.save(self.critic.state_dict(), model_path / "critic.pth")
            torch.save(self.critic_target.state_dict(), model_path / "critic_target.pth")

            # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ì €ì¥
            torch.save(self.actor_optimizer.state_dict(), model_path / "actor_optimizer.pth")
            torch.save(self.critic_optimizer.state_dict(), model_path / "critic_optimizer.pth")

            # Alpha ê´€ë ¨ ìƒíƒœ ì €ì¥
            if self.use_automatic_entropy_tuning:
                torch.save(self.log_alpha, model_path / "log_alpha.pth")
                torch.save(self.alpha_optimizer.state_dict(), model_path / "alpha_optimizer.pth")

            # í•™ìŠµ í†µê³„ ì €ì¥
            training_stats = {
                'actor_losses': self.actor_losses,
                'critic_losses': self.critic_losses,
                'alpha_losses': self.alpha_losses,
                'entropy_values': self.entropy_values,
                'train_step_counter': self.train_step_counter
            }
            torch.save(training_stats, model_path / "training_stats.pth")

            # âœ… ì„¤ì • ì €ì¥ (CNN config ì •ë³´ í¬í•¨) - ì•ˆì „í•œ ì†ì„± ì ‘ê·¼ìœ¼ë¡œ ìˆ˜ì •
            config = {
                # ê¸°ë³¸ ëª¨ë¸ ì •ë³´
                'model_type': model_type.lower(),
                'symbol': symbol or getattr(self, 'symbol', None),
                'symbols': symbols,
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'hidden_dim': self.hidden_dim,
                
                # ëª¨ë¸ ì•„í‚¤í…ì²˜
                'use_cnn': self.use_cnn,
                'use_lstm': self.use_lstm,
                'input_shape': self.input_shape,
                
                # LSTM ì „ìš© íŒŒë¼ë¯¸í„°
                'lstm_hidden_dim': getattr(self, 'lstm_hidden_dim', None),
                'num_lstm_layers': getattr(self, 'num_lstm_layers', None),
                'lstm_dropout': getattr(self, 'lstm_dropout', None),
                
                # âœ… CNN/ê³µí†µ íŒŒë¼ë¯¸í„° (ì•ˆì „í•œ ì ‘ê·¼ìœ¼ë¡œ ìˆ˜ì •)
                'dropout_rate': getattr(self, 'dropout_rate', None),
                'learning_rate_actor': getattr(self, 'actor_lr', None),
                'learning_rate_critic': getattr(self, 'critic_lr', None),
                'learning_rate_alpha': getattr(self, 'alpha_lr', None),
                'alpha_init': getattr(self, 'alpha_init', None),
                'gradient_clip_norm': getattr(self, 'gradient_clip_norm', None),
                
                # ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© í˜¸í™˜ì„± ì •ë³´
                'realtime_compatible': True,
                'supports_dict_state': self.use_cnn or self.use_lstm,
                'supports_flat_state': not (self.use_cnn or self.use_lstm),
                'state_format': 'dict' if (self.use_cnn or self.use_lstm) else 'flat',
                
                # ë©”íƒ€ ì •ë³´
                'timestamp': timestamp,
                'saved_at': time.strftime("%Y-%m-%d %H:%M:%S"),
                'device': str(self.device),
                'framework': 'pytorch',
                'sac_version': '2.0'
            }
            torch.save(config, model_path / "config.pth")

            model_type_display = model_type.upper()
            LOGGER.info(f"âœ… {model_type_display} ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
            LOGGER.info(f"   â””â”€ ëª¨ë¸ íƒ€ì…: {model_type_display}")
            LOGGER.info(f"   â””â”€ ì‹¬ë³¼: {symbol or symbols or 'Multi'}")
            LOGGER.info(f"   â””â”€ ì‹¤ì‹œê°„ í˜¸í™˜: {'YES' if config['realtime_compatible'] else 'NO'}")
            LOGGER.info(f"   â””â”€ ìƒíƒœ í˜•ì‹: {config['state_format'].upper()}")
            LOGGER.info(f"   â””â”€ íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")
            
            # âœ… ëª¨ë¸ë³„ ì„¤ì • ì •ë³´ ì¶œë ¥ (ì•ˆì „í•œ ì ‘ê·¼ìœ¼ë¡œ ìˆ˜ì •)
            if self.use_cnn:
                dropout_rate = getattr(self, 'dropout_rate', 'N/A')
                actor_lr = getattr(self, 'actor_lr', 'N/A')
                LOGGER.info(f"   â””â”€ CNN ì„¤ì •: Dropout={dropout_rate}, Actor LR={actor_lr}")
            elif self.use_lstm:
                lstm_hidden = getattr(self, 'lstm_hidden_dim', 'N/A')
                lstm_layers = getattr(self, 'num_lstm_layers', 'N/A')
                LOGGER.info(f"   â””â”€ LSTM ì„¤ì •: Hidden={lstm_hidden}, Layers={lstm_layers}")
            else:
                actor_lr = getattr(self, 'actor_lr', 'N/A')
                LOGGER.info(f"   â””â”€ MLP ì„¤ì •: Actor LR={actor_lr}")

            return str(model_path)
        
    def load_model(self, model_path: Union[str, Path]) -> None:
        """
        ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_path: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        model_path = Path(model_path)
        
        if not model_path.exists():
            LOGGER.error(f"âŒ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
            return
        
        # ëª¨ë¸ íƒ€ì… ê°ì§€ ë° ë¡œê·¸
        model_name = model_path.name.lower()
        # í˜„ì¬ ì—ì´ì „íŠ¸ ì„¤ì •ì„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ íƒ€ì… íŒë‹¨
        if self.use_lstm:
            LOGGER.info(f"ğŸ”„ LSTM SAC ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        elif self.use_cnn:
            LOGGER.info(f"ğŸ”„ CNN SAC ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        elif self.use_mamba:
            LOGGER.info(f"ğŸ”„ Mamba SAC ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        else:
            LOGGER.info(f"ğŸ”„ MLP SAC ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        
        try:
            # ë¨¼ì € ì›ë˜ ë°©ì‹ìœ¼ë¡œ ë¡œë“œ ì‹œë„
            self.actor.load_state_dict(torch.load(model_path / "actor.pth", map_location=self.device))
            self.critic.load_state_dict(torch.load(model_path / "critic.pth", map_location=self.device))
            self.critic_target.load_state_dict(torch.load(model_path / "critic_target.pth", map_location=self.device))
            
            # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ
            self.actor_optimizer.load_state_dict(torch.load(model_path / "actor_optimizer.pth", map_location=self.device))
            self.critic_optimizer.load_state_dict(torch.load(model_path / "critic_optimizer.pth", map_location=self.device))
            
            # ì˜µí‹°ë§ˆì´ì € ìƒíƒœë„ GPUë¡œ ì´ë™
            if self.device.type == 'cuda':
                for state in self.actor_optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(self.device)
                            
                for state in self.critic_optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(self.device)
            
            # Alpha ê´€ë ¨ ë¡œë“œ
            if self.use_automatic_entropy_tuning:
                self.log_alpha = torch.load(model_path / "log_alpha.pth", map_location=self.device)
                self.log_alpha.requires_grad = True
                self.alpha = self.log_alpha.exp()
                self.alpha_optimizer.load_state_dict(torch.load(model_path / "alpha_optimizer.pth", map_location=self.device))
                
                # alpha_optimizer ìƒíƒœë„ GPUë¡œ ì´ë™
                if self.device.type == 'cuda':
                    for state in self.alpha_optimizer.state.values():
                        for k, v in state.items():
                            if isinstance(v, torch.Tensor):
                                state[k] = v.to(self.device)
            else:
                self.alpha = torch.tensor(self.alpha_init, device=self.device)
            
            # í•™ìŠµ í†µê³„ ë¡œë“œ
            training_stats = torch.load(model_path / "training_stats.pth", map_location=self.device)
            self.actor_losses = training_stats.get('actor_losses', [])
            self.critic_losses = training_stats.get('critic_losses', [])
            self.alpha_losses = training_stats.get('alpha_losses', [])
            self.entropy_values = training_stats.get('entropy_values', [])
            self.train_step_counter = training_stats.get('train_step_counter', 0)
            
            # ì„±ê³µ ë¡œê·¸
            model_type = "LSTM" if self.use_lstm else ("CNN" if self.use_cnn else "MLP")
            LOGGER.info(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            LOGGER.info(f"   â””â”€ í•™ìŠµ ìŠ¤í…: {self.train_step_counter:,}")
            LOGGER.info(f"   â””â”€ ë²„í¼ í¬ê¸°: {len(self.replay_buffer):,}")
            
        except RuntimeError as e:
            # í¬ê¸° ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ í¬ê¸° ì¡°ì • ë¡œë“œ ë©”ì„œë“œ ì‚¬ìš©
            LOGGER.warning(f"âš ï¸ í‘œì¤€ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            LOGGER.info("ğŸ”„ í¬ê¸° ì¡°ì • ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„ ì¤‘...")
            self.load_model_with_resize(model_path)
    
    def get_latest_model_path(self, save_dir: Union[str, Path] = None, prefix: str = '') -> Optional[Path]:
        """
        ìµœì‹  ëª¨ë¸ ê²½ë¡œ ë°˜í™˜
        
        Args:
            save_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ (Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©)
            prefix: íŒŒì¼ëª… ì ‘ë‘ì‚¬
            
        Returns:
            ìµœì‹  ëª¨ë¸ ê²½ë¡œ (ì—†ìœ¼ë©´ None)
        """
        if save_dir is None:
            save_dir = MODELS_DIR
        
        save_dir = Path(save_dir)
        if not save_dir.exists():
            return None
        
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ íŒ¨í„´ í™•ì¸
        patterns = [f"{prefix}lstm_sac_model_", f"{prefix}cnn_sac_model_", f"{prefix}mlp_sac_model_", f"{prefix}sac_model_"]
        
        model_dirs = []
        for pattern in patterns:
            model_dirs.extend([d for d in save_dir.iterdir() if d.is_dir() and d.name.startswith(pattern)])
        
        if not model_dirs:
            return None
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        model_dirs.sort(key=lambda d: d.name, reverse=True)
        return model_dirs[0]
    
    def load_model_with_resize(self, model_path):
        """
        í¬ê¸°ê°€ ë‹¤ë¥¸ ëª¨ë¸ì„ ë¶€ë¶„ì ìœ¼ë¡œ ë¡œë“œ
        
        Args:
            model_path: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        model_path = Path(model_path)
        
        LOGGER.info("ğŸ”§ í¬ê¸° ì¡°ì • ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„ ì¤‘...")
        
        # ì €ì¥ëœ ìƒíƒœ ì‚¬ì „ ë¡œë“œ
        saved_actor_dict = torch.load(model_path / "actor.pth", map_location=self.device)
        saved_critic_dict = torch.load(model_path / "critic.pth", map_location=self.device)
        
        # í˜„ì¬ ëª¨ë¸ì˜ ìƒíƒœ ì‚¬ì „
        current_actor_dict = self.actor.state_dict()
        current_critic_dict = self.critic.state_dict()
        
        # ğŸ†• CNN/LSTM ëª¨ë¸ ì „ìš© í˜¸í™˜ì„± ì²˜ë¦¬
        if self.use_cnn or self.use_lstm:
            LOGGER.info("ğŸ–¼ï¸ CNN/LSTM ëª¨ë¸ í˜¸í™˜ì„± ì²˜ë¦¬ ì¤‘...")
            
            # CNN ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ í˜¸í™˜ì„± ê²€ì‚¬
            if self.use_cnn:
                # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ì…ë ¥ ì±„ë„ ìˆ˜ í™•ì¸
                saved_conv1_weight = saved_actor_dict.get('conv_block1.0.weight')  # ì²« ë²ˆì§¸ Conv1d
                current_conv1_weight = current_actor_dict.get('conv_block1.0.weight')
                
                if saved_conv1_weight is not None and current_conv1_weight is not None:
                    saved_in_channels = saved_conv1_weight.shape[1]  # [out_ch, in_ch, kernel]
                    current_in_channels = current_conv1_weight.shape[1]
                    
                    LOGGER.info(f"ğŸ” CNN ì…ë ¥ ì±„ë„ ë¹„êµ:")
                    LOGGER.info(f"   â””â”€ ì €ì¥ëœ ëª¨ë¸: {saved_in_channels}ê°œ ì±„ë„")
                    LOGGER.info(f"   â””â”€ í˜„ì¬ ëª¨ë¸: {current_in_channels}ê°œ ì±„ë„")
                    
                    if saved_in_channels != current_in_channels:
                        LOGGER.warning(f"âš ï¸ CNN ì…ë ¥ ì±„ë„ ë¶ˆì¼ì¹˜! ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ì œì™¸í•˜ê³  ë¡œë“œ")
                        
                        # ì»¨ë³¼ë£¨ì…˜ ê´€ë ¨ ë ˆì´ì–´ ì œì™¸ ëª©ë¡
                        conv_layer_prefixes = [
                            'conv_block1', 'conv_block2', 'conv_block3',
                            'q1_conv_block1', 'q1_conv_block2', 'q1_conv_block3',
                            'q2_conv_block1', 'q2_conv_block2', 'q2_conv_block3'
                        ]
                        
                        # Actorì—ì„œ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ì œì™¸
                        filtered_actor_dict = {}
                        for k, v in saved_actor_dict.items():
                            if not any(k.startswith(prefix) for prefix in conv_layer_prefixes):
                                if k in current_actor_dict and v.shape == current_actor_dict[k].shape:
                                    filtered_actor_dict[k] = v
                        
                        # Criticì—ì„œ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ì œì™¸  
                        filtered_critic_dict = {}
                        for k, v in saved_critic_dict.items():
                            if not any(k.startswith(prefix) for prefix in conv_layer_prefixes):
                                if k in current_critic_dict and v.shape == current_critic_dict[k].shape:
                                    filtered_critic_dict[k] = v
                        
                        LOGGER.info(f"ğŸ“Š CNN í˜¸í™˜ ë¡œë”© ê²°ê³¼:")
                        LOGGER.info(f"   â””â”€ Actor: {len(filtered_actor_dict)}/{len(current_actor_dict)} ë ˆì´ì–´")
                        LOGGER.info(f"   â””â”€ Critic: {len(filtered_critic_dict)}/{len(current_critic_dict)} ë ˆì´ì–´")
                        LOGGER.info(f"   â””â”€ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤")
                        
                        # í•„í„°ë§ëœ ê°€ì¤‘ì¹˜ ì ìš©
                        current_actor_dict.update(filtered_actor_dict)
                        current_critic_dict.update(filtered_critic_dict)
                        
                    else:
                        # ì±„ë„ ìˆ˜ê°€ ê°™ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                        LOGGER.info("âœ… CNN ì…ë ¥ ì±„ë„ ì¼ì¹˜, ì¼ë°˜ í˜¸í™˜ì„± ê²€ì‚¬ ì§„í–‰")
                        actor_dict = {k: v for k, v in saved_actor_dict.items() 
                                    if k in current_actor_dict and v.shape == current_actor_dict[k].shape}
                        critic_dict = {k: v for k, v in saved_critic_dict.items() 
                                    if k in current_critic_dict and v.shape == current_critic_dict[k].shape}
                        
                        current_actor_dict.update(actor_dict)
                        current_critic_dict.update(critic_dict)
                # LSTM ëª¨ë¸ í˜¸í™˜ì„± ì²˜ë¦¬
                elif self.use_lstm:
                    LOGGER.info("ğŸ§  LSTM ëª¨ë¸ í˜¸í™˜ì„± ì²˜ë¦¬")
                    # LSTMì˜ ê²½ìš° hidden_sizeê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
                    lstm_layer_prefixes = ['lstm', 'q1_lstm', 'q2_lstm']
                    
                    filtered_actor_dict = {}
                    for k, v in saved_actor_dict.items():
                        if not any(k.startswith(prefix) for prefix in lstm_layer_prefixes):
                            if k in current_actor_dict and v.shape == current_actor_dict[k].shape:
                                filtered_actor_dict[k] = v
                    
                    filtered_critic_dict = {}
                    for k, v in saved_critic_dict.items():
                        if not any(k.startswith(prefix) for prefix in lstm_layer_prefixes):
                            if k in current_critic_dict and v.shape == current_critic_dict[k].shape:
                                filtered_critic_dict[k] = v
                    
                    current_actor_dict.update(filtered_actor_dict)
                    current_critic_dict.update(filtered_critic_dict)
                    
                    LOGGER.info(f"ğŸ“Š LSTM í˜¸í™˜ ë¡œë”© ê²°ê³¼:")
                    LOGGER.info(f"   â””â”€ LSTM ë ˆì´ì–´ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤")
        
        # í¬ê¸°ê°€ ì¼ì¹˜í•˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ ë¡œë“œ
        else:
            actor_dict = {k: v for k, v in saved_actor_dict.items() if k in current_actor_dict and v.shape == current_actor_dict[k].shape}
            critic_dict = {k: v for k, v in saved_critic_dict.items() if k in current_critic_dict and v.shape == current_critic_dict[k].shape}
            
            # í˜¸í™˜ í†µê³„ ë¡œê·¸
            actor_loaded = len(actor_dict)
            actor_total = len(current_actor_dict)
            critic_loaded = len(critic_dict)
            critic_total = len(current_critic_dict)
            
            LOGGER.info(f"ğŸ“Š MLP ëª¨ë¸ í˜¸í™˜ì„± ë¶„ì„:")
            LOGGER.info(f"   â””â”€ Actor: {actor_loaded}/{actor_total} ë ˆì´ì–´ í˜¸í™˜ ({actor_loaded/actor_total*100:.1f}%)")
            LOGGER.info(f"   â””â”€ Critic: {critic_loaded}/{critic_total} ë ˆì´ì–´ í˜¸í™˜ ({critic_loaded/critic_total*100:.1f}%)")
        
            # ìƒíƒœ ì‚¬ì „ ì—…ë°ì´íŠ¸
            current_actor_dict.update(actor_dict)
            current_critic_dict.update(critic_dict)
        
        # ëª¨ë¸ì— ì ìš©
        self.actor.load_state_dict(current_actor_dict)
        self.critic.load_state_dict(current_critic_dict)
        
        # critic_target ì—…ë°ì´íŠ¸
        try:
            saved_critic_target_dict = torch.load(model_path / "critic_target.pth", map_location=self.device)
            current_critic_target_dict = self.critic_target.state_dict()
            critic_target_dict = {k: v for k, v in saved_critic_target_dict.items() if k in current_critic_target_dict and v.shape == current_critic_target_dict[k].shape}
            current_critic_target_dict.update(critic_target_dict)
            self.critic_target.load_state_dict(current_critic_target_dict)
        except:
            # critic_target íŒŒì¼ì´ ì—†ìœ¼ë©´ criticì„ ë³µì‚¬
            LOGGER.warning("âš ï¸ critic_target íŒŒì¼ì´ ì—†ì–´ criticì—ì„œ ë³µì‚¬í•©ë‹ˆë‹¤.")
            for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
                target_param.data.copy_(param.data)
        
        # ì˜µí‹°ë§ˆì´ì €ì™€ ê¸°íƒ€ ìƒíƒœ ë¡œë“œ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            self.actor_optimizer.load_state_dict(torch.load(model_path / "actor_optimizer.pth", map_location=self.device))
            self.critic_optimizer.load_state_dict(torch.load(model_path / "critic_optimizer.pth", map_location=self.device))
            
            # ì˜µí‹°ë§ˆì´ì € ìƒíƒœë„ GPUë¡œ ì´ë™
            if self.device.type == 'cuda':
                for state in self.actor_optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(self.device)
                            
                for state in self.critic_optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(self.device)
            
            if self.use_automatic_entropy_tuning:
                self.log_alpha = torch.load(model_path / "log_alpha.pth", map_location=self.device)
                self.log_alpha.requires_grad = True
                self.alpha = self.log_alpha.exp()
                self.alpha_optimizer.load_state_dict(torch.load(model_path / "alpha_optimizer.pth", map_location=self.device))
                
                # alpha_optimizer ìƒíƒœë„ GPUë¡œ ì´ë™
                if self.device.type == 'cuda':
                    for state in self.alpha_optimizer.state.values():
                        for k, v in state.items():
                            if isinstance(v, torch.Tensor):
                                state[k] = v.to(self.device)
        except:
            LOGGER.warning(f"âš ï¸ ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ê°’ ìœ ì§€.")
        
        # í•™ìŠµ í†µê³„ ë¡œë“œ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            training_stats = torch.load(model_path / "training_stats.pth", map_location=self.device)
            self.actor_losses = training_stats.get('actor_losses', [])
            self.critic_losses = training_stats.get('critic_losses', [])
            self.alpha_losses = training_stats.get('alpha_losses', [])
            self.entropy_values = training_stats.get('entropy_values', [])
            self.train_step_counter = training_stats.get('train_step_counter', 0)
        except:
            LOGGER.warning(f"âš ï¸ í•™ìŠµ í†µê³„ ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ê°’ ìœ ì§€.")
        
        model_type = "LSTM" if self.use_lstm else ("CNN" if self.use_cnn else "MLP")
        LOGGER.info(f"âœ… {model_type} ëª¨ë¸ í¬ê¸° ì¡°ì • ë¡œë“œ ì™„ë£Œ: {model_path}")
        LOGGER.info("ğŸ’¡ ì¼ë¶€ íŒŒë¼ë¯¸í„°ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")


def train_sac_agent(env, agent, num_episodes: int = 1000, 
                   max_steps_per_episode=MAX_STEPS_PER_EPISODE, update_frequency: int = 1,
                   log_frequency: int = 100):
    """
    SAC ì—ì´ì „íŠ¸ í•™ìŠµ í•¨ìˆ˜

    Args:
        env: TradingEnvironment ì¸ìŠ¤í„´ìŠ¤
        agent: SAC ì—ì´ì „íŠ¸
        num_episodes: í•™ìŠµí•  ì—í”¼ì†Œë“œ ìˆ˜
        max_steps_per_episode: ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜
        update_frequency: ì—…ë°ì´íŠ¸ ë¹ˆë„
        log_frequency: ë¡œê·¸ ì¶œë ¥ ë¹ˆë„
    
    Returns:
        í•™ìŠµëœ SAC ì—ì´ì „íŠ¸
    """
    model_type = "LSTM" if agent.use_lstm else ("CNN" if agent.use_cnn else "MLP")
    LOGGER.info(f"ğŸš€ {model_type} SAC ì—ì´ì „íŠ¸ í•™ìŠµ ì‹œì‘: {num_episodes} ì—í”¼ì†Œë“œ")
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        episode_length = 0
        
        for step in range(max_steps_per_episode):
            # í–‰ë™ ì„ íƒ
            action = agent.select_action(state, evaluate=False)
            
            # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            next_state, reward, done, info = env.step(action)
            
            # ê²½í—˜ ì €ì¥
            agent.add_experience(state, action, reward, next_state, done)
            
            # ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            if step % update_frequency == 0:
                stats = agent.update_parameters()
            
            episode_reward += reward
            episode_length += 1
            state = next_state
            
            if done:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # ë¡œê·¸ ì¶œë ¥
        if episode % log_frequency == 0:
            avg_reward = np.mean(episode_rewards[-log_frequency:])
            avg_length = np.mean(episode_lengths[-log_frequency:])
            
            LOGGER.info(f"Episode {episode}")
            LOGGER.info(f"  í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
            LOGGER.info(f"  í‰ê·  ê¸¸ì´: {avg_length:.1f}")
            LOGGER.info(f"  í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: ${info['portfolio_value']:.2f}")
            LOGGER.info(f"  ì´ ìˆ˜ìµë¥ : {info['total_return'] * 100:.2f}%")
            
            if len(agent.actor_losses) > 0:
                LOGGER.info(f"  Actor Loss: {agent.actor_losses[-1]:.6f}")
                LOGGER.info(f"  Critic Loss: {agent.critic_losses[-1]:.6f}")
                LOGGER.info(f"  Alpha: {agent.alpha.item():.6f}")
    
    LOGGER.info(f"ğŸ‰ {model_type} SAC ì—ì´ì „íŠ¸ í•™ìŠµ ì™„ë£Œ!")
    return agent