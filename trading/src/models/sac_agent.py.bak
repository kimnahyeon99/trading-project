"""
SAC (Soft Actor-Critic) ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ëª¨ë“ˆ (ê¸°ë³¸ ë²„ì „)
"""
import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
import torch

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from typing import Dict, List, Tuple, Union, Optional, Any
from collections import deque
import random
import pickle
import time
from pathlib import Path
import sys
import os


from src.config.ea_teb_config import (
    DEVICE,
    HIDDEN_DIM,
    GAMMA,
    TAU,
    LEARNING_RATE_ACTOR,
    LEARNING_RATE_CRITIC,
    LEARNING_RATE_ALPHA,
    ALPHA_INIT,
    REPLAY_BUFFER_SIZE,
    TARGET_UPDATE_INTERVAL,
    BATCH_SIZE,
    MODELS_DIR,
    LOGGER,
    MAX_STEPS_PER_EPISODE,
    WINDOW_SIZE,
    window_size
)

from src.models.networks import (
    ActorNetwork, 
    CriticNetwork, 
    CNNActorNetwork, 
    CNNCriticNetwork,
    LSTMActorNetwork,
    LSTMCriticNetwork
)

from src.utils.utils import soft_update, create_directory

    
class ReplayBuffer:
    """
    ê²½í—˜ ë¦¬í”Œë ˆì´ ë²„í¼
    RL ì—ì´ì „íŠ¸ê°€ ê²½í—˜í•œ ìƒ˜í”Œì„ ì €ì¥í•˜ê³  ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§
    """
    
    def __init__(self, capacity: int = REPLAY_BUFFER_SIZE):
        """
        ReplayBuffer í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            capacity: ë²„í¼ì˜ ìµœëŒ€ ìš©ëŸ‰
        """
        self.capacity = capacity
        self.buffer = []
        self.position = 0
    
    def push(self, state: Any, action: Any, reward: float, next_state: Any, done: bool) -> None:
        """
        ë²„í¼ì— ìƒ˜í”Œ ì¶”ê°€
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
            action: ìˆ˜í–‰í•œ í–‰ë™
            reward: ë°›ì€ ë³´ìƒ
            next_state: ë‹¤ìŒ ìƒíƒœ
            done: ì¢…ë£Œ ì—¬ë¶€
        """
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int) -> Tuple:
        """
        ë²„í¼ì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œ ì¶”ì¶œ
        
        Args:
            batch_size: ì¶”ì¶œí•  ìƒ˜í”Œ ìˆ˜
            
        Returns:
            (ìƒíƒœ, í–‰ë™, ë³´ìƒ, ë‹¤ìŒ ìƒíƒœ, ì¢…ë£Œ ì—¬ë¶€) íŠœí”Œ
        """
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(list, zip(*batch))
        
        return state, action, reward, next_state, done
    
    def __len__(self) -> int:
        """
        ë²„í¼ì˜ í˜„ì¬ í¬ê¸° ë°˜í™˜
        
        Returns:
            ë²„í¼ì˜ í˜„ì¬ í¬ê¸°
        """
        return len(self.buffer)
    
    def save(self, path: Union[str, Path]) -> None:
        """
        ë²„í¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        with open(path, 'wb') as f:
            pickle.dump(self.buffer, f)
    
    def load(self, path: Union[str, Path]) -> None:
        """
        íŒŒì¼ì—ì„œ ë²„í¼ ë¡œë“œ
        
        Args:
            path: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
        """
        with open(path, 'rb') as f:
            self.buffer = pickle.load(f)
        self.position = len(self.buffer) % self.capacity


class SequentialReplayBuffer(ReplayBuffer):
    """ì‹œê³„ì—´ì„±ì„ ìœ ì§€í•˜ëŠ” ë¦¬í”Œë ˆì´ ë²„í¼"""
    
    def __init__(self, capacity: int = REPLAY_BUFFER_SIZE, sequence_length: int = 32):
        super().__init__(capacity)
        self.sequence_length = sequence_length
        LOGGER.info(f"ğŸ”„ ìˆœì°¨ì  ë¦¬í”Œë ˆì´ ë²„í¼ ì´ˆê¸°í™”: ì‹œí€€ìŠ¤ ê¸¸ì´ {sequence_length}")
    
    def sample_sequential(self, batch_size: int) -> Tuple:
        """ì—°ì†ëœ ì‹œí€€ìŠ¤ë“¤ì„ ë°°ì¹˜ë¡œ ìƒ˜í”Œë§"""
        if len(self.buffer) < self.sequence_length:
            LOGGER.warning("ë²„í¼ í¬ê¸° ë¶€ì¡±, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ìƒ˜í”Œë§")
            return self.sample(batch_size)
        
        # ì—°ì†ëœ ì‹œí€€ìŠ¤ ì‹œì‘ì ë“¤ì„ ëœë¤ ì„ íƒ
        max_start_idx = len(self.buffer) - self.sequence_length
        start_indices = np.random.choice(max_start_idx, batch_size, replace=True)
        
        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ì¤‘ìš”!)
        start_indices = np.sort(start_indices)
        
        sequences = []
        for start_idx in start_indices:
            # ì—°ì†ëœ sequence_lengthë§Œí¼ì˜ ê²½í—˜ë“¤ ì¶”ì¶œ
            sequence = []
            for i in range(self.sequence_length):
                if start_idx + i < len(self.buffer):
                    sequence.append(self.buffer[start_idx + i])
            
            if len(sequence) == self.sequence_length:
                sequences.append(sequence)
        
        if not sequences:
            return self.sample(batch_size)
        
        return self._process_sequences(sequences)
    
    def _process_sequences(self, sequences):
        """ì‹œí€€ìŠ¤ë“¤ì„ ë°°ì¹˜ë¡œ ë³€í™˜"""
        # ê° ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ transition ì‚¬ìš©
        batch = [seq[-1] for seq in sequences]
        state, action, reward, next_state, done = map(list, zip(*batch))
        return state, action, reward, next_state, done


class SACAgent:
    """
    SAC ì•Œê³ ë¦¬ì¦˜ ì—ì´ì „íŠ¸ (ê°œì„ ëœ CNN ì§€ì›)
    """
        
    def __init__(
        self,
        state_dim: int = None,
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        actor_lr: float = LEARNING_RATE_ACTOR,
        critic_lr: float = LEARNING_RATE_CRITIC,
        alpha_lr: float = LEARNING_RATE_ALPHA,
        gamma: float = GAMMA,
        tau: float = TAU,
        alpha_init: float = ALPHA_INIT,
        target_update_interval: int = TARGET_UPDATE_INTERVAL,
        use_automatic_entropy_tuning: bool = True,
        device: torch.device = DEVICE,
        buffer_capacity: int = REPLAY_BUFFER_SIZE,
        input_shape: Tuple[int, int] = None,
        use_cnn: bool = False,
        use_lstm: bool = False,
		model_type: str = None,  # ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
        lstm_hidden_dim: int = 128,
        num_lstm_layers: int = 2,
        lstm_dropout: float = 0.2,
        buffer_type: str = 'sequential',
        sequence_length: int = 32,
        # CNN ì „ìš© íŒŒë¼ë¯¸í„°
        cnn_dropout_rate: float = 0.1,
        cnn_learning_rate_factor: float = 0.5,  # CNNì€ ë” ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš©
        cnn_alpha_init: float = 0.1,  # CNNì€ ë” ë‚®ì€ ì´ˆê¸° alpha ì‚¬ìš©
        cnn_batch_norm: bool = True,
        # âœ… ìƒˆë¡œìš´ ì•ˆì •ì„± íŒŒë¼ë¯¸í„°ë“¤ ì¶”ê°€
        gradient_clip_norm: float = 1.0,
        use_gradient_clipping: bool = True,
        use_lr_scheduling: bool = True,
        lr_scheduler_factor: float = 0.8,
        lr_scheduler_patience: int = 100,
        target_update_method: str = 'soft',  # 'soft' or 'hard'
        adaptive_alpha: bool = True
    ):
        """SACAgent í´ë˜ìŠ¤ ì´ˆê¸°í™” (ê°œì„ ëœ CNN ì§€ì›)"""
        
        # ê¸°ë³¸ ì†ì„± ì„¤ì •
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.gamma = gamma
        self.tau = tau
        self.target_update_interval = target_update_interval
        self.use_automatic_entropy_tuning = use_automatic_entropy_tuning
        self.device = device
        self.use_cnn = use_cnn
        self.use_lstm = use_lstm
        self.input_shape = input_shape
        self.lstm_hidden_dim = lstm_hidden_dim
        self.num_lstm_layers = num_lstm_layers
        self.lstm_dropout = lstm_dropout
        
		# model_type ì²˜ë¦¬ (auto-detect if not provided)
        if model_type is None:
            if use_lstm:
                model_type = 'lstm'
            elif use_cnn:
                model_type = 'cnn'
            else:
                model_type = 'mlp'
        self.model_type = model_type
        # CNN ì „ìš© ì„¤ì •
        self.cnn_dropout_rate = cnn_dropout_rate
        self.cnn_batch_norm = cnn_batch_norm
        
        # âœ… ì•ˆì •ì„± ê°œì„  ì„¤ì •
        self.gradient_clip_norm = gradient_clip_norm
        self.use_gradient_clipping = use_gradient_clipping
        self.use_lr_scheduling = use_lr_scheduling
        self.target_update_method = target_update_method
        self.adaptive_alpha = adaptive_alpha
        
        # âœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì§€í‘œ ì¶”ì 
        self.recent_critic_losses = deque(maxlen=50)
        self.lr_plateau_counter = 0
        self.lr_scheduler_patience = lr_scheduler_patience
        self.lr_scheduler_factor = lr_scheduler_factor
        
        # âœ… ì ì‘ì  Alpha ì¡°ì •ì„ ìœ„í•œ ì¶”ì 
        if self.adaptive_alpha:
            self.recent_entropies = deque(maxlen=100)
            self.target_entropy_range = (-action_dim * 1.5, -action_dim * 0.5)  # ë™ì  ë²”ìœ„
        
        # ğŸ†• ëª¨ë¸ë³„ ì—”íŠ¸ë¡œí”¼ ì„¤ì • ì ìš©
        if model_type:
            from src.config.ea_teb_config import get_entropy_config_for_model
            entropy_config = get_entropy_config_for_model(model_type, action_dim)
            
            if entropy_config['use_adaptive']:
                self.use_automatic_entropy_tuning = True
                self.target_entropy_range = entropy_config['entropy_range']
                self.target_entropy = entropy_config['initial_target']
            else:
                # MLPì˜ ê²½ìš° ê³ ì •ê°’ ì‚¬ìš©
                self.target_entropy = entropy_config['fixed_target']
                
            LOGGER.info(f"ğŸ“Š {model_type.upper()} ì—”íŠ¸ë¡œí”¼ ì„¤ì •: {entropy_config['description']}")
        else:
            # ê¸°ì¡´ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜)
            self.target_entropy = -action_dim
        
        # ğŸ“š ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
        if use_cnn:
            self.actor_lr = actor_lr * cnn_learning_rate_factor
            self.critic_lr = critic_lr * cnn_learning_rate_factor  
            self.alpha_lr = alpha_lr * cnn_learning_rate_factor
            self.alpha_init = max(cnn_alpha_init, 0.2)  # ìµœì†Œ 0.2ë¡œ ì„¤ì •
            
            LOGGER.info("ğŸ–¼ï¸ CNN ì „ìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©:")
            LOGGER.info(f"   â””â”€ Actor LR: {self.actor_lr:.6f} (ì›ë³¸ì˜ {cnn_learning_rate_factor:.1f}ë°°)")
            LOGGER.info(f"   â””â”€ Critic LR: {self.critic_lr:.6f}")
            LOGGER.info(f"   â””â”€ Alpha LR: {self.alpha_lr:.6f}")
            LOGGER.info(f"   â””â”€ ì´ˆê¸° Alpha: {self.alpha_init}")
            LOGGER.info(f"   â””â”€ Dropout: {self.cnn_dropout_rate}")
        else:
            self.actor_lr = actor_lr
            self.critic_lr = critic_lr
            self.alpha_lr = alpha_lr
            self.alpha_init = alpha_init

        # í•™ìŠµ ë‹¨ê³„ ì¹´ìš´í„°
        self.train_step_counter = 0
        self.update_counter = 0

        # ëª¨ë¸ íƒ€ì… ê²€ì¦
        if use_cnn and use_lstm:
            raise ValueError("CNNê³¼ LSTM ëª¨ë¸ì„ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ëª¨ë¸ íƒ€ì… ë¡œê¹…
        if use_lstm:
            LOGGER.info("LSTM ê¸°ë°˜ SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        elif use_cnn:
            LOGGER.info("ê°œì„ ëœ CNN ê¸°ë°˜ SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        else:
            LOGGER.info("MLP ê¸°ë°˜ SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")

        # TradingEnvironmentë¥¼ ìœ„í•œ ìƒíƒœ ì°¨ì› ìë™ ê³„ì‚°
        if not use_cnn and not use_lstm and state_dim is None:
            if input_shape is None:
                raise ValueError("input_shapeë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")
            self.state_dim = input_shape[0] * input_shape[1] + 2
            state_dim = self.state_dim
            LOGGER.info(f"ğŸ“ ìƒíƒœ ì°¨ì› ìë™ ê³„ì‚°: {self.state_dim}")

        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self._initialize_networks()

        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(param.data)

        # ì˜µí‹°ë§ˆì´ì € (ëª¨ë¸ë³„ ë‹¤ë¥¸ í•™ìŠµë¥  ì ìš©)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.critic_lr)

        # âœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€
        if self.use_lr_scheduling:
            self.actor_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.actor_optimizer, mode='min', factor=self.lr_scheduler_factor, 
                patience=self.lr_scheduler_patience, min_lr=1e-6
            )
            self.critic_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.critic_optimizer, mode='min', factor=self.lr_scheduler_factor,
                patience=self.lr_scheduler_patience, min_lr=1e-6
            )
            LOGGER.info("âœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ í™œì„±í™”ë¨")

        # ğŸ“š CNNì¼ ë•Œ ì¶”ê°€ ì •ê·œí™” ê¸°ë²•
        if use_cnn:
            # L2 ì •ê·œí™” ì¶”ê°€
            for param_group in self.actor_optimizer.param_groups:
                param_group['weight_decay'] = 1e-4
            for param_group in self.critic_optimizer.param_groups:
                param_group['weight_decay'] = 1e-4
            
            LOGGER.info("âœ… CNN ëª¨ë¸ì— L2 ì •ê·œí™” (weight_decay=1e-4) ì ìš©")

        # ìë™ ì—”íŠ¸ë¡œí”¼ ì¡°ì • (ğŸš¨ ì¤‘ë³µ ì œê±°ëœ ë²„ì „)
        if self.use_automatic_entropy_tuning:
            # target_entropyëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì„¤ì •ë¨ (ì¤‘ë³µ ì œê±°)
            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
            self.alpha = self.log_alpha.exp()
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.alpha_lr)
        else:
            self.alpha = torch.tensor(self.alpha_init, device=device)
        
        # CUDA ìµœì í™”
        self._setup_cuda_streams()
        self._optimize_gpu_settings()
        self._move_all_optimizers_to_device()
        
        # ë¦¬í”Œë ˆì´ ë²„í¼
        if buffer_type == 'sequential':
            self.replay_buffer = SequentialReplayBuffer(
                capacity=buffer_capacity,
                sequence_length=sequence_length
            )
            self.use_sequential_sampling = True
        else:
            self.replay_buffer = ReplayBuffer(capacity=buffer_capacity)
            self.use_sequential_sampling = False
        
        # í•™ìŠµ í†µê³„
        self.actor_losses = []
        self.critic_losses = []
        self.alpha_losses = []
        self.entropy_values = []
        
        # ìµœì¢… ë¡œê·¸
        model_type = "LSTM" if use_lstm else ("CNN" if use_cnn else "MLP")
        LOGGER.info(f"ğŸ‰ SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        LOGGER.info(f"   â””â”€ ëª¨ë¸ íƒ€ì…: {model_type}")
        LOGGER.info(f"   â””â”€ í–‰ë™ ì°¨ì›: {action_dim}")
        LOGGER.info(f"   â””â”€ ìƒíƒœ ì°¨ì›: {self.state_dim if not (use_cnn or use_lstm) else input_shape}")
        LOGGER.info(f"   â””â”€ ì¥ì¹˜: {device}")
        
    def _move_optimizer_to_device(self, optimizer, device):
        """
        ì˜µí‹°ë§ˆì´ì € ìƒíƒœë¥¼ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        
        Args:
            optimizer: PyTorch ì˜µí‹°ë§ˆì´ì €
            device: ì´ë™í•  ë””ë°”ì´ìŠ¤
        """
        if device.type == 'cuda':
            for state in optimizer.state.values():
                for k, v in state.items():
                    if isinstance(v, torch.Tensor):
                        state[k] = v.to(device)

    def _move_all_optimizers_to_device(self):
        """ëª¨ë“  ì˜µí‹°ë§ˆì´ì €ë¥¼ í˜„ì¬ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        self._move_optimizer_to_device(self.actor_optimizer, self.device)
        self._move_optimizer_to_device(self.critic_optimizer, self.device)
        
        if hasattr(self, 'alpha_optimizer'):
            self._move_optimizer_to_device(self.alpha_optimizer, self.device)

    def clear_gpu_cache(self):
        """GPU ìºì‹œ ì •ë¦¬"""
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
            if hasattr(torch.cuda, 'synchronize'):
                torch.cuda.synchronize()

    def get_gpu_memory_info(self):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
        if self.device.type == 'cuda':
            allocated = torch.cuda.memory_allocated(self.device) / 1024**3  # GB
            cached = torch.cuda.memory_reserved(self.device) / 1024**3     # GB
            return {
                'allocated_gb': allocated,
                'cached_gb': cached,
                'total_memory_gb': torch.cuda.get_device_properties(self.device).total_memory / 1024**3
            }
        return {'allocated_gb': 0, 'cached_gb': 0, 'total_memory_gb': 0}

    def _setup_cuda_streams(self):
        """CUDA ìŠ¤íŠ¸ë¦¼ ì„¤ì •"""
        if self.device.type == 'cuda':
            self.data_stream = torch.cuda.Stream()
            self.compute_stream = torch.cuda.Stream()
            LOGGER.info(f"âœ… CUDA ìŠ¤íŠ¸ë¦¼ ìƒì„± ì™„ë£Œ: {self.device}")
        else:
            self.data_stream = None
            self.compute_stream = None
            
    def _optimize_gpu_settings(self):
        """GPU ìµœì í™” ì„¤ì •"""
        if self.device.type == 'cuda':
            # ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # í˜¼í•© ì •ë°€ë„ ì‚¬ìš© ì¤€ë¹„ (ì˜µì…˜)
            if hasattr(torch.cuda, 'amp'):
                self.use_amp = True
                self.scaler = torch.cuda.amp.GradScaler()
            else:
                self.use_amp = False
                
            LOGGER.info(f"âœ… GPU ìµœì í™” ì„¤ì • ì™„ë£Œ: {self.device}")
    
    def _initialize_networks(self):
        """ê°œì„ ëœ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”"""
        if self.use_lstm:
            # LSTM ë„¤íŠ¸ì›Œí¬
            if self.input_shape is None:
                raise ValueError("LSTM ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” input_shapeê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            self.actor = LSTMActorNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                lstm_hidden_dim=self.lstm_hidden_dim,
                num_lstm_layers=self.num_lstm_layers,
                dropout=self.lstm_dropout,
                device=self.device
            )
            
            self.critic = LSTMCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                lstm_hidden_dim=self.lstm_hidden_dim,
                num_lstm_layers=self.num_lstm_layers,
                dropout=self.lstm_dropout,
                device=self.device
            )
            
            self.critic_target = LSTMCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                lstm_hidden_dim=self.lstm_hidden_dim,
                num_lstm_layers=self.num_lstm_layers,
                dropout=self.lstm_dropout,
                device=self.device
            )
            
        elif self.use_cnn:
            # ğŸ–¼ï¸ ê°œì„ ëœ CNN ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
            if self.input_shape is None:
                raise ValueError("CNN ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” input_shapeê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            LOGGER.info("ğŸ—ï¸ ê°œì„ ëœ CNN ë„¤íŠ¸ì›Œí¬ ìƒì„± ì¤‘...")
            
            self.actor = CNNActorNetwork(
            input_shape=self.input_shape,
            action_dim=self.action_dim,
            hidden_dim=self.hidden_dim,
            dropout_rate=self.cnn_dropout_rate,
            device=self.device
            )
            
            self.critic = CNNCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                dropout_rate=self.cnn_dropout_rate,
                device=self.device
            )
            
            self.critic_target = CNNCriticNetwork(
                input_shape=self.input_shape,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                dropout_rate=self.cnn_dropout_rate,
                device=self.device
            )
            
            LOGGER.info("âœ… ê°œì„ ëœ CNN ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
                
        else:
            # MLP ë„¤íŠ¸ì›Œí¬
            if self.state_dim is None:
                raise ValueError("ì¼ë°˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” state_dimì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            self.actor = ActorNetwork(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                device=self.device
            )
            
            self.critic = CriticNetwork(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                device=self.device
            )
            
            self.critic_target = CriticNetwork(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                device=self.device
            )

    def select_action(self, state: Union[Dict[str, np.ndarray], Dict[str, torch.Tensor], np.ndarray], evaluate: bool = False) -> float:
        """
        ìƒíƒœì— ë”°ë¥¸ í–‰ë™ ì„ íƒ (ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© í˜¸í™˜)
        """
        try:
            # ìƒíƒœë¥¼ ë„¤íŠ¸ì›Œí¬ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
            if isinstance(state, dict):
                state_tensor = self._process_state_for_network(state)
            elif isinstance(state, np.ndarray):
                if self.use_cnn or self.use_lstm:
                    if self.input_shape and len(state) == (self.input_shape[0] * self.input_shape[1] + 2):
                        market_data_flat = state[:-2]
                        portfolio_state = state[-2:]
                        market_data = market_data_flat.reshape(self.input_shape)
                        state = {
                            'market_data': market_data,
                            'portfolio_state': portfolio_state
                        }
                        state_tensor = self._process_state_for_network(state)
                    else:
                        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                else:
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            elif isinstance(state, torch.Tensor):
                state_tensor = state.to(self.device)
                if state_tensor.dim() == 1:
                    state_tensor = state_tensor.unsqueeze(0)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìƒíƒœ íƒ€ì…: {type(state)}")
            
            # í–‰ë™ ì„ íƒ - BatchNorm ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ eval ëª¨ë“œ ì„¤ì •
            self.actor.eval()
            with torch.no_grad():
                if evaluate:
                    _, _, action = self.actor.sample(state_tensor)
                else:
                    action, _, _ = self.actor.sample(state_tensor)
            self.actor.train()
            
            action_value = float(action.detach().cpu().numpy().flatten()[0])
            
            # í•™ìŠµ ì´ˆê¸°ì— ì¶”ê°€ íƒìƒ‰ ë…¸ì´ì¦ˆ ì£¼ì…
            if not evaluate and self.train_step_counter < 5000:  # ì¶”ê°€
                noise_std = max(0.1, 0.5 * (1 - self.train_step_counter / 5000))
                noise = np.random.normal(0, noise_std)
                action_value = np.clip(action_value + noise, -1.0, 1.0)
                
            return action_value
            
        except Exception as e:
            LOGGER.error(f"í–‰ë™ ì„ íƒ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0.0

    def _process_state_for_network(self, state: Dict[str, Union[np.ndarray, torch.Tensor]]) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        ìƒíƒœë¥¼ ë„¤íŠ¸ì›Œí¬ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
        
        Args:
            state: TradingEnvironment ìƒíƒœ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ë„¤íŠ¸ì›Œí¬ ì…ë ¥ìš© í…ì„œ ë˜ëŠ” í…ì„œ ë”•ì…”ë„ˆë¦¬
        """
        try:
            if self.use_cnn or self.use_lstm:
                # CNN / LSTM: ìƒíƒœë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ìœ ì§€
                market_data = state["market_data"]
                portfolio_state = state["portfolio_state"]
                
                # numpy ë°°ì—´ì„ í…ì„œë¡œ ë³€í™˜
                if isinstance(market_data, np.ndarray):
                    market_tensor = torch.FloatTensor(market_data).to(self.device)
                else:
                    market_tensor = market_data.to(self.device)
                    
                if isinstance(portfolio_state, np.ndarray):
                    portfolio_tensor = torch.FloatTensor(portfolio_state).to(self.device)
                else:
                    portfolio_tensor = portfolio_state.to(self.device)
                
                # ë°°ì¹˜ ì°¨ì› í™•ì¸ ë° ì¶”ê°€
                if market_tensor.dim() == 2:  # (window_size, feature_dim)
                    market_tensor = market_tensor.unsqueeze(0)  # (1, window_size, feature_dim)
                if portfolio_tensor.dim() == 1:  # (2,)
                    portfolio_tensor = portfolio_tensor.unsqueeze(0)  # (1, 2)
                
                return {
                    "market_data": market_tensor,
                    "portfolio_state": portfolio_tensor
                }
            else:
                # MLP: ìƒíƒœë¥¼ flatten
                market_data = state['market_data']
                portfolio_state = state['portfolio_state']
                
                # numpy ë°°ì—´ì„ í…ì„œë¡œ ë³€í™˜
                if isinstance(market_data, np.ndarray):
                    market_flat = market_data.flatten()
                else:
                    market_flat = market_data.flatten().cpu().numpy()
                    
                if isinstance(portfolio_state, np.ndarray):
                    portfolio_flat = portfolio_state
                else:
                    portfolio_flat = portfolio_state.cpu().numpy()
                
                # ê²°í•©
                combined_state = np.concatenate([market_flat, portfolio_flat])
                
                return torch.FloatTensor(combined_state).unsqueeze(0).pin_memory().to(self.device, non_blocking=True)
            
        except Exception as e:
            LOGGER.error(f"ìƒíƒœ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ìƒíƒœ ë°˜í™˜
            if self.use_cnn or self.use_lstm:
                return {
                    "market_data": torch.zeros((1, self.input_shape[0], self.input_shape[1]), device=self.device),
                    "portfolio_state": torch.zeros((1, 2), device=self.device)
                }
            else:
                return torch.zeros((1, self.state_dim), device=self.device)

    def _process_batch_states(self, states: List[Dict[str, np.ndarray]]) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        ë°°ì¹˜ ìƒíƒœë“¤ì„ ë„¤íŠ¸ì›Œí¬ ì…ë ¥ìœ¼ë¡œ ë³€í™˜
        """
        # ğŸ”„ GPU ë©”ëª¨ë¦¬ ì²´í¬ë¥¼ ë§¨ ì•ìœ¼ë¡œ ì´ë™
        if self.device.type == 'cuda':
            total_memory_gb = torch.cuda.get_device_properties(self.device).total_memory / 1024**3
            memory_threshold = total_memory_gb * 0.85
            memory_allocated = torch.cuda.memory_allocated() / 1024**3
            
            if memory_allocated > memory_threshold:
                LOGGER.warning(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_allocated:.1f}GB/{total_memory_gb:.1f}GB ({memory_allocated/total_memory_gb*100:.1f}%)")
        
        if self.use_cnn or self.use_lstm:
            market_batch = []
            portfolio_batch = []
            for state in states:
                market_batch.append(state["market_data"])
                portfolio_batch.append(state["portfolio_state"])
            
            market_tensor = torch.FloatTensor(np.stack(market_batch)).pin_memory().to(self.device, non_blocking=True)
            portfolio_tensor = torch.FloatTensor(np.stack(portfolio_batch)).pin_memory().to(self.device, non_blocking=True)

            return {
                "market_data": market_tensor,
                "portfolio_state": portfolio_tensor
            }
        else:
            batch_states = []
            for state in states:
                processed_state = self._process_state_for_network(state)
                batch_states.append(processed_state)
            return torch.cat(batch_states, dim=0).pin_memory().to(self.device, non_blocking=True)
        
    def add_experience(self, state: Dict[str, np.ndarray], action: float, reward: float, 
                      next_state: Dict[str, np.ndarray], done: bool) -> None:
        """
        ê²½í—˜ì„ ë¦¬í”Œë ˆì´ ë²„í¼ì— ì¶”ê°€
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
            action: ìˆ˜í–‰í•œ í–‰ë™
            reward: ë°›ì€ ë³´ìƒ
            next_state: ë‹¤ìŒ ìƒíƒœ
            done: ì¢…ë£Œ ì—¬ë¶€
        """
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update_parameters(self, batch_size: int = BATCH_SIZE) -> Dict[str, float]:
        """í†µí•©ëœ ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ì•ˆì •ì„± ê°œì„ )"""
        if len(self.replay_buffer) < batch_size:
            return {
                'actor_loss': 0.0,
                'critic_loss': 0.0,
                'alpha_loss': 0.0,
                'entropy': 0.0,
                'alpha': self.alpha.item() if isinstance(self.alpha, torch.Tensor) else self.alpha
            }
        
        # ìƒ˜í”Œë§
        try:
            if self.use_sequential_sampling and hasattr(self.replay_buffer, 'sample_sequential'):
                states, actions, rewards, next_states, dones = self.replay_buffer.sample_sequential(batch_size)
            else:
                states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
        except Exception as e:
            LOGGER.warning(f"ìƒ˜í”Œë§ ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
            states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
        
        # ë°°ì¹˜ í…ì„œ ë³€í™˜
        batched_states = self._process_batch_states(states)
        batched_next_states = self._process_batch_states(next_states)
        
        batched_actions = torch.FloatTensor(actions).unsqueeze(1).pin_memory().to(self.device, non_blocking=True)
        batched_rewards = torch.FloatTensor(rewards).unsqueeze(1).pin_memory().to(self.device, non_blocking=True)
        batched_dones = torch.FloatTensor(dones).unsqueeze(1).pin_memory().to(self.device, non_blocking=True)
        
        # âœ… í†µí•©ëœ ì—…ë°ì´íŠ¸ (ëª¨ë“  ëª¨ë¸ì— ë™ì¼í•œ ì•ˆì •ì„± ì ìš©)
        critic_loss = self._update_critic(
            batched_states, batched_actions, batched_rewards, batched_next_states, batched_dones
        )
        actor_loss = self._update_actor(batched_states)
        alpha_loss = self._update_alpha(batched_states)
        
        # âœ… ê°œì„ ëœ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.train_step_counter += 1
        if self.train_step_counter % self.target_update_interval == 0:
            if self.target_update_method == 'soft':
                soft_update(self.critic_target, self.critic, self.tau)
            else:  # hard update
                self.critic_target.load_state_dict(self.critic.state_dict())
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device.type == 'cuda' and self.update_counter % 100 == 0:
            torch.cuda.empty_cache()
        
        # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        entropy_value = 0.0
        if self.update_counter % 10 == 0:
            with torch.no_grad():
                _, log_probs, _ = self.actor.sample(batched_states)
                entropy_value = -log_probs.mean().item()

        # í•™ìŠµ í†µê³„
        stats = {
            'actor_loss': actor_loss,
            'critic_loss': critic_loss,
            'alpha_loss': alpha_loss if self.use_automatic_entropy_tuning else 0.0,
            'entropy': entropy_value,
            'alpha': self.alpha.item()
        }
        
        self.actor_losses.append(stats['actor_loss'])
        self.critic_losses.append(stats['critic_loss'])
        self.alpha_losses.append(stats['alpha_loss'])
        self.entropy_values.append(stats['entropy'])
        
        return stats
    
    def _update_critic(self, states, actions, rewards, next_states, dones):
        """í†µí•©ëœ Critic ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ì•ˆì •ì„± ê°œì„ )"""
        # í˜„ì¬ ì •ì±…ì—ì„œ ë‹¤ìŒ í–‰ë™ ìƒ˜í”Œë§
        with torch.no_grad():
            next_actions, next_log_probs, _ = self.actor.sample(next_states)
            
            # íƒ€ê²Ÿ Q ê°’ ê³„ì‚°
            next_q1_target, next_q2_target = self.critic_target(next_states, next_actions)
            next_q_target = torch.min(next_q1_target, next_q2_target)
            next_q_target = next_q_target - self.alpha * next_log_probs
            expected_q = rewards + (1.0 - dones) * self.gamma * next_q_target
        
        current_q1, current_q2 = self.critic(states, actions)
        
        q1_loss = F.mse_loss(current_q1, expected_q)
        q2_loss = F.mse_loss(current_q2, expected_q)
        
        critic_loss = q1_loss + q2_loss
        
        # âœ… ì•ˆì •ì„± ê°œì„ : ì†ì‹¤ ê°’ ì²´í¬
        if torch.isnan(critic_loss) or torch.isinf(critic_loss):
            LOGGER.warning("Critic ì†ì‹¤ì— NaN/Inf ê°ì§€ë¨. ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€.")
            return 0.0
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        
        # âœ… í†µí•©ëœ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ëª¨ë“  ëª¨ë¸ì— ì ìš©)
        if self.use_gradient_clipping:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.critic.parameters(), 
                max_norm=self.gradient_clip_norm
            )
            
            # ê·¹ë‹¨ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ê°ì§€
            if grad_norm > self.gradient_clip_norm * 5:
                LOGGER.warning(f"í° ê·¸ë˜ë””ì–¸íŠ¸ ê°ì§€: {grad_norm:.6f}")
        
        self.critic_optimizer.step()
        
        # âœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì—…ë°ì´íŠ¸
        critic_loss_value = critic_loss.item()
        if self.use_lr_scheduling:
            self.recent_critic_losses.append(critic_loss_value)
            if len(self.recent_critic_losses) >= 50:
                avg_loss = sum(self.recent_critic_losses) / len(self.recent_critic_losses)
                self.critic_scheduler.step(avg_loss)
        
        return critic_loss_value

    def _update_actor(self, states):
        """í†µí•©ëœ Actor ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ì•ˆì •ì„± ê°œì„ )"""
        new_actions, log_probs, _ = self.actor.sample(states)
        q1, q2 = self.critic(states, new_actions)
        q = torch.min(q1, q2)
        
        actor_loss = (self.alpha * log_probs - q).mean()
        
        # âœ… ì•ˆì •ì„± ê°œì„ : ì†ì‹¤ ê°’ ì²´í¬
        if torch.isnan(actor_loss) or torch.isinf(actor_loss):
            LOGGER.warning("Actor ì†ì‹¤ì— NaN/Inf ê°ì§€ë¨. ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€.")
            return 0.0
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        
        # âœ… í†µí•©ëœ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        if self.use_gradient_clipping:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.actor.parameters(), 
                max_norm=self.gradient_clip_norm
            )
            
            if grad_norm > self.gradient_clip_norm * 5:
                LOGGER.warning(f"Actor í° ê·¸ë˜ë””ì–¸íŠ¸ ê°ì§€: {grad_norm:.6f}")
        
        self.actor_optimizer.step()
        
        # âœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
        if self.use_lr_scheduling:
            self.actor_scheduler.step(actor_loss.item())
        
        return actor_loss.item()

    def _update_alpha(self, states):
        """ê°œì„ ëœ Alpha ì—…ë°ì´íŠ¸ (ì ì‘ì  ì¡°ì •)"""
        if not self.use_automatic_entropy_tuning:
            return 0.0
        
        with torch.no_grad():
            _, log_probs, _ = self.actor.sample(states)
            current_entropy = -log_probs.mean().item()
        
        # âœ… ì ì‘ì  target_entropy ì¡°ì •
        if self.adaptive_alpha:
            self.recent_entropies.append(current_entropy)
            
            if len(self.recent_entropies) >= 50:
                avg_entropy = sum(self.recent_entropies) / len(self.recent_entropies)
                
                # ì—”íŠ¸ë¡œí”¼ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ targetì„ ë‚®ì¶¤, ë„ˆë¬´ ë†’ìœ¼ë©´ ë†’ì„
                if avg_entropy < self.target_entropy_range[0]:
                    self.target_entropy = max(self.target_entropy * 1.02, self.target_entropy_range[0])
                elif avg_entropy > self.target_entropy_range[1]:
                    self.target_entropy = min(self.target_entropy * 0.98, self.target_entropy_range[1])
        
        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
        
        # âœ… ì•ˆì •ì„± ì²´í¬
        if torch.isnan(alpha_loss) or torch.isinf(alpha_loss):
            LOGGER.warning("Alpha ì†ì‹¤ì— NaN/Inf ê°ì§€ë¨. ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€.")
            return 0.0
        
        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        
        # âœ… Alphaë„ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        if self.use_gradient_clipping:
            torch.nn.utils.clip_grad_norm_([self.log_alpha], max_norm=self.gradient_clip_norm)
        
        self.alpha_optimizer.step()
        
        self.alpha = self.log_alpha.exp()
        
        # âœ… Alpha ê°’ ë²”ìœ„ ì œí•œ
        with torch.no_grad():
            self.log_alpha.clamp_(min=-10, max=2)  # alpha ë²”ìœ„ë¥¼ [exp(-10), exp(2)]ë¡œ ì œí•œ
            self.alpha = self.log_alpha.exp()
        
        return alpha_loss.item()
    
    def save_model(self, save_dir: Union[str, Path] = None, prefix: str = "",
               model_type: str = None, symbol: str = None, symbols: List[str] = None) -> str:
        """
        ëª¨ë¸ ì €ì¥ (ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© í˜¸í™˜ì„± ê°œì„ )

        Args:
            save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            prefix: íŒŒì¼ëª… ì ‘ë‘ì‚¬
            model_type: ëª¨ë¸ íƒ€ì… ('mlp', 'cnn', 'lstm')
            symbol: ë‹¨ì¼ ì‹¬ë³¼ (MLPìš©)
            symbols: ë‹¤ì¤‘ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            str: ì €ì¥ëœ ëª¨ë¸ì˜ ê²½ë¡œ
        """
        if save_dir is None:
            save_dir = MODELS_DIR

        create_directory(save_dir)

        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = time.strftime("%Y%m%d_%H%M%S")

         # ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€ (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
        if model_type is None:
            model_type = getattr(self, 'model_type', None)
            if model_type is None:
                if self.use_lstm:
                    model_type = 'lstm'
                elif self.use_cnn:
                    model_type = 'cnn'
                else:
                    model_type = 'mlp'

        # íŒŒì¼ëª… íŒ¨í„´ ìƒì„±
        if model_type.lower() == 'mlp':
            # MLP: ì‹¬ë³¼ë³„ë¡œ êµ¬ë¶„
            if symbol:
                model_name = f"final_sac_model_{symbol}_{timestamp}"
            elif symbols and len(symbols) == 1:
                model_name = f"final_sac_model_{symbols[0]}_{timestamp}"
            else:
                # ë‹¤ì¤‘ ì‹¬ë³¼ì´ê±°ë‚˜ ì‹¬ë³¼ ì •ë³´ ì—†ìŒ
                model_name = f"final_sac_model_multi_{timestamp}"
        else:
            # CNN, LSTM: ì‹¬ë³¼ êµ¬ë¶„ ì—†ìŒ
            model_name = f"final_{model_type.lower()}_sac_model_{timestamp}"

        # ì ‘ë‘ì‚¬ ì ìš©
        if prefix:
            model_name = f"{prefix}{model_name}"

        # ì €ì¥ ê²½ë¡œ ìƒì„±
        model_path = Path(save_dir) / model_name
        create_directory(model_path)

        # ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ì €ì¥
        torch.save(self.actor.state_dict(), model_path / "actor.pth")
        torch.save(self.critic.state_dict(), model_path / "critic.pth")
        torch.save(self.critic_target.state_dict(), model_path / "critic_target.pth")

        # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ì €ì¥
        torch.save(self.actor_optimizer.state_dict(), model_path / "actor_optimizer.pth")
        torch.save(self.critic_optimizer.state_dict(), model_path / "critic_optimizer.pth")

        # Alpha ê´€ë ¨ ìƒíƒœ ì €ì¥
        if self.use_automatic_entropy_tuning:
            torch.save(self.log_alpha, model_path / "log_alpha.pth")
            torch.save(self.alpha_optimizer.state_dict(), model_path / "alpha_optimizer.pth")

        # í•™ìŠµ í†µê³„ ì €ì¥
        training_stats = {
            'actor_losses': self.actor_losses,
            'critic_losses': self.critic_losses,
            'alpha_losses': self.alpha_losses,
            'entropy_values': self.entropy_values,
            'train_step_counter': self.train_step_counter
        }
        torch.save(training_stats, model_path / "training_stats.pth")

        # ì„¤ì • ì €ì¥ (ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© í˜¸í™˜ì„± ì •ë³´ í¬í•¨)
        config = {
            # ê¸°ë³¸ ëª¨ë¸ ì •ë³´
            'model_type': model_type.lower(),
            'symbol': symbol,
            'symbols': symbols,
            'state_dim': self.state_dim,
            'action_dim': self.action_dim,
            'hidden_dim': self.hidden_dim,
            
            # ëª¨ë¸ ì•„í‚¤í…ì²˜
            'use_cnn': self.use_cnn,
            'use_lstm': self.use_lstm,
            'input_shape': self.input_shape,
            
            # LSTM ì „ìš© íŒŒë¼ë¯¸í„°
            'lstm_hidden_dim': getattr(self, 'lstm_hidden_dim', None),
            'num_lstm_layers': getattr(self, 'num_lstm_layers', None),
            'lstm_dropout': getattr(self, 'lstm_dropout', None),
            
            # ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© í˜¸í™˜ì„± ì •ë³´
            'realtime_compatible': True,
            'supports_dict_state': self.use_cnn or self.use_lstm,
            'supports_flat_state': not (self.use_cnn or self.use_lstm),
            'state_format': 'dict' if (self.use_cnn or self.use_lstm) else 'flat',
            
            # ë©”íƒ€ ì •ë³´
            'timestamp': timestamp,
            'saved_at': time.strftime("%Y-%m-%d %H:%M:%S"),
            'device': str(self.device),
            'framework': 'pytorch',
            'sac_version': '2.0'
        }
        torch.save(config, model_path / "config.pth")

        model_type_display = model_type.upper()
        LOGGER.info(f"âœ… {model_type_display} ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        LOGGER.info(f"   â””â”€ ëª¨ë¸ íƒ€ì…: {model_type_display}")
        LOGGER.info(f"   â””â”€ ì‹¬ë³¼: {symbol or symbols or 'Multi'}")
        LOGGER.info(f"   â””â”€ ì‹¤ì‹œê°„ í˜¸í™˜: {'YES' if config['realtime_compatible'] else 'NO'}")
        LOGGER.info(f"   â””â”€ ìƒíƒœ í˜•ì‹: {config['state_format'].upper()}")
        LOGGER.info(f"   â””â”€ íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")

        return str(model_path)
        
    def load_model(self, model_path: Union[str, Path]) -> None:
        """
        ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_path: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        model_path = Path(model_path)
        
        if not model_path.exists():
            LOGGER.error(f"âŒ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
            return
        
        # ëª¨ë¸ íƒ€ì… ê°ì§€ ë° ë¡œê·¸
        model_name = model_path.name.lower()
        if "lstm" in model_name:
            LOGGER.info(f"ğŸ”„ LSTM SAC ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        elif "cnn" in model_name:
            LOGGER.info(f"ğŸ”„ CNN SAC ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        else:
            LOGGER.info(f"ğŸ”„ MLP SAC ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        
        try:
            # ë¨¼ì € ì›ë˜ ë°©ì‹ìœ¼ë¡œ ë¡œë“œ ì‹œë„
            self.actor.load_state_dict(torch.load(model_path / "actor.pth", map_location=self.device))
            self.critic.load_state_dict(torch.load(model_path / "critic.pth", map_location=self.device))
            self.critic_target.load_state_dict(torch.load(model_path / "critic_target.pth", map_location=self.device))
            
            # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ
            self.actor_optimizer.load_state_dict(torch.load(model_path / "actor_optimizer.pth", map_location=self.device))
            self.critic_optimizer.load_state_dict(torch.load(model_path / "critic_optimizer.pth", map_location=self.device))
            
            # ì˜µí‹°ë§ˆì´ì € ìƒíƒœë„ GPUë¡œ ì´ë™
            if self.device.type == 'cuda':
                for state in self.actor_optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(self.device)
                            
                for state in self.critic_optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(self.device)
            
            # Alpha ê´€ë ¨ ë¡œë“œ
            if self.use_automatic_entropy_tuning:
                self.log_alpha = torch.load(model_path / "log_alpha.pth", map_location=self.device)
                self.log_alpha.requires_grad = True
                self.alpha = self.log_alpha.exp()
                self.alpha_optimizer.load_state_dict(torch.load(model_path / "alpha_optimizer.pth", map_location=self.device))
                
                # alpha_optimizer ìƒíƒœë„ GPUë¡œ ì´ë™
                if self.device.type == 'cuda':
                    for state in self.alpha_optimizer.state.values():
                        for k, v in state.items():
                            if isinstance(v, torch.Tensor):
                                state[k] = v.to(self.device)
            else:
                self.alpha = torch.tensor(self.alpha_init, device=self.device)
            
            # í•™ìŠµ í†µê³„ ë¡œë“œ
            training_stats = torch.load(model_path / "training_stats.pth", map_location=self.device)
            self.actor_losses = training_stats.get('actor_losses', [])
            self.critic_losses = training_stats.get('critic_losses', [])
            self.alpha_losses = training_stats.get('alpha_losses', [])
            self.entropy_values = training_stats.get('entropy_values', [])
            self.train_step_counter = training_stats.get('train_step_counter', 0)
            
            # ì„±ê³µ ë¡œê·¸
            model_type = "LSTM" if self.use_lstm else ("CNN" if self.use_cnn else "MLP")
            LOGGER.info(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            LOGGER.info(f"   â””â”€ í•™ìŠµ ìŠ¤í…: {self.train_step_counter:,}")
            LOGGER.info(f"   â””â”€ ë²„í¼ í¬ê¸°: {len(self.replay_buffer):,}")
            
        except RuntimeError as e:
            # í¬ê¸° ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ í¬ê¸° ì¡°ì • ë¡œë“œ ë©”ì„œë“œ ì‚¬ìš©
            LOGGER.warning(f"âš ï¸ í‘œì¤€ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            LOGGER.info("ğŸ”„ í¬ê¸° ì¡°ì • ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„ ì¤‘...")
            self.load_model_with_resize(model_path)
    
    def get_latest_model_path(self, save_dir: Union[str, Path] = None, prefix: str = '') -> Optional[Path]:
        """
        ìµœì‹  ëª¨ë¸ ê²½ë¡œ ë°˜í™˜
        
        Args:
            save_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ (Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©)
            prefix: íŒŒì¼ëª… ì ‘ë‘ì‚¬
            
        Returns:
            ìµœì‹  ëª¨ë¸ ê²½ë¡œ (ì—†ìœ¼ë©´ None)
        """
        if save_dir is None:
            save_dir = MODELS_DIR
        
        save_dir = Path(save_dir)
        if not save_dir.exists():
            return None
        
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ íŒ¨í„´ í™•ì¸
        patterns = [f"{prefix}lstm_sac_model_", f"{prefix}cnn_sac_model_", f"{prefix}mlp_sac_model_", f"{prefix}sac_model_"]
        
        model_dirs = []
        for pattern in patterns:
            model_dirs.extend([d for d in save_dir.iterdir() if d.is_dir() and d.name.startswith(pattern)])
        
        if not model_dirs:
            return None
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        model_dirs.sort(key=lambda d: d.name, reverse=True)
        return model_dirs[0]
    
    def load_model_with_resize(self, model_path):
        """
        í¬ê¸°ê°€ ë‹¤ë¥¸ ëª¨ë¸ì„ ë¶€ë¶„ì ìœ¼ë¡œ ë¡œë“œ
        
        Args:
            model_path: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        model_path = Path(model_path)
        
        LOGGER.info("ğŸ”§ í¬ê¸° ì¡°ì • ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„ ì¤‘...")
        
        # ì €ì¥ëœ ìƒíƒœ ì‚¬ì „ ë¡œë“œ
        saved_actor_dict = torch.load(model_path / "actor.pth", map_location=self.device)
        saved_critic_dict = torch.load(model_path / "critic.pth", map_location=self.device)
        
        # í˜„ì¬ ëª¨ë¸ì˜ ìƒíƒœ ì‚¬ì „
        current_actor_dict = self.actor.state_dict()
        current_critic_dict = self.critic.state_dict()
        
        # í¬ê¸°ê°€ ì¼ì¹˜í•˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ ë¡œë“œ
        actor_dict = {k: v for k, v in saved_actor_dict.items() if k in current_actor_dict and v.shape == current_actor_dict[k].shape}
        critic_dict = {k: v for k, v in saved_critic_dict.items() if k in current_critic_dict and v.shape == current_critic_dict[k].shape}
        
        # í˜¸í™˜ í†µê³„ ë¡œê·¸
        actor_loaded = len(actor_dict)
        actor_total = len(current_actor_dict)
        critic_loaded = len(critic_dict)
        critic_total = len(current_critic_dict)
        
        LOGGER.info(f"ğŸ“Š ëª¨ë¸ í˜¸í™˜ì„± ë¶„ì„:")
        LOGGER.info(f"   â””â”€ Actor: {actor_loaded}/{actor_total} ë ˆì´ì–´ í˜¸í™˜ ({actor_loaded/actor_total*100:.1f}%)")
        LOGGER.info(f"   â””â”€ Critic: {critic_loaded}/{critic_total} ë ˆì´ì–´ í˜¸í™˜ ({critic_loaded/critic_total*100:.1f}%)")
        
        # ìƒíƒœ ì‚¬ì „ ì—…ë°ì´íŠ¸
        current_actor_dict.update(actor_dict)
        current_critic_dict.update(critic_dict)
        
        # ëª¨ë¸ì— ì ìš©
        self.actor.load_state_dict(current_actor_dict)
        self.critic.load_state_dict(current_critic_dict)
        
        # critic_target ì—…ë°ì´íŠ¸
        try:
            saved_critic_target_dict = torch.load(model_path / "critic_target.pth", map_location=self.device)
            current_critic_target_dict = self.critic_target.state_dict()
            critic_target_dict = {k: v for k, v in saved_critic_target_dict.items() if k in current_critic_target_dict and v.shape == current_critic_target_dict[k].shape}
            current_critic_target_dict.update(critic_target_dict)
            self.critic_target.load_state_dict(current_critic_target_dict)
        except:
            # critic_target íŒŒì¼ì´ ì—†ìœ¼ë©´ criticì„ ë³µì‚¬
            LOGGER.warning("âš ï¸ critic_target íŒŒì¼ì´ ì—†ì–´ criticì—ì„œ ë³µì‚¬í•©ë‹ˆë‹¤.")
            for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
                target_param.data.copy_(param.data)
        
        # ì˜µí‹°ë§ˆì´ì €ì™€ ê¸°íƒ€ ìƒíƒœ ë¡œë“œ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            self.actor_optimizer.load_state_dict(torch.load(model_path / "actor_optimizer.pth", map_location=self.device))
            self.critic_optimizer.load_state_dict(torch.load(model_path / "critic_optimizer.pth", map_location=self.device))
            
            # ì˜µí‹°ë§ˆì´ì € ìƒíƒœë„ GPUë¡œ ì´ë™
            if self.device.type == 'cuda':
                for state in self.actor_optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(self.device)
                            
                for state in self.critic_optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(self.device)
            
            if self.use_automatic_entropy_tuning:
                self.log_alpha = torch.load(model_path / "log_alpha.pth", map_location=self.device)
                self.log_alpha.requires_grad = True
                self.alpha = self.log_alpha.exp()
                self.alpha_optimizer.load_state_dict(torch.load(model_path / "alpha_optimizer.pth", map_location=self.device))
                
                # alpha_optimizer ìƒíƒœë„ GPUë¡œ ì´ë™
                if self.device.type == 'cuda':
                    for state in self.alpha_optimizer.state.values():
                        for k, v in state.items():
                            if isinstance(v, torch.Tensor):
                                state[k] = v.to(self.device)
        except:
            LOGGER.warning(f"âš ï¸ ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ê°’ ìœ ì§€.")
        
        # í•™ìŠµ í†µê³„ ë¡œë“œ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            training_stats = torch.load(model_path / "training_stats.pth", map_location=self.device)
            self.actor_losses = training_stats.get('actor_losses', [])
            self.critic_losses = training_stats.get('critic_losses', [])
            self.alpha_losses = training_stats.get('alpha_losses', [])
            self.entropy_values = training_stats.get('entropy_values', [])
            self.train_step_counter = training_stats.get('train_step_counter', 0)
        except:
            LOGGER.warning(f"âš ï¸ í•™ìŠµ í†µê³„ ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ê°’ ìœ ì§€.")
        
        model_type = "LSTM" if self.use_lstm else ("CNN" if self.use_cnn else "MLP")
        LOGGER.info(f"âœ… {model_type} ëª¨ë¸ í¬ê¸° ì¡°ì • ë¡œë“œ ì™„ë£Œ: {model_path}")
        LOGGER.info("ğŸ’¡ ì¼ë¶€ íŒŒë¼ë¯¸í„°ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")


def train_sac_agent(env, agent, num_episodes: int = 1000, 
                   max_steps_per_episode=MAX_STEPS_PER_EPISODE, update_frequency: int = 1,
                   log_frequency: int = 100):
    """
    SAC ì—ì´ì „íŠ¸ í•™ìŠµ í•¨ìˆ˜

    Args:
        env: TradingEnvironment ì¸ìŠ¤í„´ìŠ¤
        agent: SAC ì—ì´ì „íŠ¸
        num_episodes: í•™ìŠµí•  ì—í”¼ì†Œë“œ ìˆ˜
        max_steps_per_episode: ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜
        update_frequency: ì—…ë°ì´íŠ¸ ë¹ˆë„
        log_frequency: ë¡œê·¸ ì¶œë ¥ ë¹ˆë„
    
    Returns:
        í•™ìŠµëœ SAC ì—ì´ì „íŠ¸
    """
    model_type = "LSTM" if agent.use_lstm else ("CNN" if agent.use_cnn else "MLP")
    LOGGER.info(f"ğŸš€ {model_type} SAC ì—ì´ì „íŠ¸ í•™ìŠµ ì‹œì‘: {num_episodes} ì—í”¼ì†Œë“œ")
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        episode_length = 0
        
        for step in range(max_steps_per_episode):
            # í–‰ë™ ì„ íƒ
            action = agent.select_action(state, evaluate=False)
            
            # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            next_state, reward, done, info = env.step(action)
            
            # ê²½í—˜ ì €ì¥
            agent.add_experience(state, action, reward, next_state, done)
            
            # ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            if step % update_frequency == 0:
                stats = agent.update_parameters()
            
            episode_reward += reward
            episode_length += 1
            state = next_state
            
            if done:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # ë¡œê·¸ ì¶œë ¥
        if episode % log_frequency == 0:
            avg_reward = np.mean(episode_rewards[-log_frequency:])
            avg_length = np.mean(episode_lengths[-log_frequency:])
            
            LOGGER.info(f"Episode {episode}")
            LOGGER.info(f"  í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
            LOGGER.info(f"  í‰ê·  ê¸¸ì´: {avg_length:.1f}")
            LOGGER.info(f"  í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: ${info['portfolio_value']:.2f}")
            LOGGER.info(f"  ì´ ìˆ˜ìµë¥ : {info['total_return'] * 100:.2f}%")
            
            if len(agent.actor_losses) > 0:
                LOGGER.info(f"  Actor Loss: {agent.actor_losses[-1]:.6f}")
                LOGGER.info(f"  Critic Loss: {agent.critic_losses[-1]:.6f}")
                LOGGER.info(f"  Alpha: {agent.alpha.item():.6f}")
    
    LOGGER.info(f"ğŸ‰ {model_type} SAC ì—ì´ì „íŠ¸ í•™ìŠµ ì™„ë£Œ!")
    return agent

def train_agent_sequential(agent, train_env, args, timer):
    """ìˆœì°¨ì  ë°ì´í„° í™œìš© ì—ì´ì „íŠ¸ í•™ìŠµ (CNN íŠ¹í™” ê°œì„ )"""
    LOGGER.info("ìˆœì°¨ì  í•™ìŠµ ì‹œì‘...")
    global episode_actions_history
    
    episode_rewards = []
    portfolio_values = []
    shares_history = []
    data_coverage_log = []
    
    # ìŠ¤í…ë³„ loss ì¶”ì ê¸° ì´ˆê¸°í™”
    loss_tracker = StepLossTracker(window_size=args.step_log_interval)
    global_step_count = 0
    
    # ğŸ–¼ï¸ CNN ì „ìš© ì„¤ì • ì ìš©
    if args.use_cnn:
        from src.config.ea_teb_config import config
        cnn_config = config.get_optimized_cnn_config_for_symbol(
            args.symbols[0] if args.symbols else "AAPL"
        )
        
        # CNN ì „ìš© ê°„ê²© ì ìš©
        args.log_interval = cnn_config['log_interval']
        args.save_interval = cnn_config['save_interval']
        warmup_steps = cnn_config['warmup_steps']
        min_buffer_size = cnn_config['min_buffer_size']
        
        LOGGER.info(f"ğŸ–¼ï¸ CNN ì „ìš© í•™ìŠµ ì„¤ì • ì ìš©:")
        LOGGER.info(f"   â””â”€ ë¡œê·¸ ê°„ê²©: {args.log_interval}")
        LOGGER.info(f"   â””â”€ ì €ì¥ ê°„ê²©: {args.save_interval}")
        LOGGER.info(f"   â””â”€ ì›Œë°ì—…: {warmup_steps} ë‹¨ê³„")
        LOGGER.info(f"   â””â”€ ìµœì†Œ ë²„í¼: {min_buffer_size}")
    else:
        warmup_steps = 0
        min_buffer_size = args.batch_size
    
    for episode in range(args.num_episodes):
        episode_actions = []
        timer.start_episode()
        
        # í™˜ê²½ ë¦¬ì…‹ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        max_reset_attempts = 3
        reset_success = False

        for attempt in range(max_reset_attempts):
            try:
                state = train_env.reset(episode_num=episode)
                reset_success = True
                break
            except Exception as e:
                LOGGER.warning(f"âš ï¸ Episode {episode+1} ë¦¬ì…‹ ì‹œë„ {attempt+1}/{max_reset_attempts} ì‹¤íŒ¨: {e}")
                if attempt < max_reset_attempts - 1:
                    time.sleep(0.1)
                    
        if not reset_success:
            LOGGER.error(f"âŒ Episode {episode+1} ìµœì¢… ë¦¬ì…‹ ì‹¤íŒ¨, ì´ì „ ìƒíƒœë¡œ ë³µêµ¬")
            try:
                state = train_env.reset(episode_num=max(0, episode-1))
            except:
                state = train_env._get_observation()

        episode_reward = 0
        steps = 0
        
        # ì—í”¼ì†Œë“œ ë©”íƒ€ì •ë³´ ë° ê¸¸ì´ ê²°ì • (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        meta_info = getattr(train_env, 'episode_meta_info', {})
        planned_length = meta_info.get('actual_length', 1000)
        is_last_episode = meta_info.get('is_last_in_cycle', False)
        
        if planned_length > MAX_STEPS_PER_EPISODE:
            chunks_needed = (planned_length + MAX_STEPS_PER_EPISODE - 1) // MAX_STEPS_PER_EPISODE
            max_episode_steps = min(planned_length * 1.2, MAX_STEPS_PER_EPISODE * 1.5)
        else:
            max_episode_steps = planned_length
        
        if is_last_episode:
            max_episode_steps = planned_length
        
        # ì—í”¼ì†Œë“œ ì§„í–‰
        done = False
        while not done and steps < max_episode_steps:
            # í–‰ë™ ì„ íƒ
            action = agent.select_action(state, evaluate=False)
            episode_actions.append(action)
            
            # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            next_state, reward, done, info = train_env.step(action)
            
            # ê²½í—˜ì„ ë¦¬í”Œë ˆì´ ë²„í¼ì— ì¶”ê°€
            agent.add_experience(state, action, reward, next_state, done)
                     
            # ğŸ–¼ï¸ CNN íŠ¹í™” ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            buffer_ready = len(agent.replay_buffer) > min_buffer_size
            past_warmup = global_step_count > warmup_steps
            
            if buffer_ready and past_warmup:
                stats = agent.update_parameters(args.batch_size)
                loss_tracker.add_stats(stats)
                
                # CNN íŠ¹í™” ë””ë²„ê¹… (ë” ìì£¼)
                if args.use_cnn and steps % 50 == 0:
                    LOGGER.debug(f"CNN Step {steps}: "
                                f"Actor {stats.get('actor_loss', 0):.6f}, "
                                f"Critic {stats.get('critic_loss', 0):.6f}, "
                                f"Alpha {stats.get('alpha', 0):.6f}")
            elif not past_warmup:
                # ì›Œë°ì—… ì¤‘ ë¡œê¹…
                if global_step_count % 100 == 0:
                    LOGGER.info(f"ğŸ”¥ ì›Œë°ì—… ì¤‘: {global_step_count}/{warmup_steps}")
                loss_tracker.add_stats(None)
            else:
                # ë²„í¼ ë¶€ì¡±
                if steps <= 100:
                    LOGGER.debug(f"ë²„í¼ ì¤€ë¹„ ì¤‘: {len(agent.replay_buffer)}/{min_buffer_size}")
                loss_tracker.add_stats(None)
            
            episode_reward += reward
            steps += 1
            global_step_count += 1
            state = next_state
            
            # ğŸ–¼ï¸ CNNìš© ìŠ¤í…ë³„ ì§„í–‰ìƒí™© ë¡œê¹… (ë” ìì£¼)
            if args.use_cnn and args.step_log_interval > 0 and steps % (args.step_log_interval // 2) == 0:
                log_step_progress(
                    episode + 1, steps, args.num_episodes, max_episode_steps, 
                    loss_tracker, timer, args, agent
                )
            elif not args.use_cnn and args.step_log_interval > 0 and steps % args.step_log_interval == 0:
                log_step_progress(
                    episode + 1, steps, args.num_episodes, max_episode_steps, 
                    loss_tracker, timer, args, agent
                )
        
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
        episode_actions_history.append(episode_actions)
        episode_time = timer.end_episode()
        episode_rewards.append(episode_reward)
        
        # ğŸ–¼ï¸ CNN íŠ¹í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ (ë” ì ê·¹ì )
        if args.use_cnn:
            if episode % 3 == 0:  # 3 ì—í”¼ì†Œë“œë§ˆë‹¤
                torch.cuda.empty_cache()
                if hasattr(torch.cuda, 'reset_peak_memory_stats'):
                    torch.cuda.reset_peak_memory_stats()
        else:
            if episode % 5 == 0:
                torch.cuda.empty_cache()
        
        # VRAM ëª¨ë‹ˆí„°ë§
        if args.use_cnn and episode % 25 == 0:  # CNNì€ ë” ìì£¼ ëª¨ë‹ˆí„°ë§
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            LOGGER.info(f"   ğŸ–¼ï¸ CNN VRAM: í• ë‹¹ {allocated:.2f}GB, ìºì‹œ {reserved:.2f}GB")
        
        portfolio_values.append(info.get('portfolio_value', args.initial_balance))
        shares_history.append(info.get('shares_held', 0))
        
        # ë°ì´í„° í™œìš©ë¥  ê³„ì‚°
        data_utilization = steps / planned_length if planned_length > 0 else 1.0
        steps_processed = steps
        steps_remaining = max(0, planned_length - steps)
        
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ë¡œê¹…
        final_averages = loss_tracker.get_averages()
        model_prefix = "ğŸ–¼ï¸ CNN" if args.use_cnn else "ğŸ§  LSTM" if args.use_lstm else "ğŸ”¢ MLP"
        
        LOGGER.info(f"âœ… {model_prefix} Episode {episode+1} ì™„ë£Œ:")
        LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ë³´ìƒ: {episode_reward:.4f}")
        LOGGER.info(f"   â””â”€ ì‹¤í–‰ ìŠ¤í…: {steps_processed}/{max_episode_steps}")
        LOGGER.info(f"   â””â”€ ë°ì´í„° í™œìš©ë¥ : {data_utilization:.1%}")
        LOGGER.info(f"   â””â”€ í¬íŠ¸í´ë¦¬ì˜¤: ${info.get('portfolio_value', args.initial_balance):.2f}")
        LOGGER.info(f"   â””â”€ í‰ê·  Actor Loss: {final_averages['avg_actor_loss']:.6f}")
        LOGGER.info(f"   â””â”€ í‰ê·  Critic Loss: {final_averages['avg_critic_loss']:.6f}")
        LOGGER.info(f"   â””â”€ í‰ê·  Alpha: {final_averages['avg_alpha']:.6f}")
        
        # ì›Œë°ì—… ì™„ë£Œ ì•Œë¦¼
        if args.use_cnn and global_step_count == warmup_steps:
            LOGGER.info(f"ğŸ”¥ CNN ì›Œë°ì—… ì™„ë£Œ! ({warmup_steps} ë‹¨ê³„)")
        
        # ë‚˜ë¨¸ì§€ ë¡œê¹… ë° ì €ì¥ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼...
        
    LOGGER.info(f"{model_prefix} ìˆœì°¨ì  í•™ìŠµ ì™„ë£Œ!")
    return episode_rewards, portfolio_values, shares_history

if __name__ == "__main__":
    # ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("SAC ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # TradingEnvironment ìŠ¤íƒ€ì¼ì˜ í…ŒìŠ¤íŠ¸ ì„¤ì •
    window_size = WINDOW_SIZE
    feature_dim = 40
    action_dim = 1
    batch_size = 4
    
    print("\n=== ê¸°ë³¸ MLP SAC ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ===")
    
    # ê¸°ë³¸ MLP ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (TradingEnvironmentìš©)
    basic_agent = SACAgent(
        state_dim=None,  # ìë™ ê³„ì‚°ë¨ (30*40 + 2 = 1202)
        action_dim=1,
        input_shape=(window_size, feature_dim),  # ìë™ ê³„ì‚°ì„ ìœ„í•œ íŒíŠ¸
        use_cnn=False,
        use_lstm=False
    )
    
    print(f"ê¸°ë³¸ MLP ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ - ìƒíƒœ ì°¨ì›: {basic_agent.state_dim}")
    
    print("\n=== CNN SAC ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ===")
    
    # CNN ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    cnn_agent = SACAgent(
        state_dim=None,
        action_dim=1,
        input_shape=(window_size, feature_dim),
        use_cnn=True,
        use_lstm=False
    )
    
    print(f"CNN ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    print("\n=== LSTM SAC ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ===")
    
    # LSTM ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    lstm_agent = SACAgent(
        state_dim=None,
        action_dim=1,
        input_shape=(window_size, feature_dim),
        use_cnn=False,
        use_lstm=True,
        lstm_hidden_dim=64,
        num_lstm_layers=1,
        lstm_dropout=0.1
    )
    
    print(f"LSTM ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ëª¨ë“  ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸
    agents = [
        ("ê¸°ë³¸ MLP", basic_agent),
        ("CNN", cnn_agent), 
        ("LSTM", lstm_agent)
    ]
    
    for agent_name, agent in agents:
        print(f"\n--- {agent_name} ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ---")
        
        # TradingEnvironment ìŠ¤íƒ€ì¼ì˜ ìƒíƒœë¡œ í…ŒìŠ¤íŠ¸
        for i in range(batch_size):
            state = {
                'market_data': np.random.randn(window_size, feature_dim),
                'portfolio_state': np.random.randn(2)
            }
            next_state = {
                'market_data': np.random.randn(window_size, feature_dim),
                'portfolio_state': np.random.randn(2)
            }
            
            action = np.random.uniform(-1.0, 1.0)
            reward = np.random.randn()
            done = np.random.choice([True, False])
            
            # ê²½í—˜ ì¶”ê°€
            agent.add_experience(state, action, reward, next_state, done)
            
            # í–‰ë™ ì„ íƒ í…ŒìŠ¤íŠ¸
            if i == 0:
                selected_action = agent.select_action(state, evaluate=False)
                print(f"  ì„ íƒëœ í–‰ë™: {selected_action}")
        
        print(f"  ë¦¬í”Œë ˆì´ ë²„í¼ í¬ê¸°: {len(agent.replay_buffer)}")
        
        # í•™ìŠµ í…ŒìŠ¤íŠ¸
        print(f"  í•™ìŠµ í…ŒìŠ¤íŠ¸ ì¤‘...")
        stats = agent.update_parameters(batch_size=batch_size)
        print(f"  í•™ìŠµ í†µê³„: Actor Loss {stats['actor_loss']:.6f}, Critic Loss {stats['critic_loss']:.6f}")
        
        # ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸
        print(f"  ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸...")
        model_path = agent.save_model(prefix=f'{agent_name.replace(" ", "_").lower()}_test_')
        print(f"  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
        print(f"  ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸...")
        agent.load_model(model_path)
        print(f"  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ë¡œë“œ í›„ í–‰ë™ ì„ íƒ í…ŒìŠ¤íŠ¸
        test_state = {
            'market_data': np.random.randn(window_size, feature_dim),
            'portfolio_state': np.random.randn(2)
        }
        test_action = agent.select_action(test_state, evaluate=True)
        print(f"  ë¡œë“œ í›„ í…ŒìŠ¤íŠ¸ í–‰ë™: {test_action}")
    
    print("\nğŸ‰ ëª¨ë“  SAC ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")