"""
TinyTransformer ê¸°ë°˜ SAC Actorì™€ Critic ë„¤íŠ¸ì›Œí¬ êµ¬í˜„
ì‹œê³„ì—´ ë°ì´í„°ì— ìµœì í™”ëœ ê²½ëŸ‰ Transformer ì•„í‚¤í…ì²˜
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Tuple, Dict, List, Union, Optional
import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

from src.config.ea_teb_config import (
    HIDDEN_DIM,
    DEVICE,
    LOGGER,
    WINDOW_SIZE
)


class PositionalEncoding(nn.Module):
    """ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìœ„í•œ ìœ„ì¹˜ ì¸ì½”ë”©"""
    
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor of shape [seq_len, batch_size, embedding_dim]
        """
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)


class TinyTransformerBlock(nn.Module):
    """ê²½ëŸ‰í™”ëœ Transformer ë¸”ë¡"""
    
    def __init__(
        self, 
        d_model: int, 
        nhead: int = 4, 
        dim_feedforward: int = None, 
        dropout: float = 0.1,
        activation: str = 'gelu'
    ):
        super().__init__()
        
        if dim_feedforward is None:
            dim_feedforward = d_model * 2  # ì¼ë°˜ì ì¸ 4ë°° ëŒ€ì‹  2ë°°ë¡œ ê²½ëŸ‰í™”
        
        # Multi-head self-attention
        self.self_attn = nn.MultiheadAttention(
            d_model, nhead, dropout=dropout, batch_first=True
        )
        
        # Feed-forward network
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        
        # Layer normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
        # Activation function
        if activation == 'gelu':
            self.activation = F.gelu
        elif activation == 'relu':
            self.activation = F.relu
        else:
            raise ValueError(f"Unsupported activation: {activation}")
    
    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            src: [batch_size, seq_len, d_model]
            src_mask: attention mask
        """
        # Self-attention with residual connection
        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        
        # Feed-forward with residual connection
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        
        return src


class TinyTransformerEncoder(nn.Module):
    """ê²½ëŸ‰í™”ëœ Transformer ì¸ì½”ë”"""
    
    def __init__(
        self,
        input_dim: int,
        d_model: int = 128,
        nhead: int = 4,
        num_layers: int = 2,
        dim_feedforward: int = None,
        dropout: float = 0.1,
        max_seq_len: int = 200
    ):
        super().__init__()
        
        self.d_model = d_model
        self.input_dim = input_dim
        
        # Input projection
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # Positional encoding
        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_len)
        
        # Transformer blocks
        self.transformer_blocks = nn.ModuleList([
            TinyTransformerBlock(d_model, nhead, dim_feedforward, dropout)
            for _ in range(num_layers)
        ])
        
        # Layer normalization
        self.norm = nn.LayerNorm(d_model)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, src: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            src: [batch_size, seq_len, input_dim]
            mask: attention mask
        Returns:
            encoded: [batch_size, seq_len, d_model]
        """
        # Input projection
        src = self.input_projection(src) * math.sqrt(self.d_model)
        
        # Positional encoding (transpose for positional encoding, then back)
        src = src.transpose(0, 1)  # [seq_len, batch_size, d_model]
        src = self.pos_encoder(src)
        src = src.transpose(0, 1)  # [batch_size, seq_len, d_model]
        
        # Apply transformer blocks
        for transformer_block in self.transformer_blocks:
            src = transformer_block(src, mask)
        
        # Final layer norm
        src = self.norm(src)
        
        return src


class TinyTransformerActorNetwork(nn.Module):
    """
    TinyTransformer ê¸°ë°˜ Actor ë„¤íŠ¸ì›Œí¬
    ì‹œê³„ì—´ ë°ì´í„°ì˜ temporal dependencyë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ë§
    """
    
    def __init__(
        self,
        input_shape: Tuple[int, int],  # (window_size, feature_dim)
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        d_model: int = 128,
        nhead: int = 4,
        num_layers: int = 2,
        dropout_rate: float = 0.1,
        log_std_min: float = -2.0,
        log_std_max: float = 2.0,
        device: torch.device = DEVICE
    ):
        super().__init__()
        
        self.window_size, self.feature_dim = input_shape
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.d_model = d_model
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.device = device
        
        # ë™ì  ì…ë ¥ ì¡°ì • ì„¤ì •
        self.expected_feature_dim = self.feature_dim
        self.adaptive_input = True
        
        LOGGER.info(f"ğŸ¤– TinyTransformer Actor ì´ˆê¸°í™”: ê¸°ëŒ€ íŠ¹ì„± ì°¨ì› {self.expected_feature_dim}")
        
        # TinyTransformer ì¸ì½”ë” (market_data ì²˜ë¦¬ìš©)
        self.transformer_encoder = TinyTransformerEncoder(
            input_dim=self.feature_dim,
            d_model=d_model,
            nhead=nhead,
            num_layers=num_layers,
            dropout=dropout_rate,
            max_seq_len=self.window_size + 10  # ì—¬ìœ ë¶„ ì¶”ê°€
        )
        
        # Global average pooling ë˜ëŠ” attention pooling
        self.attention_pooling = nn.MultiheadAttention(
            d_model, num_heads=1, dropout=dropout_rate, batch_first=True
        )
        self.pooling_query = nn.Parameter(torch.randn(1, 1, d_model))
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì²˜ë¦¬
        self.portfolio_fc = nn.Sequential(
            nn.Linear(2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # íŠ¹ì„± ìœµí•© ë„¤íŠ¸ì›Œí¬
        fusion_input_size = d_model + hidden_dim // 4
        self.fusion_network = nn.Sequential(
            nn.Linear(fusion_input_size, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
        self.to(device)
        LOGGER.info(f"TinyTransformer Actor ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ: "
                   f"d_model={d_model}, heads={nhead}, layers={num_layers}")
    
    def _initialize_weights(self):
        """Xavier ì´ˆê¸°í™” ì ìš©"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """ìˆœì „íŒŒ"""
        market_data = state['market_data']
        portfolio_state = state['portfolio_state']
        
        # ë™ì  ì…ë ¥ ì°¨ì› ì¡°ì •
        if self.adaptive_input:
            market_data = self._adjust_input_dimensions(market_data)
        
        # ì°¨ì› ì²˜ë¦¬
        if len(market_data.shape) == 2:  # (W, F)
            market_data = market_data.unsqueeze(0)  # (1, W, F)
        if len(portfolio_state.shape) == 1:
            portfolio_state = portfolio_state.unsqueeze(0)
        
        batch_size = market_data.size(0)
        
        # Transformer encoding
        encoded = self.transformer_encoder(market_data)  # [B, W, d_model]
        
        # Attention pooling to get global representation
        query = self.pooling_query.expand(batch_size, -1, -1)  # [B, 1, d_model]
        pooled_output, _ = self.attention_pooling(query, encoded, encoded)  # [B, 1, d_model]
        market_features = pooled_output.squeeze(1)  # [B, d_model]
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì²˜ë¦¬
        portfolio_features = self.portfolio_fc(portfolio_state)
        
        # íŠ¹ì„± ìœµí•©
        combined_features = torch.cat([market_features, portfolio_features], dim=1)
        fused_features = self.fusion_network(combined_features)
        
        # ì¶œë ¥ ê³„ì‚°
        mean = self.mean(fused_features)
        log_std = self.log_std(fused_features)
        log_std = torch.clamp(log_std, min=self.log_std_min, max=self.log_std_max)
        
        return mean, log_std
    
    def sample(self, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """í–‰ë™ ìƒ˜í”Œë§"""
        mean, log_std = self.forward(state)
        std = log_std.exp()
        
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()
        y_t = torch.tanh(x_t)
        
        log_prob = normal.log_prob(x_t) - torch.log(1 - y_t.pow(2) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        
        return y_t, log_prob, mean
    
    def _adjust_input_dimensions(self, market_data: torch.Tensor) -> torch.Tensor:
        """ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì„ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ë™ì  ì¡°ì •"""
        if len(market_data.shape) == 3:  # (B, W, F)
            current_feature_dim = market_data.shape[2]
            batch_size, window_size = market_data.shape[0], market_data.shape[1]
        elif len(market_data.shape) == 2:  # (W, F)
            current_feature_dim = market_data.shape[1]
            batch_size, window_size = 1, market_data.shape[0]
            market_data = market_data.unsqueeze(0)
        else:
            LOGGER.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì…ë ¥ í˜•íƒœ: {market_data.shape}")
            return market_data
        
        # ì°¨ì›ì´ ì¼ì¹˜í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if current_feature_dim == self.expected_feature_dim:
            return market_data
        
        # ì°¨ì› ì¡°ì •
        if current_feature_dim > self.expected_feature_dim:
            # íŠ¹ì„± ì°¨ì›ì´ ë” í° ê²½ìš°: ìë¥´ê¸°
            adjusted_data = market_data[:, :, :self.expected_feature_dim]
            LOGGER.debug(f"ğŸ“Š TinyTransformer Actor ì…ë ¥ ì°¨ì› ì¶•ì†Œ: {current_feature_dim} â†’ {self.expected_feature_dim}")
        else:
            # íŠ¹ì„± ì°¨ì›ì´ ë” ì‘ì€ ê²½ìš°: ì œë¡œ íŒ¨ë”©
            padding_size = self.expected_feature_dim - current_feature_dim
            padding = torch.zeros(batch_size, window_size, padding_size, device=market_data.device)
            adjusted_data = torch.cat([market_data, padding], dim=2)
            LOGGER.debug(f"ğŸ“Š TinyTransformer Actor ì…ë ¥ ì°¨ì› í™•ì¥: {current_feature_dim} â†’ {self.expected_feature_dim}")
        
        return adjusted_data


class TinyTransformerCriticNetwork(nn.Module):
    """
    TinyTransformer ê¸°ë°˜ Critic ë„¤íŠ¸ì›Œí¬
    Double Q-learningì„ ìœ„í•œ ë‘ ê°œì˜ Q ë„¤íŠ¸ì›Œí¬
    """
    
    def __init__(
        self,
        input_shape: Tuple[int, int],  # (window_size, feature_dim)
        action_dim: int = 1,
        hidden_dim: int = HIDDEN_DIM,
        d_model: int = 128,
        nhead: int = 4,
        num_layers: int = 2,
        dropout_rate: float = 0.1,
        device: torch.device = DEVICE
    ):
        super().__init__()
        
        self.window_size, self.feature_dim = input_shape
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.d_model = d_model
        self.device = device
        
        # ë™ì  ì…ë ¥ ì¡°ì • ì„¤ì •
        self.expected_feature_dim = self.feature_dim
        self.adaptive_input = True
        
        LOGGER.info(f"ğŸ¤– TinyTransformer Critic ì´ˆê¸°í™”: ê¸°ëŒ€ íŠ¹ì„± ì°¨ì› {self.expected_feature_dim}")
        
        # Q1 ë„¤íŠ¸ì›Œí¬ì˜ Transformer ì¸ì½”ë”
        self.q1_transformer_encoder = TinyTransformerEncoder(
            input_dim=self.feature_dim,
            d_model=d_model,
            nhead=nhead,
            num_layers=num_layers,
            dropout=dropout_rate,
            max_seq_len=self.window_size + 10
        )
        
        # Q2 ë„¤íŠ¸ì›Œí¬ì˜ Transformer ì¸ì½”ë”
        self.q2_transformer_encoder = TinyTransformerEncoder(
            input_dim=self.feature_dim,
            d_model=d_model,
            nhead=nhead,
            num_layers=num_layers,
            dropout=dropout_rate,
            max_seq_len=self.window_size + 10
        )
        
        # Attention pooling for Q1 and Q2
        self.q1_attention_pooling = nn.MultiheadAttention(
            d_model, num_heads=1, dropout=dropout_rate, batch_first=True
        )
        self.q2_attention_pooling = nn.MultiheadAttention(
            d_model, num_heads=1, dropout=dropout_rate, batch_first=True
        )
        self.q1_pooling_query = nn.Parameter(torch.randn(1, 1, d_model))
        self.q2_pooling_query = nn.Parameter(torch.randn(1, 1, d_model))
        
        # Q1 í¬íŠ¸í´ë¦¬ì˜¤ ë° í–‰ë™ ì²˜ë¦¬
        self.q1_portfolio_fc = nn.Sequential(
            nn.Linear(2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        self.q1_action_fc = nn.Sequential(
            nn.Linear(action_dim, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # Q2 í¬íŠ¸í´ë¦¬ì˜¤ ë° í–‰ë™ ì²˜ë¦¬
        self.q2_portfolio_fc = nn.Sequential(
            nn.Linear(2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        self.q2_action_fc = nn.Sequential(
            nn.Linear(action_dim, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # Q1, Q2 ìœµí•© ë„¤íŠ¸ì›Œí¬
        fusion_input_size = d_model + hidden_dim // 2  # market + portfolio + action
        
        self.q1_fusion = nn.Sequential(
            nn.Linear(fusion_input_size, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, 1)
        )
        
        self.q2_fusion = nn.Sequential(
            nn.Linear(fusion_input_size, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, 1)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
        self.to(device)
        LOGGER.info(f"TinyTransformer Critic ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ: "
                   f"d_model={d_model}, heads={nhead}, layers={num_layers}")
    
    def _initialize_weights(self):
        """Xavier ì´ˆê¸°í™” ì ìš©"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, state: Dict[str, torch.Tensor], action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """ìˆœì „íŒŒ"""
        market_data = state['market_data']
        portfolio_state = state['portfolio_state']
        
        # ë™ì  ì…ë ¥ ì°¨ì› ì¡°ì •
        if self.adaptive_input:
            market_data = self._adjust_input_dimensions(market_data)
        
        # ì°¨ì› ì²˜ë¦¬
        if len(market_data.shape) == 2:
            market_data = market_data.unsqueeze(0)
        if len(action.shape) == 1:
            action = action.unsqueeze(0)
        if len(portfolio_state.shape) == 1:
            portfolio_state = portfolio_state.unsqueeze(0)
        
        batch_size = market_data.size(0)
        
        # Q1 ê³„ì‚°
        q1_encoded = self.q1_transformer_encoder(market_data)
        q1_query = self.q1_pooling_query.expand(batch_size, -1, -1)
        q1_pooled_output, _ = self.q1_attention_pooling(q1_query, q1_encoded, q1_encoded)
        q1_market_features = q1_pooled_output.squeeze(1)
        
        q1_portfolio_features = self.q1_portfolio_fc(portfolio_state)
        q1_action_features = self.q1_action_fc(action)
        q1_combined = torch.cat([q1_market_features, q1_portfolio_features, q1_action_features], dim=1)
        q1 = self.q1_fusion(q1_combined)
        
        # Q2 ê³„ì‚°
        q2_encoded = self.q2_transformer_encoder(market_data)
        q2_query = self.q2_pooling_query.expand(batch_size, -1, -1)
        q2_pooled_output, _ = self.q2_attention_pooling(q2_query, q2_encoded, q2_encoded)
        q2_market_features = q2_pooled_output.squeeze(1)
        
        q2_portfolio_features = self.q2_portfolio_fc(portfolio_state)
        q2_action_features = self.q2_action_fc(action)
        q2_combined = torch.cat([q2_market_features, q2_portfolio_features, q2_action_features], dim=1)
        q2 = self.q2_fusion(q2_combined)
        
        return q1, q2
    
    def _adjust_input_dimensions(self, market_data: torch.Tensor) -> torch.Tensor:
        """ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì„ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ë™ì  ì¡°ì •"""
        if len(market_data.shape) == 3:  # (B, W, F)
            current_feature_dim = market_data.shape[2]
            batch_size, window_size = market_data.shape[0], market_data.shape[1]
        elif len(market_data.shape) == 2:  # (W, F)
            current_feature_dim = market_data.shape[1]
            batch_size, window_size = 1, market_data.shape[0]
            market_data = market_data.unsqueeze(0)
        else:
            LOGGER.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì…ë ¥ í˜•íƒœ: {market_data.shape}")
            return market_data
        
        # ì°¨ì›ì´ ì¼ì¹˜í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if current_feature_dim == self.expected_feature_dim:
            return market_data
        
        # ì°¨ì› ì¡°ì •
        if current_feature_dim > self.expected_feature_dim:
            # íŠ¹ì„± ì°¨ì›ì´ ë” í° ê²½ìš°: ìë¥´ê¸°
            adjusted_data = market_data[:, :, :self.expected_feature_dim]
            LOGGER.debug(f"ğŸ“Š TinyTransformer Critic ì…ë ¥ ì°¨ì› ì¶•ì†Œ: {current_feature_dim} â†’ {self.expected_feature_dim}")
        else:
            # íŠ¹ì„± ì°¨ì›ì´ ë” ì‘ì€ ê²½ìš°: ì œë¡œ íŒ¨ë”©
            padding_size = self.expected_feature_dim - current_feature_dim
            padding = torch.zeros(batch_size, window_size, padding_size, device=market_data.device)
            adjusted_data = torch.cat([market_data, padding], dim=2)
            LOGGER.debug(f"ğŸ“Š TinyTransformer Critic ì…ë ¥ ì°¨ì› í™•ì¥: {current_feature_dim} â†’ {self.expected_feature_dim}")
        
        return adjusted_data


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("=== TinyTransformer ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ ===")
    
    # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
    batch_size = 4
    window_size = WINDOW_SIZE
    feature_dim = 5
    action_dim = 1
    d_model = 64  # í…ŒìŠ¤íŠ¸ìš© ì‘ì€ ëª¨ë¸
    
    # ë„¤íŠ¸ì›Œí¬ ìƒì„±
    actor = TinyTransformerActorNetwork(
        input_shape=(window_size, feature_dim),
        action_dim=action_dim,
        d_model=d_model,
        nhead=2,
        num_layers=2
    )
    
    critic = TinyTransformerCriticNetwork(
        input_shape=(window_size, feature_dim),
        action_dim=action_dim,
        d_model=d_model,
        nhead=2,
        num_layers=2
    )
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    market_data = torch.randn(batch_size, window_size, feature_dim)
    portfolio_state = torch.randn(batch_size, 2)
    action = torch.randn(batch_size, action_dim)
    
    state_dict = {
        'market_data': market_data,
        'portfolio_state': portfolio_state
    }
    
    # Actor í…ŒìŠ¤íŠ¸
    print(f"ì…ë ¥ í˜•íƒœ: market_data={market_data.shape}, portfolio_state={portfolio_state.shape}")
    
    mean, log_std = actor(state_dict)
    sampled_action, log_prob, _ = actor.sample(state_dict)
    
    print(f"Actor ì¶œë ¥: mean={mean.shape}, log_std={log_std.shape}")
    print(f"Actor ìƒ˜í”Œ: action={sampled_action.shape}, log_prob={log_prob.shape}")
    
    # Critic í…ŒìŠ¤íŠ¸
    q1, q2 = critic(state_dict, action)
    print(f"Critic ì¶œë ¥: Q1={q1.shape}, Q2={q2.shape}")
    
    print("âœ… TinyTransformer ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 