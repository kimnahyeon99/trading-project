"""
SAC ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (í†µí•© ê°„ì†Œí™” ë²„ì „)
- ê²€ì¦ ì—†ì´ í›ˆë ¨ì—ë§Œ ì§‘ì¤‘
- í•™ìŠµ ì‹œê°„ ì¸¡ì • ë° ë¡œê¹…
- ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ë¡œê·¸ ì¶œë ¥
- í†µí•©ëœ ëª¨ë¸ ì„¤ì • ì‚¬ìš©
"""
import sys
import os
import time
from datetime import datetime, timedelta

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

import argparse
import torch
import numpy as np
from pathlib import Path

from src.config.ea_teb_config import (
    DEVICE,
    HIDDEN_DIM,
    BATCH_SIZE,
    NUM_EPISODES,
    EVALUATE_INTERVAL,
    SAVE_MODEL_INTERVAL,
    MAX_STEPS_PER_EPISODE,
    TARGET_SYMBOLS,
    LOGGER,
    INITIAL_BALANCE,
    WINDOW_SIZE,
    LEARNING_RATE_ACTOR,
    LEARNING_RATE_CRITIC,
    LEARNING_RATE_ALPHA,
    ALPHA_INIT,
    GRADIENT_CLIP,
)
from src.data_collection.data_collector import DataCollector
from src.preprocessing.data_processor import DataProcessor
from src.environment.trading_env import TradingEnvironment, MultiAssetTradingEnvironment
from src.models.sac_agent import SACAgent
from src.utils.utils import create_directory, get_timestamp

episode_actions_history: list = []

class TrainingTimer:
    """í•™ìŠµ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.start_time = None
        self.episode_start_time = None
        self.episode_times = []
        
    def start_training(self):
        """ì „ì²´ í•™ìŠµ ì‹œì‘"""
        self.start_time = time.time()
        LOGGER.info(f"ğŸš€ í•™ìŠµ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    def start_episode(self):
        """ì—í”¼ì†Œë“œ ì‹œì‘"""
        self.episode_start_time = time.time()
        
    def end_episode(self):
        """ì—í”¼ì†Œë“œ ì¢…ë£Œ"""
        if self.episode_start_time:
            episode_time = time.time() - self.episode_start_time
            self.episode_times.append(episode_time)
            return episode_time
        return 0
        
    def get_training_time(self):
        """ì „ì²´ í•™ìŠµ ì‹œê°„ ë°˜í™˜"""
        if self.start_time:
            return time.time() - self.start_time
        return 0
        
    def get_avg_episode_time(self):
        """í‰ê·  ì—í”¼ì†Œë“œ ì‹œê°„ ë°˜í™˜"""
        if self.episode_times:
            return np.mean(self.episode_times)
        return 0
        
    def get_eta(self, current_episode, total_episodes):
        """ë‚¨ì€ ì‹œê°„ ì¶”ì •"""
        if len(self.episode_times) > 0:
            avg_time = self.get_avg_episode_time()
            remaining_episodes = total_episodes - current_episode
            return remaining_episodes * avg_time
        return 0
        
    def format_time(self, seconds):
        """ì‹œê°„ì„ ë³´ê¸° ì¢‹ê²Œ í¬ë§·"""
        return str(timedelta(seconds=int(seconds)))

def parse_args():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='SAC ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (í†µí•© ê°„ì†Œí™” ë²„ì „)')
    
    # ë°ì´í„° ê´€ë ¨ ì¸ì
    parser.add_argument('--symbols', nargs='+', default=None, help='í•™ìŠµì— ì‚¬ìš©í•  ì£¼ì‹ ì‹¬ë³¼ ëª©ë¡')
    parser.add_argument('--collect_data', action='store_true', help='ë°ì´í„° ìˆ˜ì§‘ ì—¬ë¶€')
    
    # í™˜ê²½ ê´€ë ¨ ì¸ì
    parser.add_argument('--window_size', type=int, default=WINDOW_SIZE, help='ê´€ì¸¡ ìœˆë„ìš° í¬ê¸°')
    parser.add_argument('--initial_balance', type=float, default=INITIAL_BALANCE, help='ì´ˆê¸° ìë³¸ê¸ˆ')
    parser.add_argument('--multi_asset', action='store_true', help='ë‹¤ì¤‘ ìì‚° í™˜ê²½ ì‚¬ìš© ì—¬ë¶€')
    
    # ëª¨ë¸ ê´€ë ¨ ì¸ì(ê¸°ë³¸ MLP)
    parser.add_argument('--hidden_dim', type=int, default=HIDDEN_DIM, help='ì€ë‹‰ì¸µ ì°¨ì›')
    parser.add_argument('--model_type', type=str, choices=['mlp', 'cnn', 'lstm', 'mamba', 'tinytransformer'], 
                       help='ëª¨ë¸ íƒ€ì… (--use_* ì˜µì…˜ ëŒ€ì‹  ì‚¬ìš© ê°€ëŠ¥)')
    # CNN
    parser.add_argument('--use_cnn', action='store_true', help='CNN ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€')
    # LSTM
    parser.add_argument('--use_lstm', action='store_true', help='LSTM ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€')
    # Mamba
    parser.add_argument('--use_mamba', action='store_true', help='Mamba ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€')
    # TinyTransformer
    parser.add_argument('--use_tinytransformer', action='store_true', help='TinyTransformer ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€')
    # ë¡œë“œ ê²½ë¡œ
    parser.add_argument('--load_model', type=str, default=None, help='ë¡œë“œí•  ëª¨ë¸ ê²½ë¡œ')
    
    # í•™ìŠµ ê´€ë ¨ ì¸ì
    parser.add_argument('--batch_size', type=int, default=BATCH_SIZE, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_episodes', type=int, default=NUM_EPISODES, help='í•™ìŠµí•  ì´ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--log_interval', type=int, default=EVALUATE_INTERVAL, help='ë¡œê·¸ ì¶œë ¥ ê°„ê²©')
    parser.add_argument('--save_interval', type=int, default=SAVE_MODEL_INTERVAL, help='ëª¨ë¸ ì €ì¥ ê°„ê²©')
    parser.add_argument('--max_steps', type=int, default=MAX_STEPS_PER_EPISODE, help='ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜')
    parser.add_argument('--buffer_type', type=str, default='standard', choices=['standard', 'prioritized'], help='ë¦¬í”Œë ˆì´ ë²„í¼ íƒ€ì…')

    
    # ìŠ¤í…ë³„ ë¡œê¹… ê´€ë ¨ ì¸ì
    parser.add_argument('--step_log_interval', type=int, default=100, help='ìŠ¤í…ë³„ ë¡œê·¸ ì¶œë ¥ ê°„ê²©')
    
    # ë¡œê·¸ ë ˆë²¨
    parser.add_argument("--log_level", choices=['minimal', 'normal', 'detailed'], default='normal', help="ë¡œê·¸ ë ˆë²¨")
    
    return parser.parse_args()

def create_training_environment(results, symbols, args):
    """í‘œì¤€ SAC í•™ìŠµìš© í™˜ê²½ì„ ìƒì„±"""
    LOGGER.info("í‘œì¤€ SAC í•™ìŠµìš© í™˜ê²½ ìƒì„± ì¤‘...")
    
    if args.multi_asset:
        # ë‹¤ì¤‘ ìì‚°ì˜ ê²½ìš° ê¸°ì¡´ ë°©ì‹ ìœ ì§€ (ì¼ë‹¨)
        LOGGER.error("âŒ ë‹¤ì¤‘ ìì‚° í™˜ê²½ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        LOGGER.info("ë‹¨ì¼ ìì‚° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì„¸ìš”: --multi_asset ì˜µì…˜ ì œê±°")
        return None
        
    else:
        # ë‹¨ì¼ ìì‚° ê¸°ë³¸ í™˜ê²½ ìƒì„±
        symbol = symbols[0]
        LOGGER.info(f"ë‹¨ì¼ ìì‚° íŠ¸ë ˆì´ë”© í™˜ê²½ ìƒì„± ì¤‘: {symbol}")
        
        if symbol not in results:
            LOGGER.error(f"{symbol} ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        if 'train' in results[symbol] and 'featured_data' in results[symbol]:
            normalized_data = results[symbol]['train']
            original_data = results[symbol]['featured_data']
        else:
            LOGGER.error(f"{symbol} í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ê¸°ë³¸ í™˜ê²½ ìƒì„±
        base_env = TradingEnvironment(
            data=normalized_data,
            raw_data=original_data,
            window_size=args.window_size,
            initial_balance=args.initial_balance,
            symbol=symbol,
            log_level=args.log_level,
            train_data=True
        )
        
        LOGGER.info(f"âœ… í‘œì¤€ SAC í•™ìŠµ í™˜ê²½ ìƒì„± ì™„ë£Œ")
        LOGGER.info(f"   â””â”€ ë°ì´í„° ê¸¸ì´: {len(normalized_data)}")
        LOGGER.info(f"   â””â”€ ìœˆë„ìš° í¬ê¸°: {args.window_size}")
        LOGGER.info(f"   â””â”€ ì´ˆê¸° ì”ê³ : ${args.initial_balance:,.0f}")
        
        return base_env

def create_agent(env, args):
    """SAC ì—ì´ì „íŠ¸ ìƒì„± (ë™ì  ì„¤ì • ì ìš© ë²„ì „)"""
    LOGGER.info("SAC ì—ì´ì „íŠ¸ ìƒì„± ì¤‘...")

    # í–‰ë™ ì°¨ì› ê²°ì •
    if args.multi_asset:
        action_dim = len(env.envs)
    else:
        action_dim = 1

    # --model_type ì¸ìê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if args.model_type:
        model_type = args.model_type.lower()
        args.use_cnn = (model_type == 'cnn')
        args.use_lstm = (model_type == 'lstm')
        args.use_mamba = (model_type == 'mamba')
        args.use_tinytransformer = (model_type == 'tinytransformer')
        LOGGER.info(f"ğŸ“‹ --model_type ì¸ìë¡œ {model_type.upper()} ëª¨ë¸ ì§€ì •ë¨")
    else:
        # ìƒí˜¸ ë°°íƒ€ì  ê²€ì¦ ì¶”ê°€
        model_flags = [args.use_cnn, args.use_lstm, args.use_mamba, args.use_tinytransformer]
        if sum(model_flags) > 1:
            LOGGER.error("âŒ CNN, LSTM, Mamba, TinyTransformer ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return None

        # --use_* ì˜µì…˜ìœ¼ë¡œ ëª¨ë¸ íƒ€ì… ê²°ì •
        if args.use_tinytransformer:
            model_type = 'tinytransformer'
            LOGGER.info("ğŸ¤– TinyTransformer ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        elif args.use_mamba:
            model_type = 'mamba'
            LOGGER.info("ğŸ Mamba ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        elif args.use_lstm:
            model_type = 'lstm'
            LOGGER.info("ğŸ§  LSTM ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        elif args.use_cnn:
            model_type = 'cnn'
            LOGGER.info("ğŸ–¼ï¸ CNN ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            model_type = 'mlp'
            LOGGER.info("ğŸ“Š ê¸°ë³¸ MLP ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ğŸ”¥ ì‹¬ë³¼ ì •ë³´ (ë™ì  ì„¤ì •ì— í•„ìš”)
    symbol = None
    if hasattr(args, 'symbols') and args.symbols and len(args.symbols) == 1:
        symbol = args.symbols[0]
    elif hasattr(env, 'symbol'):
        symbol = env.symbol

    LOGGER.info(f"ğŸ”§ ë™ì  ëª¨ë¸ ì„¤ì •:")
    LOGGER.info(f"   â””â”€ íƒ€ì…: {model_type.upper()}")
    LOGGER.info(f"   â””â”€ ì‹¬ë³¼: {symbol or 'Multi/Unknown'}")
    LOGGER.info(f"   â””â”€ ì„¤ì • ì†ŒìŠ¤: ea_teb_config.MODEL_DEFAULTS")

    # ğŸ¤– TinyTransformer ì „ìš© ìµœì í™” ì„¤ì • ì ìš©
    tt_config = {}  # ê¸°ë³¸ê°’ ì´ˆê¸°í™”
    
    if args.use_tinytransformer:
        try:
            from src.config.tinytransformer_config import get_optimized_config
            tt_config = get_optimized_config()
            
            LOGGER.info("ğŸ¤– TinyTransformer ìµœì í™” ì„¤ì • ì ìš© ì¤‘...")
            LOGGER.info(f"   â””â”€ Batch Size: {tt_config['batch_size']}")
            LOGGER.info(f"   â””â”€ Actor LR: {tt_config['actor_lr']}")
            LOGGER.info(f"   â””â”€ Critic LR: {tt_config['critic_lr']}")
            LOGGER.info(f"   â””â”€ Dropout: {tt_config['dropout_rate']}")
            LOGGER.info(f"   â””â”€ Max Steps/Episode: {tt_config['max_steps_per_episode']}")
            
            # argsì— TinyTransformer ì„¤ì • ì ìš©
            args.batch_size = tt_config['batch_size']
            args.max_steps_per_episode = tt_config['max_steps_per_episode']
            args.replay_buffer_size = tt_config['replay_buffer_size']
            args.num_episodes = tt_config['num_episodes']
            args.evaluate_interval = tt_config['evaluate_interval']
            args.save_model_interval = tt_config['save_model_interval']
            
        except ImportError as e:
            LOGGER.warning(f"âš ï¸ TinyTransformer ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            LOGGER.info("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            tt_config = {
                'actor_lr': 8e-5,
                'critic_lr': 1e-4,
                'alpha_lr': 8e-5,
                'dropout_rate': 0.15,
                'alpha_init': 0.15,
                'gradient_clip_norm': 0.5,
            }

    # ğŸ”¥ ì—ì´ì „íŠ¸ ìƒì„± íŒŒë¼ë¯¸í„° (TinyTransformer ìµœì í™” ì ìš©)
    agent_kwargs = {
        'state_dim': None,
        'action_dim': action_dim,
        'hidden_dim': args.hidden_dim,
        'input_shape': (args.window_size, env.feature_dim if not args.multi_asset else list(env.envs.values())[0].feature_dim),
        'use_cnn': args.use_cnn,
        'use_lstm': args.use_lstm,
        'use_mamba': args.use_mamba,
        'use_tinytransformer': args.use_tinytransformer,
        'model_type': model_type,
        'symbol': symbol,  # ğŸ†• ì‹¬ë³¼ ì „ë‹¬ (ë™ì  ì„¤ì •ìš©)
        
        # ğŸ”¥ ëª¨ë¸ë³„ ìµœì í™”ëœ ì„¤ì • ì ìš©
        'actor_lr': tt_config['actor_lr'] if args.use_tinytransformer else LEARNING_RATE_ACTOR,     
        'critic_lr': tt_config['critic_lr'] if args.use_tinytransformer else LEARNING_RATE_CRITIC,   
        'alpha_lr': tt_config['alpha_lr'] if args.use_tinytransformer else LEARNING_RATE_ALPHA,     
        'alpha_init': tt_config['alpha_init'] if args.use_tinytransformer else ALPHA_INIT,            
        'dropout_rate': tt_config['dropout_rate'] if args.use_tinytransformer else 0.1,                 
        'gradient_clip_norm': tt_config['gradient_clip_norm'] if args.use_tinytransformer else GRADIENT_CLIP, 
        
        # LSTM ì „ìš© íŒŒë¼ë¯¸í„°
        'lstm_hidden_dim': 128,              
        'num_lstm_layers': 2,                
        'lstm_dropout': 0.2,                 
              
        # ğŸ†• ì•ˆì •ì„± íŒŒë¼ë¯¸í„° (SAC í‘œì¤€)
        'use_gradient_clipping': True,
        'adaptive_alpha': True,
        
        # ê¸°íƒ€ íŒŒë¼ë¯¸í„°
        'training_symbols': args.symbols if args.symbols else TARGET_SYMBOLS,
    }

    # ğŸ”¥ ì—ì´ì „íŠ¸ ìƒì„± (ë‚´ë¶€ì—ì„œ ë™ì  ì„¤ì • ì ìš©ë¨)
    agent = SACAgent(**agent_kwargs)

    # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    agent.actor = agent.actor.to(DEVICE)
    agent.critic = agent.critic.to(DEVICE)
    agent.critic_target = agent.critic_target.to(DEVICE)
    
    # ëª¨ë¸ íƒ€ì… ì •ë³´ë¥¼ ì—ì´ì „íŠ¸ì— ì €ì¥ (ì €ì¥ ì‹œ ì‚¬ìš©)
    agent.model_type = model_type
    agent.training_symbols = args.symbols if args.symbols else TARGET_SYMBOLS

    # ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
    if args.load_model:
        LOGGER.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {args.load_model}")
        try:
            agent.load_model(args.load_model)
            # ë¡œë“œ í›„ì—ë„ ëª…ì‹œì ìœ¼ë¡œ ë‹¤ì‹œ ì´ë™ (GPU ë¡œë“œ ì‹œ í•„ìš”í•¨)
            agent.actor = agent.actor.to(DEVICE)
            agent.critic = agent.critic.to(DEVICE)
            agent.critic_target = agent.critic_target.to(DEVICE)
            LOGGER.info("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            LOGGER.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            LOGGER.info("âœ… ìƒˆ ëª¨ë¸ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # ğŸ”¥ ìµœì¢… ì„¤ì • í™•ì¸ ë¡œê·¸ (ë™ì  ì ìš© ê²°ê³¼)
    LOGGER.info(f"âœ… SAC ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ")
    LOGGER.info(f"   â””â”€ ëª¨ë¸: {model_type.upper()}")
    LOGGER.info(f"   â””â”€ í–‰ë™ ì°¨ì›: {action_dim}")
    LOGGER.info(f"   â””â”€ ì€ë‹‰ì¸µ: {args.hidden_dim}")
    LOGGER.info(f"   â””â”€ ì¥ì¹˜: {DEVICE}")
    LOGGER.info(f"   â””â”€ ìµœì¢… Actor LR: {agent.actor_lr:.6f}")
    LOGGER.info(f"   â””â”€ ìµœì¢… Critic LR: {agent.critic_lr:.6f}")
    LOGGER.info(f"   â””â”€ ìµœì¢… Dropout: {agent.dropout_rate}")
    LOGGER.info(f"   â””â”€ ë²„í¼ í¬ê¸°: {len(agent.replay_buffer) if hasattr(agent.replay_buffer, '__len__') else 'N/A'}")
    
    return agent

def validate_training_environment(train_env, args):
    """í•™ìŠµ í™˜ê²½ ê²€ì¦ ë° ë°ì´í„° ì†ì‹¤ ìœ„í—˜ ì‚¬ì „ ì ê²€"""
    LOGGER.info("ğŸ” í•™ìŠµ í™˜ê²½ ê²€ì¦ ì¤‘...")
    
    # ìˆœì°¨ì  í™˜ê²½ì¸ì§€ í™•ì¸
    if not hasattr(train_env, 'episode_manager'):
        LOGGER.warning("âš ï¸ ìˆœì°¨ì  í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ë°ì´í„° í™œìš©ë¥ ì´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return True
    
    # ì»¤ë²„ë¦¬ì§€ ë¶„ì„
    episode_manager = train_env.episode_manager
    
    if hasattr(episode_manager, 'get_coverage_summary'):
        coverage = episode_manager.get_coverage_summary()
        
        LOGGER.info("ğŸ“Š í™˜ê²½ ê²€ì¦ ê²°ê³¼:")
        LOGGER.info(f"   â””â”€ ì´ ì—í”¼ì†Œë“œ ìˆ˜: {coverage['total_episodes']}")
        LOGGER.info(f"   â””â”€ ë°ì´í„° ì»¤ë²„ë¦¬ì§€: {coverage['unique_coverage_pct']:.1f}%")
        LOGGER.info(f"   â””â”€ í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´: {coverage['average_episode_length']:.1f}")
        
        # ê²½ê³  ë° ê¶Œì¥ì‚¬í•­
        if coverage['unique_coverage_pct'] < 95:
            LOGGER.warning(f"âš ï¸ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ê°€ ë‚®ìŠµë‹ˆë‹¤: {coverage['unique_coverage_pct']:.1f}%")
        
        if coverage['total_episodes'] > args.num_episodes:
            LOGGER.warning(f"âš ï¸ ê³„íšëœ ì—í”¼ì†Œë“œ ìˆ˜({coverage['total_episodes']})ê°€ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜({args.num_episodes})ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤.")
            LOGGER.info("ğŸ’¡ ê¶Œì¥ì‚¬í•­: --num_episodes ê°’ì„ ëŠ˜ë ¤ì„œ ì „ì²´ ë°ì´í„°ë¥¼ í™œìš©í•˜ì„¸ìš”.")
        
        # ì—í”¼ì†Œë“œ íƒ€ì… ë¶„í¬ í‘œì‹œ
        if 'episode_types' in coverage:
            LOGGER.info("ğŸ“Š ì—í”¼ì†Œë“œ íƒ€ì… ë¶„í¬:")
            for ep_type, count in coverage['episode_types'].items():
                percentage = (count / coverage['total_episodes']) * 100
                LOGGER.info(f"   â””â”€ {ep_type}: {count}ê°œ ({percentage:.1f}%)")
        
        LOGGER.info("âœ… í™˜ê²½ ê²€ì¦ ì™„ë£Œ")
        return True
        
    else:
        LOGGER.info("âœ… ê¸°ë³¸ ìˆœì°¨ì  í™˜ê²½ ì‚¬ìš©")
        return True

class StepLossTracker:
    """ìŠ¤í…ë³„ loss ì¶”ì ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, window_size=100):
        self.window_size = window_size
        self.actor_losses = []
        self.critic_losses = []
        self.alpha_losses = []
        self.alpha_values = []
        self.entropy_values = []
        
    def add_stats(self, stats):
        """í†µê³„ ì¶”ê°€"""
        if stats is not None:
            self.actor_losses.append(stats.get('actor_loss', 0.0))
            self.critic_losses.append(stats.get('critic_loss', 0.0))
            self.alpha_losses.append(stats.get('alpha_loss', 0.0))
            self.alpha_values.append(stats.get('alpha', 0.0))
            self.entropy_values.append(stats.get('entropy', 0.0))
            
            # ìœˆë„ìš° í¬ê¸° ìœ ì§€
            if len(self.actor_losses) > self.window_size:
                self.actor_losses.pop(0)
                self.critic_losses.pop(0)
                self.alpha_losses.pop(0)
                self.alpha_values.pop(0)
                self.entropy_values.pop(0)
    
    def get_averages(self):
        """í‰ê· ê°’ ë°˜í™˜"""
        if not self.actor_losses:
            return {
                'avg_actor_loss': 0.0,
                'avg_critic_loss': 0.0,
                'avg_alpha_loss': 0.0,
                'avg_alpha': 0.0,
                'avg_entropy': 0.0,
                'num_samples': 0
            }
        
        return {
            'avg_actor_loss': np.mean(self.actor_losses),
            'avg_critic_loss': np.mean(self.critic_losses),
            'avg_alpha_loss': np.mean(self.alpha_losses),
            'avg_alpha': np.mean(self.alpha_values),
            'avg_entropy': np.mean(self.entropy_values),
            'num_samples': len(self.actor_losses)
        }
    
    def check_convergence(self, episode, min_episodes=100):
        """ì–¼ë¦¬ìŠ¤íƒ‘ ìˆ˜ë ´ ì¡°ê±´ ì²´í¬"""
        if episode < min_episodes:
            return False, "ìµœì†Œ ì—í”¼ì†Œë“œ ë¯¸ë‹¬ì„±"
        
        if len(self.actor_losses) < 20:
            return False, "ë°ì´í„° ë¶€ì¡±"
        
        # ìµœê·¼ 20ê°œ ìƒ˜í”Œ ê¸°ì¤€
        recent_alpha = np.mean(self.alpha_values[-20:]) if self.alpha_values else 1.0
        recent_entropy = np.mean(self.entropy_values[-20:]) if self.entropy_values else 1.0
        recent_actor_loss = np.mean(np.abs(self.actor_losses[-20:]))
        
        # ìˆ˜ë ´ ì¡°ê±´ ì²´í¬
        alpha_converged = recent_alpha < 0.001
        entropy_converged = 0.06 < recent_entropy < 0.08
        loss_converged = recent_actor_loss < 0.001
        
        all_converged = alpha_converged and entropy_converged and loss_converged
        
        if all_converged:
            reason = f"ìˆ˜ë ´ ê°ì§€ - Alpha:{recent_alpha:.6f}, Entropy:{recent_entropy:.3f}, Loss:{recent_actor_loss:.6f}"
            return True, reason
        
        return False, "ìˆ˜ë ´ ì¡°ê±´ ë¯¸ë‹¬ì„±"

def log_step_progress(episode, step, total_episodes, max_episode_steps, 
                     loss_tracker, timer, args, agent):
    """ìŠ¤í…ë³„ ì§„í–‰ìƒí™© ë¡œê¹…"""
    
    # ì§„í–‰ë¥  ê³„ì‚°
    episode_progress = (step / max_episode_steps) * 100
    total_progress = ((episode - 1) * 100 + episode_progress) / total_episodes
    
    # í‰ê·  loss ê°€ì ¸ì˜¤ê¸°
    averages = loss_tracker.get_averages()
    
    # ì‹œê°„ ì •ë³´
    elapsed_time = timer.get_training_time()
    eta = timer.get_eta(episode, total_episodes)
    
    LOGGER.info("=" * 50)
    LOGGER.info(f"âœ… STEP PROGRESS | Episode {episode}/{total_episodes} | Step {step}/{max_episode_steps}")
    LOGGER.info("=" * 50)
    LOGGER.info(f"â° ì§„í–‰ë¥ :")
    LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ì§„í–‰ë¥ : {episode_progress:.1f}%")
    LOGGER.info(f"   â””â”€ ì „ì²´ ì§„í–‰ë¥ : {total_progress:.1f}%")
    LOGGER.info(f"   â””â”€ ì™„ë£Œëœ ì—í”¼ì†Œë“œ: {episode-1}/{total_episodes}")
    
    LOGGER.info(f"â° ì‹œê°„ ì •ë³´:")
    LOGGER.info(f"   â””â”€ ê²½ê³¼ ì‹œê°„: {timer.format_time(elapsed_time)}")
    LOGGER.info(f"   â””â”€ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {timer.format_time(eta)}")
    
    LOGGER.info(f"âœ… í‰ê·  Loss (ìµœê·¼ {averages['num_samples']}ìŠ¤í…):")
    LOGGER.info(f"   â””â”€ Actor Loss: {averages['avg_actor_loss']:.6f}")
    LOGGER.info(f"   â””â”€ Critic Loss: {averages['avg_critic_loss']:.6f}")
    LOGGER.info(f"   â””â”€ Alpha Loss: {averages['avg_alpha_loss']:.6f}")
    LOGGER.info(f"   â””â”€ Alpha: {averages['avg_alpha']:.6f}")
    LOGGER.info(f"   â””â”€ Entropy: {averages['avg_entropy']:.6f}")
    
    # ë””ë²„ê¹… ì •ë³´
    LOGGER.info(f"ğŸ” ë””ë²„ê¹… ì •ë³´:")
    LOGGER.info(f"   â””â”€ ë²„í¼ í¬ê¸°: {len(agent.replay_buffer):,}")
    LOGGER.info(f"   â””â”€ ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    LOGGER.info(f"   â””â”€ ì—…ë°ì´íŠ¸ ì¹´ìš´í„°: {agent.update_counter}")
    LOGGER.info(f"   â””â”€ Loss ìƒ˜í”Œ ìˆ˜: {averages['num_samples']}")
    
    LOGGER.info("=" * 50)

def train_agent_sac(agent, train_env, args, timer):
    """í‘œì¤€ SAC ì•Œê³ ë¦¬ì¦˜ í•™ìŠµ"""
    LOGGER.info("í‘œì¤€ SAC í•™ìŠµ ì‹œì‘...")
    global episode_actions_history
    
    episode_rewards = []
    portfolio_values = []
    shares_history = []
    
    # ìŠ¤í…ë³„ loss ì¶”ì ê¸° ì´ˆê¸°í™”
    loss_tracker = StepLossTracker(window_size=args.step_log_interval)
    global_step_count = 0
    
    for episode in range(args.num_episodes):
        episode_actions = []
        timer.start_episode()
        
        # í‘œì¤€ í™˜ê²½ ë¦¬ì…‹ (ëœë¤ ì‹œì‘ì )
        state = train_env.reset()
        
        episode_reward = 0
        steps = 0
        done = False
        
        LOGGER.info(f"âœ… Episode {episode+1} ì‹œì‘")
        LOGGER.info(f"   â””â”€ í˜„ì¬ ìŠ¤í…: {train_env.current_step}")
        LOGGER.info(f"   â””â”€ ìµœëŒ€ ìŠ¤í…: {args.max_steps}")
            
        # ë‹¨ì¼ ì—í”¼ì†Œë“œ ì§„í–‰ ë£¨í”„
        while not done and steps < args.max_steps:
            # í–‰ë™ ì„ íƒ
            action = agent.select_action(state, evaluate=False)
            episode_actions.append(action)
            
            # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            next_state, reward, done, info = train_env.step(action)
            
            # ê²½í—˜ì„ ë¦¬í”Œë ˆì´ ë²„í¼ì— ì¶”ê°€
            agent.add_experience(state, action, reward, next_state, done)
                     
            # ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            if len(agent.replay_buffer) > args.batch_size:
                stats = agent.update_parameters(args.batch_size)
                loss_tracker.add_stats(stats)
                
                # ê¸´ê¸‰ ì¤‘ë‹¨ ì²´í¬ (Alpha ì™„ì „ ì†Œë©¸)
                if stats.get('alpha', 1.0) < 1e-6:
                    LOGGER.warning("ğŸš¨ Alpha ì™„ì „ ì†Œë©¸, ê¸´ê¸‰ ì¤‘ë‹¨!")
                    done = True
                    break
            else:
                # ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠëŠ” ê²½ìš° None ì¶”ê°€
                loss_tracker.add_stats(None)
            
            episode_reward += reward
            steps += 1
            global_step_count += 1
            state = next_state
            
            # ìŠ¤í…ë³„ ì§„í–‰ìƒí™© ë¡œê¹…
            if args.step_log_interval > 0 and steps % args.step_log_interval == 0:
                log_step_progress(
                    episode + 1, steps, args.num_episodes, args.max_steps, 
                    loss_tracker, timer, args, agent
                )
                
                # ì–¼ë¦¬ìŠ¤íƒ‘ ì²´í¬ 
                should_stop, reason = loss_tracker.check_convergence(episode + 1)
                if should_stop:
                    LOGGER.info(f"ğŸ›‘ ì–¼ë¦¬ìŠ¤íƒ‘ ë°œë™: {reason}")
                    done = True
                    break
        
        # ì—í”¼ì†Œë“œ ì™„ë£Œ í›„ í–‰ë™ ê¸°ë¡ ì¶”ê°€
        episode_actions_history.append(episode_actions)
        
        episode_time = timer.end_episode()
        episode_rewards.append(episode_reward)
        
        # ì ê·¹ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬
        if episode % 5 == 0:
            torch.cuda.empty_cache()
            if hasattr(torch.cuda, 'reset_peak_memory_stats'):
                torch.cuda.reset_peak_memory_stats()

        # ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ê°€ë²¼ìš´ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            
        # VRAM ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (50 ì—í”¼ì†Œë“œë§ˆë‹¤)
        if episode % 50 == 0:
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            LOGGER.info(f"   â””â”€ VRAM ì‚¬ìš©ëŸ‰: í• ë‹¹ {allocated:.2f}GB, ìºì‹œ {reserved:.2f}GB")
            
        portfolio_values.append(info.get('portfolio_value', args.initial_balance))
        shares_history.append(info.get('shares_held', 0))
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ìµœì¢… í‰ê·  loss ì¶œë ¥
        final_averages = loss_tracker.get_averages()
        LOGGER.info(f"âœ… Episode {episode+1} ì™„ë£Œ:")
        LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ë³´ìƒ: {episode_reward:.4f}")
        LOGGER.info(f"   â””â”€ ì‹¤í–‰ ìŠ¤í…: {steps}/{args.max_steps}")
        LOGGER.info(f"   â””â”€ í¬íŠ¸í´ë¦¬ì˜¤: ${info.get('portfolio_value', args.initial_balance):.2f}")
        
        # ë¦¬ìŠ¤í¬ ì •ë³´ ì¶œë ¥
        if hasattr(train_env, 'get_risk_metrics'):
            risk_metrics = train_env.get_risk_metrics()
            if 'max_drawdown_pct' in risk_metrics:
                LOGGER.info(f"   â””â”€ ìµœëŒ€ ë‚™í­: {risk_metrics['max_drawdown_pct']:.2f}%")
        
        LOGGER.info(f"   â””â”€ í‰ê·  Actor Loss: {final_averages['avg_actor_loss']:.6f}")
        LOGGER.info(f"   â””â”€ í‰ê·  Critic Loss: {final_averages['avg_critic_loss']:.6f}")
        LOGGER.info(f"   â””â”€ í‰ê·  Alpha: {final_averages['avg_alpha']:.6f}")
        LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ì‹œê°„: {episode_time:.2f}ì´ˆ")
        
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì‹œ ì–¼ë¦¬ìŠ¤íƒ‘ ì²´í¬
        should_stop, reason = loss_tracker.check_convergence(episode + 1)
        if should_stop:
            LOGGER.info(f"ğŸ›‘ ì—í”¼ì†Œë“œ ì™„ë£Œ í›„ ì–¼ë¦¬ìŠ¤íƒ‘ ë°œë™: {reason}")
            break
            
        # ë³´ìƒ ë¶„ì„ (10 ì—í”¼ì†Œë“œë§ˆë‹¤)
        if episode % 10 == 0 and len(episode_rewards) >= 10:
            recent_rewards = episode_rewards[-10:]
            max_reward = max(recent_rewards)
            min_reward = min(recent_rewards)
            avg_reward = sum(recent_rewards) / len(recent_rewards)
            
            LOGGER.info(f"âœ… ìµœê·¼ 10 ì—í”¼ì†Œë“œ ë¶„ì„:")
            LOGGER.info(f"   â””â”€ ë³´ìƒ ë²”ìœ„: [{min_reward:.4f} ~ {max_reward:.4f}]")
            LOGGER.info(f"   â””â”€ í‰ê·  ë³´ìƒ: {avg_reward:.4f}")
            
            # ê·¹ë‹¨ì  ë³´ìƒ ê°ì§€
            threshold = 100
            if abs(max_reward) > threshold or abs(min_reward) > threshold:
                LOGGER.warning(f"âš ï¸ ê·¹ë‹¨ì  ë³´ìƒ ê°ì§€! Episode {episode+1}")
                LOGGER.warning(f"   â””â”€ ìµœëŒ€: {max_reward:.4f}, ìµœì†Œ: {min_reward:.4f}")
                
        # ì£¼ê¸°ì  í‰ê°€ ë° ë¡œê¹…
        if (episode + 1) % args.log_interval == 0:
            log_training_progress(
                episode + 1, args, episode_rewards, portfolio_values, 
                info, agent, timer, shares_history
            )
            
            # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥
            if (episode + 1) % args.save_interval == 0:
                model_name = f"checkpoint_episode_{episode + 1}"
                saved_path = agent.save_model(prefix=model_name)
                LOGGER.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {saved_path}")
    
    LOGGER.info("=" * 50)
    LOGGER.info("ğŸ‰ í‘œì¤€ SAC í•™ìŠµ ì™„ë£Œ!")
    LOGGER.info("=" * 50)
    
    return episode_rewards, portfolio_values, shares_history

def analyze_recent_actions(episode_actions_history, num_recent=10):
    """ìµœê·¼ ì—í”¼ì†Œë“œë“¤ì˜ í–‰ë™ íŒ¨í„´ ê°„ë‹¨ ë¶„ì„"""
    if not episode_actions_history:
        return None
    
    # ìµœê·¼ Nê°œ ì—í”¼ì†Œë“œ ì„ íƒ
    recent_episodes = episode_actions_history[-num_recent:] if len(episode_actions_history) >= num_recent else episode_actions_history
    
    # ëª¨ë“  í–‰ë™ í•©ì¹˜ê¸°
    all_actions = []
    for ep_actions in recent_episodes:
        all_actions.extend(ep_actions)
    
    if not all_actions:
        return None
    
    # í–‰ë™ ë¶„ë¥˜
    buy_threshold = 0.1
    sell_threshold = -0.1
    
    buy_count = sum(1 for a in all_actions if a > buy_threshold)
    sell_count = sum(1 for a in all_actions if a < sell_threshold)
    hold_count = len(all_actions) - buy_count - sell_count
    
    total = len(all_actions)
    buy_ratio = buy_count / total * 100
    sell_ratio = sell_count / total * 100
    hold_ratio = hold_count / total * 100
    
    # í‰ê·  í–‰ë™ ê°•ë„
    avg_action = np.mean(all_actions)
    action_intensity = np.mean(np.abs(all_actions))
    
    return {
        'buy_ratio': buy_ratio,
        'sell_ratio': sell_ratio,
        'hold_ratio': hold_ratio,
        'avg_action': avg_action,
        'intensity': action_intensity,
        'episodes_analyzed': len(recent_episodes)
    }

def log_training_progress(episode, args, episode_rewards, portfolio_values, 
                         info, agent, timer, shares_history):
    """í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹…"""
    
    # í›ˆë ¨ ì„±ëŠ¥ ê³„ì‚°
    recent_rewards = episode_rewards[-args.log_interval:] if len(episode_rewards) >= args.log_interval else episode_rewards
    recent_portfolios = portfolio_values[-args.log_interval:] if len(portfolio_values) >= args.log_interval else portfolio_values
    recent_shares = shares_history[-args.log_interval:] if len(shares_history) >= args.log_interval else shares_history
    
    avg_reward = np.mean(recent_rewards)
    avg_portfolio = np.mean(recent_portfolios)
    total_return = (avg_portfolio - args.initial_balance) / args.initial_balance * 100
    
    # ì•ˆì „í•œ ì£¼ì‹ ì§€í‘œ ê³„ì‚°
    current_shares = info.get('shares_held', 0)  
    
    if recent_shares:
        avg_shares = np.mean(recent_shares)
        min_shares = np.min(recent_shares)
        max_shares = np.max(recent_shares)
        
        # í¬ì§€ì…˜ íƒ€ì… ê²°ì •
        if abs(current_shares) < 0.001:
            position_type = "í˜„ê¸ˆ í¬ì§€ì…˜"
        elif current_shares > avg_shares * 1.2:
            position_type = "í‰ê·  ëŒ€ë¹„ ë†’ìŒ"
        elif current_shares < avg_shares * 0.8:
            position_type = "í‰ê·  ëŒ€ë¹„ ë‚®ìŒ"
        else:
            position_type = "í‰ê·  ìˆ˜ì¤€"
    else:
        avg_shares = 0
        min_shares = 0
        max_shares = 0
        position_type = "ë°ì´í„° ë¶€ì¡±"
    
    # ì‹œê°„ ì •ë³´
    elapsed_time = timer.get_training_time()
    avg_episode_time = timer.get_avg_episode_time()
    eta = timer.get_eta(episode, args.num_episodes)
    progress = episode / args.num_episodes * 100
    current_balance = info.get('balance', 0)  

    # ì‹œê°„ ì •ë³´
    LOGGER.info(f"â±ì‹œê°„ ì •ë³´:")
    LOGGER.info(f"   â””â”€ ê²½ê³¼ ì‹œê°„: {timer.format_time(elapsed_time)}, í‰ê·  ì—í”¼ì†Œë“œ ì‹œê°„: {avg_episode_time:.2f}ì´ˆ, ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {timer.format_time(eta)}")
    LOGGER.info(f"í›ˆë ¨ ì„±ëŠ¥ (ìµœê·¼ {len(recent_rewards)}ê°œ ì—í”¼ì†Œë“œ):")
    
    # í•™ìŠµ í†µê³„
    if hasattr(agent, 'actor_losses') and len(agent.actor_losses) > 0:
        LOGGER.info(f"í•™ìŠµ í†µê³„:")
        LOGGER.info(f"   â””â”€ Actor Loss: {agent.actor_losses[-1]:.6f}")
        if hasattr(agent, 'critic_losses') and len(agent.critic_losses) > 0:
            LOGGER.info(f"   â””â”€ Critic Loss: {agent.critic_losses[-1]:.6f}")
        if hasattr(agent, 'alpha'):
            LOGGER.info(f"   â””â”€ Alpha: {agent.alpha.item():.6f}")
        if hasattr(agent, 'replay_buffer'):
            LOGGER.info(f"   â””â”€ ë²„í¼ í¬ê¸°: {len(agent.replay_buffer):,}")
            
    LOGGER.info(f"   â””â”€ ìµœê·¼ {len(recent_rewards)}ê°œ ì—í”¼ì†Œë“œ í‰ê·  ìˆ˜ìµë¥ : {total_return:.2f}%")
    LOGGER.info(f"   â””â”€ ìµœê·¼ {len(recent_rewards)}ê°œ ì—í”¼ì†Œë“œ í‰ê·  ë³´ìƒ: {avg_reward:.4f}")
    LOGGER.info(f"   â””â”€ í˜„ì¬ í˜„ê¸ˆ: ${current_balance:,.2f}")
    
    # í†µí•©ëœ ì£¼ì‹ ì •ë³´ 
    LOGGER.info(f"   â””â”€ í˜„ì¬ ë³´ìœ  ì£¼ì‹: {current_shares:.4f}")
    if recent_shares:
        LOGGER.info(f"   â””â”€ ìµœê·¼ {len(recent_shares)}íšŒ í‰ê· : {avg_shares:.4f} (ë²”ìœ„: {min_shares:.2f}~{max_shares:.2f})")
        LOGGER.info(f"   â””â”€ í¬ì§€ì…˜ ìƒíƒœ: {position_type}")
    else:
        LOGGER.info(f"   â””â”€ ë³´ìœ ëŸ‰ íˆìŠ¤í† ë¦¬: ë°ì´í„° ë¶€ì¡±")
    
    # í–‰ë™ íŒ¨í„´ ì •ë³´ 
    global episode_actions_history
    action_pattern = analyze_recent_actions(episode_actions_history, args.log_interval)
    
    if action_pattern:
        LOGGER.info(f"âœ… í–‰ë™ íŒ¨í„´ (ìµœê·¼ {action_pattern['episodes_analyzed']}ê°œ ì—í”¼ì†Œë“œ):")
        LOGGER.info(f"   â””â”€ ë§¤ìˆ˜ {action_pattern['buy_ratio']:.1f}% | ë§¤ë„ {action_pattern['sell_ratio']:.1f}% | í™€ë“œ {action_pattern['hold_ratio']:.1f}%")
        LOGGER.info(f"   â””â”€ í‰ê·  í–‰ë™ê°’: {action_pattern['avg_action']:+.3f} | í–‰ë™ ê°•ë„: {action_pattern['intensity']:.3f}")
        
        # ì§€ë°°ì  í–‰ë™ í‘œì‹œ
        if action_pattern['buy_ratio'] > 40:
            LOGGER.info(f"   â””â”€âœ… ì„±í–¥: ì ê·¹ì  ë§¤ìˆ˜ ì„±í–¥")
        elif action_pattern['sell_ratio'] > 40:
            LOGGER.info(f"   â””â”€âœ… ì„±í–¥: ì ê·¹ì  ë§¤ë„ ì„±í–¥")
        elif action_pattern['hold_ratio'] > 60:
            LOGGER.info(f"   â””â”€âœ… ì„±í–¥: ë³´ìˆ˜ì  í™€ë“œ ì„±í–¥")
        else:
            LOGGER.info(f"   â””â”€âœ… ì„±í–¥: ê· í˜•ì  ê±°ë˜ ì„±í–¥")

    LOGGER.info("=" * 80)

def main():
    """ë©”ì¸ í•¨ìˆ˜ (í†µí•© ê°„ì†Œí™” ë²„ì „)"""
    timer = TrainingTimer()
    timer.start_training()
    
    print('=' * 50)
    LOGGER.info('SAC ëª¨ë¸ í•™ìŠµ ì‹œì‘ (í†µí•© ê°„ì†Œí™” ë²„ì „)')
    
    # ì¸ì íŒŒì‹±
    args = parse_args()
    
    # ì‹¬ë³¼ ëª©ë¡ ì„¤ì •
    symbols = args.symbols if args.symbols else TARGET_SYMBOLS
    
    LOGGER.info(f"í•™ìŠµ ëŒ€ìƒ ì‹¬ë³¼: {symbols}")
    LOGGER.info(f"í•™ìŠµ ì„¤ì •:")
    LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ìˆ˜: {args.num_episodes:,}")
    LOGGER.info(f"   â””â”€ ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    LOGGER.info(f"   â””â”€ ìœˆë„ìš° í¬ê¸°: {args.window_size}")
    LOGGER.info(f"   â””â”€ ì´ˆê¸° ìë³¸ê¸ˆ: ${args.initial_balance:,.2f}")
    LOGGER.info(f"   â””â”€ ìµœëŒ€ ì—í”¼ì†Œë“œ ìŠ¤í…: {args.max_steps}")
    
    # SAC í•™ìŠµ ì„¤ì • ë¡œê·¸
    LOGGER.info(f"âœ… SAC í•™ìŠµ ì„¤ì •:")
    LOGGER.info(f"   â””â”€ ëœë¤ ì‹œì‘ì : í™œì„±í™”")
    LOGGER.info(f"   â””â”€ ë¦¬í”Œë ˆì´ ë²„í¼: ëœë¤ ìƒ˜í”Œë§")
    LOGGER.info(f"   â””â”€ ë¡œê·¸ ë ˆë²¨: {args.log_level}")
    
    # ë°ì´í„° ìˆ˜ì§‘
    LOGGER.info("ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    collector = DataCollector(symbols=symbols)
    
    if args.collect_data:
        LOGGER.info("ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        data = collector.load_and_save()
    else:
        LOGGER.info("ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì¤‘...")
        data = collector.load_all_data()
        
        if not data:
            LOGGER.warning("ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ì–´ ìƒˆë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
            data = collector.load_and_save()
    
    if not data:
        LOGGER.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return
    
    LOGGER.info(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(data)}ê°œ ì‹¬ë³¼")
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    LOGGER.info("ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    processor = DataProcessor(window_size=args.window_size)
    results = processor.process_all_symbols(data)
    
    if not results:
        LOGGER.error("ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨")
        return
    
    LOGGER.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ì‹¬ë³¼")
    
    # ê°œì„ ëœ í™˜ê²½ ìƒì„± (í›ˆë ¨ìš©ë§Œ)
    train_env = create_training_environment(results, symbols, args)
    
    if train_env is None:
        LOGGER.error("í›ˆë ¨ í™˜ê²½ ìƒì„± ì‹¤íŒ¨")
        return
    
    # í™˜ê²½ ê²€ì¦ (ë°ì´í„° ì†ì‹¤ ìœ„í—˜ ì‚¬ì „ ì ê²€)
    validation_success = validate_training_environment(train_env, args)
    if not validation_success:
        LOGGER.error("í™˜ê²½ ê²€ì¦ ì‹¤íŒ¨")
        return
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = create_agent(train_env, args)

    if agent is None:
        LOGGER.error("ì—ì´ì „íŠ¸ ìƒì„± ì‹¤íŒ¨")
        return

    # í•™ìŠµ ì‹¤í–‰
        LOGGER.info("=" * 60)
    LOGGER.info("í‘œì¤€ SAC í•™ìŠµ ì‹œì‘")
    LOGGER.info("=" * 60)
    
    episode_rewards, portfolio_values, shares_history = train_agent_sac(agent, train_env, args, timer)

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = agent.save_model(
        save_dir="models",
        prefix='',
        model_type=getattr(agent, 'model_type', 'mlp'),
        symbol=symbols[0] if len(symbols) == 1 else None,
        symbols=symbols if len(symbols) > 1 else None
    )
    
    # ìµœì¢… í™˜ê²½ ìƒíƒœ ì •ë³´ ìˆ˜ì§‘ (ì „ì²´ í•™ìŠµ ê¸°ê°„ í‰ê· )
    final_env_state = train_env.reset()  # í™˜ê²½ ë¦¬ì…‹í•´ì„œ ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
    final_info = train_env._get_info()  # ìµœì¢… í™˜ê²½ ì •ë³´

    LOGGER.info(f"ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}")
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    total_time = timer.get_training_time()
    final_portfolio = portfolio_values[-1] if portfolio_values else args.initial_balance
    final_return = (final_portfolio - args.initial_balance) / args.initial_balance * 100
    
    # ì „ì²´ ì—í”¼ì†Œë“œ í†µê³„ ê³„ì‚°
    if episode_rewards:
        total_episodes = len(episode_rewards)
        avg_total_reward = np.mean(episode_rewards)
        avg_total_portfolio = np.mean(portfolio_values) if portfolio_values else args.initial_balance
        total_avg_return = (avg_total_portfolio - args.initial_balance) / args.initial_balance * 100
        max_portfolio = np.max(portfolio_values) if portfolio_values else args.initial_balance
        min_portfolio = np.min(portfolio_values) if portfolio_values else args.initial_balance
        max_return = (max_portfolio - args.initial_balance) / args.initial_balance * 100
        min_return = (min_portfolio - args.initial_balance) / args.initial_balance * 100
    else:
        total_episodes = 0
        avg_total_reward = 0
        total_avg_return = 0
        max_return = 0
        min_return = 0
    
    LOGGER.info("=" * 80)
    LOGGER.info(f"ğŸ‰ í•™ìŠµ ì™„ë£Œ - ìµœì¢… ê²°ê³¼")
    LOGGER.info("=" * 80)
    LOGGER.info(f"â± í•™ìŠµ ì‹œê°„:")
    LOGGER.info(f"   â””â”€ ì´ í•™ìŠµ ì‹œê°„: {timer.format_time(total_time)}")
    LOGGER.info(f"   â””â”€ í‰ê·  ì—í”¼ì†Œë“œ ì‹œê°„: {timer.get_avg_episode_time():.2f}ì´ˆ")
    LOGGER.info(f"ğŸ“Š ì „ì²´ {total_episodes}ê°œ ì—í”¼ì†Œë“œ ì„±ê³¼:")
    LOGGER.info(f"   â””â”€ ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: ${final_portfolio:,.2f}")
    LOGGER.info(f"   â””â”€ ìµœì¢… ìˆ˜ìµë¥ : {final_return:+.2f}%")
    LOGGER.info(f"   â””â”€ ì „ì²´ ê¸°ê°„ í‰ê·  ìˆ˜ìµë¥ : {total_avg_return:+.2f}%")
    LOGGER.info(f"   â””â”€ ì „ì²´ ê¸°ê°„ í‰ê·  ë³´ìƒ: {avg_total_reward:.4f}")
    LOGGER.info(f"   â””â”€ ìµœê³  ìˆ˜ìµë¥ : {max_return:+.2f}%")
    LOGGER.info(f"   â””â”€ ìµœì € ìˆ˜ìµë¥ : {min_return:+.2f}%")
    
    # ì „ì²´ ì—í”¼ì†Œë“œì˜ í‰ê·  í•™ìŠµ í†µê³„
    if hasattr(agent, 'actor_losses') and len(agent.actor_losses) > 0:
        LOGGER.info(f"ğŸ§  ì „ì²´ í•™ìŠµ í†µê³„ (í‰ê· ):")
        LOGGER.info(f"   â””â”€ ì „ì²´ ê¸°ê°„ í‰ê·  Actor Loss: {np.mean(agent.actor_losses):.6f}")
        if hasattr(agent, 'critic_losses') and len(agent.critic_losses) > 0:
            LOGGER.info(f"   â””â”€ ì „ì²´ ê¸°ê°„ í‰ê·  Critic Loss: {np.mean(agent.critic_losses):.6f}")
        if hasattr(agent, 'alpha'):
            LOGGER.info(f"   â””â”€ ìµœì¢… Alpha: {agent.alpha.item():.6f}")
        if hasattr(agent, 'replay_buffer'):
            LOGGER.info(f"   â””â”€ ìµœì¢… ë²„í¼ í¬ê¸°: {len(agent.replay_buffer):,}")
    
    # ì „ì²´ ê¸°ê°„ í–‰ë™ íŒ¨í„´ ë¶„ì„
    global episode_actions_history
    if episode_actions_history:
        total_action_pattern = analyze_recent_actions(episode_actions_history, len(episode_actions_history))
        if total_action_pattern:
            LOGGER.info(f"âš¡ ì „ì²´ {total_action_pattern['episodes_analyzed']}ê°œ ì—í”¼ì†Œë“œ í–‰ë™ íŒ¨í„´:")
            LOGGER.info(f"   â””â”€ ë§¤ìˆ˜ {total_action_pattern['buy_ratio']:.1f}% | ë§¤ë„ {total_action_pattern['sell_ratio']:.1f}% | í™€ë“œ {total_action_pattern['hold_ratio']:.1f}%")
            LOGGER.info(f"   â””â”€ ì „ì²´ ê¸°ê°„ í‰ê·  í–‰ë™ê°’: {total_action_pattern['avg_action']:+.3f}")
            LOGGER.info(f"   â””â”€ ì „ì²´ ê¸°ê°„ í–‰ë™ ê°•ë„: {total_action_pattern['intensity']:.3f}")
            
            # ì „ì²´ì ì¸ ì„±í–¥ ë¶„ì„
            if total_action_pattern['buy_ratio'] > 40:
                LOGGER.info(f"   â””â”€ ğŸ¯ ì „ì²´ ì„±í–¥: ì ê·¹ì  ë§¤ìˆ˜ ì „ëµ")
            elif total_action_pattern['sell_ratio'] > 40:
                LOGGER.info(f"   â””â”€ ğŸ¯ ì „ì²´ ì„±í–¥: ì ê·¹ì  ë§¤ë„ ì „ëµ")
            elif total_action_pattern['hold_ratio'] > 60:
                LOGGER.info(f"   â””â”€ ğŸ¯ ì „ì²´ ì„±í–¥: ë³´ìˆ˜ì  í™€ë“œ ì „ëµ")
            else:
                LOGGER.info(f"   â””â”€ ğŸ¯ ì „ì²´ ì„±í–¥: ê· í˜•ì  ê±°ë˜ ì „ëµ")
    
    # ì „ì²´ ê¸°ê°„ í‰ê·  í¬ì§€ì…˜ ì •ë³´
    if shares_history and len(shares_history) > 1:
        avg_shares = np.mean(shares_history)
        max_shares = np.max(shares_history)
        min_shares = np.min(shares_history)
        
        LOGGER.info(f"ğŸ’° ì „ì²´ ê¸°ê°„ í¬ì§€ì…˜ í†µê³„:")
        LOGGER.info(f"   â””â”€ ì „ì²´ ê¸°ê°„ í‰ê·  ì£¼ì‹ ë³´ìœ ëŸ‰: {avg_shares:.4f}")
        LOGGER.info(f"   â””â”€ ìµœëŒ€ ì£¼ì‹ ë³´ìœ ëŸ‰: {max_shares:.4f}")
        LOGGER.info(f"   â””â”€ ìµœì†Œ ì£¼ì‹ ë³´ìœ ëŸ‰: {min_shares:.4f}")
        LOGGER.info(f"   â””â”€ ìµœì¢… í˜„ê¸ˆ: ${final_info.get('balance', 0):,.2f}")
        LOGGER.info(f"   â””â”€ ìµœì¢… ì£¼ì‹ ë³´ìœ ëŸ‰: {final_info.get('shares_held', 0):.4f}")
        LOGGER.info(f"   â””â”€ ìµœì¢… ì£¼ì‹ ë¹„ìœ¨: {final_info.get('stock_ratio', 0):.1%}")
        LOGGER.info(f"   â””â”€ ìµœì¢… í˜„ê¸ˆ ë¹„ìœ¨: {final_info.get('cash_ratio', 0):.1%}")
    
    LOGGER.info(f"ğŸ’¾ ëª¨ë¸ ì •ë³´:")
    LOGGER.info(f"   â””â”€ ì €ì¥ ê²½ë¡œ: {final_model_path}")
    LOGGER.info(f"   â””â”€ í•™ìŠµëœ ì—í”¼ì†Œë“œ: {len(episode_rewards):,}ê°œ")
    
    # ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ (Buy & Hold)
    if len(episode_rewards) > 0:
        LOGGER.info("")
        LOGGER.info(f"âœ… ì„±ê³¼ ë¹„êµ:")
        LOGGER.info(f"    â””â”€ SAC ëª¨ë¸ ìˆ˜ìµë¥ : {final_return:.2f}%")
        
        # Buy & Hold ìˆ˜ìµë¥  ì¶”ì • (ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ í¬íŠ¸í´ë¦¬ì˜¤ ê°’ ê¸°ì¤€)
        if len(portfolio_values) >= 2:
            buy_hold_return = ((portfolio_values[-1] / portfolio_values[0]) - 1) * 100
            LOGGER.info(f"   â””â”€ Buy & Hold ì¶”ì •: {buy_hold_return:.2f}%")
            
            if final_return > buy_hold_return:
                outperformance = final_return - buy_hold_return
                LOGGER.info(f"   â””â”€ âœ… SAC ëª¨ë¸ì´ {outperformance:.2f}%p ë” ìš°ìˆ˜")
            else:
                underperformance = buy_hold_return - final_return
                LOGGER.info(f"   â””â”€ âŒ Buy & Holdê°€ {underperformance:.2f}%p ë” ìš°ìˆ˜")

if __name__ == "__main__":
    main()