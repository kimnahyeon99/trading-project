"""
SAC ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ì•ˆì •í™” ê¸°ëŠ¥ ì§€ì›) - ë‚™í­ ë¡œê·¸ ìˆ˜ì • ë²„ì „
- ê²€ì¦ ì—†ì´ í›ˆë ¨ì—ë§Œ ì§‘ì¤‘
- í•™ìŠµ ì‹œê°„ ì¸¡ì • ë° ë¡œê¹…
- ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ë¡œê·¸ ì¶œë ¥
- ìŠ¤í…ë³„ loss ë° ì§„í–‰ë¥  ë¡œê¹… ì¶”ê°€
- ì•ˆì •í™” ê¸°ëŠ¥ í†µí•©
"""
import sys
import os
import time
from datetime import datetime, timedelta

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

import argparse
import torch
import numpy as np
from pathlib import Path
from src.environment.sequential_episode_manager import create_sequential_training_setup

from src.config.ea_teb_config import (
    DEVICE,
    HIDDEN_DIM,
    BATCH_SIZE,
    NUM_EPISODES,
    window_size,
    EVALUATE_INTERVAL,
    SAVE_MODEL_INTERVAL,
    MAX_STEPS_PER_EPISODE,
    TARGET_SYMBOLS,
    LOGGER,
    INITIAL_BALANCE,
    WARMUP_STEPS,
    overlap_ratio
)
from src.data_collection.data_collector import DataCollector
from src.preprocessing.data_processor import DataProcessor
from src.environment.trading_env import TradingEnvironment, MultiAssetTradingEnvironment
from src.models.sac_agent import SACAgent  # í†µí•©ëœ SACAgent ì‚¬ìš©
from src.utils.utils import create_directory, get_timestamp

episode_actions_history = []

class TrainingTimer:
    """í•™ìŠµ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.start_time = None
        self.episode_start_time = None
        self.episode_times = []
        
    def start_training(self):
        """ì „ì²´ í•™ìŠµ ì‹œì‘"""
        self.start_time = time.time()
        LOGGER.info(f"ğŸš€ í•™ìŠµ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    def start_episode(self):
        """ì—í”¼ì†Œë“œ ì‹œì‘"""
        self.episode_start_time = time.time()
        
    def end_episode(self):
        """ì—í”¼ì†Œë“œ ì¢…ë£Œ"""
        if self.episode_start_time:
            episode_time = time.time() - self.episode_start_time
            self.episode_times.append(episode_time)
            return episode_time
        return 0
        
    def get_training_time(self):
        """ì „ì²´ í•™ìŠµ ì‹œê°„ ë°˜í™˜"""
        if self.start_time:
            return time.time() - self.start_time
        return 0
        
    def get_avg_episode_time(self):
        """í‰ê·  ì—í”¼ì†Œë“œ ì‹œê°„ ë°˜í™˜"""
        if self.episode_times:
            return np.mean(self.episode_times)
        return 0
        
    def get_eta(self, current_episode, total_episodes):
        """ë‚¨ì€ ì‹œê°„ ì¶”ì •"""
        if len(self.episode_times) > 0:
            avg_time = self.get_avg_episode_time()
            remaining_episodes = total_episodes - current_episode
            return remaining_episodes * avg_time
        return 0
        
    def format_time(self, seconds):
        """ì‹œê°„ì„ ë³´ê¸° ì¢‹ê²Œ í¬ë§·"""
        return str(timedelta(seconds=int(seconds)))

def parse_args():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='SAC ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ì•ˆì •í™” ê¸°ëŠ¥ ì§€ì›)')
    
    # ë°ì´í„° ê´€ë ¨ ì¸ì
    parser.add_argument('--symbols', nargs='+', default=None, help='í•™ìŠµì— ì‚¬ìš©í•  ì£¼ì‹ ì‹¬ë³¼ ëª©ë¡')
    parser.add_argument('--collect_data', action='store_true', help='ë°ì´í„° ìˆ˜ì§‘ ì—¬ë¶€')
    
    # í™˜ê²½ ê´€ë ¨ ì¸ì
    parser.add_argument('--window_size', type=int, default=window_size, help='ê´€ì¸¡ ìœˆë„ìš° í¬ê¸°')
    parser.add_argument('--initial_balance', type=float, default=INITIAL_BALANCE, help='ì´ˆê¸° ìë³¸ê¸ˆ')
    parser.add_argument('--multi_asset', action='store_true', help='ë‹¤ì¤‘ ìì‚° í™˜ê²½ ì‚¬ìš© ì—¬ë¶€')
    
    # ëª¨ë¸ ê´€ë ¨ ì¸ì(ê¸°ë³¸ MLP)
    parser.add_argument('--hidden_dim', type=int, default=HIDDEN_DIM, help='ì€ë‹‰ì¸µ ì°¨ì›')
    # CNN
    parser.add_argument('--use_cnn', action='store_true', help='CNN ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€')
    # LSTM
    parser.add_argument('--use_lstm', action='store_true', help='LSTM ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€')
    # ë¡œë“œ ê²½ë¡œ
    parser.add_argument('--load_model', type=str, default=None, help='ë¡œë“œí•  ëª¨ë¸ ê²½ë¡œ')
    
    # í•™ìŠµ ê´€ë ¨ ì¸ì
    parser.add_argument('--batch_size', type=int, default=BATCH_SIZE, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_episodes', type=int, default=NUM_EPISODES, help='í•™ìŠµí•  ì´ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--log_interval', type=int, default=EVALUATE_INTERVAL, help='ë¡œê·¸ ì¶œë ¥ ê°„ê²©')
    parser.add_argument('--save_interval', type=int, default=SAVE_MODEL_INTERVAL, help='ëª¨ë¸ ì €ì¥ ê°„ê²©')
    parser.add_argument('--max_steps', type=int, default=MAX_STEPS_PER_EPISODE, help='ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜')
    parser.add_argument("--buffer_type", type=str, choices=["random", "sequential"], default="sequential", help="ë²„í¼ íƒ€ì… ì„ íƒ(random or sequential)")
    
    # ìˆœì°¨ì  í•™ìŠµ ê´€ë ¨ ìƒˆ ì˜µì…˜ë“¤ ì¶”ê°€
    parser.add_argument('--adaptive_episode_length', action='store_true', default=True, help='ì ì‘í˜• ì—í”¼ì†Œë“œ ê¸¸ì´ ì‚¬ìš© (ê¸°ë³¸ê°’: True)')
    parser.add_argument('--min_episode_steps', type=int, default=100, help='ì—í”¼ì†Œë“œ ìµœì†Œ ê¸¸ì´')
    parser.add_argument('--episode_overlap_ratio', type=float, default=overlap_ratio, help='ì—í”¼ì†Œë“œ ê°„ ê²¹ì¹¨ ë¹„ìœ¨')
    parser.add_argument('--force_full_coverage', action='store_true', help='ì „ì²´ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ ê°•ì œ (ë°ì´í„° ì†ì‹¤ ìµœì†Œí™”)')
    
    # ìŠ¤í…ë³„ ë¡œê¹… ê´€ë ¨ ì¸ì ì¶”ê°€
    parser.add_argument('--step_log_interval', type=int, default=100, help='ìŠ¤í…ë³„ ë¡œê·¸ ì¶œë ¥ ê°„ê²©')
    
    # ===== ì•ˆì •í™” ê¸°ëŠ¥ ê´€ë ¨ ì¸ì ì¶”ê°€ =====
    parser.add_argument('--use_stabilization', action='store_true', help='ì•ˆì •í™” ê¸°ëŠ¥ ì‚¬ìš© ì—¬ë¶€')
    parser.add_argument('--max_grad_norm', type=float, default=1.0, help='ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì„ê³„ê°’')
    parser.add_argument('--reward_scale', type=float, default=1.0, help='ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜')
    parser.add_argument('--update_frequency', type=int, default=1, help='ëª¨ë¸ ì—…ë°ì´íŠ¸ ë¹ˆë„')
    parser.add_argument('--warmup_steps', type=int, default=WARMUP_STEPS, help='ì›Œë°ì—… ìŠ¤í… ìˆ˜')
    parser.add_argument('--use_layer_norm', action='store_true', help='ë ˆì´ì–´ ì •ê·œí™” ì‚¬ìš©')
    parser.add_argument('--use_reward_normalization', action='store_true', help='ë³´ìƒ ì •ê·œí™” ì‚¬ìš©')
    parser.add_argument('--use_huber_loss', action='store_true', help='Huber Loss ì‚¬ìš©')
    parser.add_argument('--target_clipping', action='store_true', help='íƒ€ê²Ÿ ê°’ í´ë¦¬í•‘ ì‚¬ìš©')
    parser.add_argument('--alpha_clipping', action='store_true', help='Alpha í´ë¦¬í•‘ ì‚¬ìš©')
    parser.add_argument('--emergency_lr_reduction', action='store_true', help='ì‘ê¸‰ í•™ìŠµë¥  ê°ì†Œ ì‚¬ìš©')
    parser.add_argument('--loss_anomaly_detection', action='store_true', help='Loss ì´ìƒê°’ ê°ì§€ ì‚¬ìš©')
    
    # ì•ˆì •í™” í”„ë¦¬ì…‹
    parser.add_argument('--stabilization_preset', type=str, choices=['conservative', 'moderate', 'aggressive'], 
                       help='ì•ˆì •í™” í”„ë¦¬ì…‹ (conservative/moderate/aggressive)')
    
    return parser.parse_args()

def get_stabilization_preset(preset_name):
    """ì•ˆì •í™” í”„ë¦¬ì…‹ ì„¤ì • ë°˜í™˜"""
    presets = {
        'conservative': {
            'use_stabilization': True,
            'max_grad_norm': 0.5,
            'reward_scale': 0.1,
            'update_frequency': 2,
            'warmup_steps': 2000,
            'use_layer_norm': True,
            'use_reward_normalization': True,
            'use_huber_loss': True,
            'target_clipping': True,
            'alpha_clipping': True,
            'emergency_lr_reduction': True,
            'loss_anomaly_detection': True
        },
        'moderate': {
            'use_stabilization': True,
            'max_grad_norm': 1.0,
            'reward_scale': 1.0,
            'update_frequency': 1,
            'warmup_steps': 1000,
            'use_layer_norm': False,
            'use_reward_normalization': True,
            'use_huber_loss': True,
            'target_clipping': True,
            'alpha_clipping': True,
            'emergency_lr_reduction': False,
            'loss_anomaly_detection': True
        },
        'aggressive': {
            'use_stabilization': True,
            'max_grad_norm': 2.0,
            'reward_scale': 1.0,
            'update_frequency': 1,
            'warmup_steps': 500,
            'use_layer_norm': False,
            'use_reward_normalization': False,
            'use_huber_loss': False,
            'target_clipping': False,
            'alpha_clipping': True,
            'emergency_lr_reduction': False,
            'loss_anomaly_detection': True
        }
    }
    return presets.get(preset_name, {})

def create_training_environment(results, symbols, args):
    """ê°œì„ ëœ ìˆœì°¨ì  í•™ìŠµìš© í™˜ê²½ì„ ìƒì„±"""
    LOGGER.info("ê°œì„ ëœ ìˆœì°¨ì  í•™ìŠµìš© í™˜ê²½ ìƒì„± ì¤‘...")
    
    if args.multi_asset:
        # ë‹¤ì¤‘ ìì‚°ì˜ ê²½ìš° ê¸°ì¡´ ë°©ì‹ ìœ ì§€ (ì¼ë‹¨)
        LOGGER.error("âŒ ë‹¤ì¤‘ ìì‚° í™˜ê²½ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        LOGGER.info("ë‹¨ì¼ ìì‚° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì„¸ìš”: âŒ --multi_asset ì˜µì…˜ ì œê±°")
        return None
        
    else:
        # ë‹¨ì¼ ìì‚°ì˜ ê²½ìš° ê°œì„ ëœ ìˆœì°¨ì  í™˜ê²½ ìƒì„±
        symbol = symbols[0]
        LOGGER.info(f"ê°œì„ ëœ ìˆœì°¨ì  ë‹¨ì¼ ìì‚° íŠ¸ë ˆì´ë”© í™˜ê²½ ìƒì„± ì¤‘: {symbol}")
        
        if symbol not in results:
            LOGGER.error(f"{symbol} ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        if 'train' in results[symbol] and 'featured_data' in results[symbol]:
            normalized_data = results[symbol]['train']
            original_data = results[symbol]['featured_data']
        else:
            LOGGER.error(f"{symbol} í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ê¸°ë³¸ í™˜ê²½ ìƒì„±
        base_env = TradingEnvironment(
            data=normalized_data,
            raw_data=original_data,
            window_size=args.window_size,
            initial_balance=args.initial_balance,
            symbol=symbol,
            train_data=True
        )
        
        # âœ… ê°œì„ ëœ ìˆœì°¨ì  í™˜ê²½ìœ¼ë¡œ ë˜í•‘
        try:
            # ê°œì„ ëœ ë²„ì „ import ì‹œë„, ë§¨ ìœ„ì—ì„œ ì„í¬íŠ¸ 
            from src.environment.sequential_episode_manager import create_improved_sequential_training_setup
            
            sequential_env, episode_manager = create_improved_sequential_training_setup(
                base_env, 
                overlap_ratio=0.0,  # ë°ì´í„° ì†ì‹¤ ë°©ì§€ë¥¼ ìœ„í•´ ê²¹ì¹¨ ì—†ìŒ
                adaptive_length=True,  # ì ì‘í˜• ê¸¸ì´ í™œì„±í™”
                logger=LOGGER
            )
            
            LOGGER.info(f"âœ… ê°œì„ ëœ ìˆœì°¨ì  í•™ìŠµ í™˜ê²½ ìƒì„± ì™„ë£Œ") # ë°ì´í„° ê¸¸ì´ 1000 ë‚˜ëˆ´ì„ë•Œ ì§œíˆ¬ë¦¬ê¹Œì§€ í•™ìŠµ
            LOGGER.info(f"   â””â”€ íƒ€ì…: {type(sequential_env)}")
            LOGGER.info(f"   â””â”€ ì´ ì—í”¼ì†Œë“œ ìˆ˜: {episode_manager.episode_plan.__len__()}")
            LOGGER.info(f"   â””â”€ ì ì‘í˜• ê¸¸ì´: {'í™œì„±í™”' if episode_manager.adaptive_length else 'ë¹„í™œì„±í™”'}")
            
            # ì»¤ë²„ë¦¬ì§€ ìš”ì•½ ì¶œë ¥
            coverage = episode_manager.get_coverage_summary()
            LOGGER.info(f"   â””â”€ ë°ì´í„° ì»¤ë²„ë¦¬ì§€: {coverage['unique_coverage_pct']:.1f}% (ê³ ìœ )")
            LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ê¸¸ì´ ë²”ìœ„: {coverage['min_episode_length']}~{coverage['max_episode_length']}")
            LOGGER.info(f"   â””â”€ í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´: {coverage['average_episode_length']:.1f}")
            
        except ImportError as e:
            LOGGER.warning(f"âš ï¸ ê°œì„ ëœ ìˆœì°¨ì  í™˜ê²½ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}")
            
            # ê¸°ë³¸ ë²„ì „ìœ¼ë¡œ fallback, ë§¨ ìœ„ì—ì„œ ì„í¬íŠ¸ 
            from src.environment.sequential_episode_manager import create_sequential_training_setup
            
            sequential_env, episode_manager = create_sequential_training_setup(
                base_env, 
                overlap_ratio=0.0,
                logger=LOGGER
            )
            
            LOGGER.info(f"âœ… ê¸°ë³¸ ìˆœì°¨ì  í•™ìŠµ í™˜ê²½ ìƒì„± ì™„ë£Œ (fallback)") # ë°ì´í„° ê¸¸ì´ 1000 ë‚˜ëˆ´ì„ë•Œ ë§ˆì§€ë§‰ ì—í”¼ì—ì„œ ë‚˜ë¨¸ì§€ê¹Œì§€ +@ ìŠ¤í… í•™ìŠµ
            LOGGER.info(f"   â””â”€ ì´ ì—í”¼ì†Œë“œ ìˆ˜: {episode_manager.total_episodes}")
            LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ê°„ê²©: {episode_manager.episode_stride}")
        
        return sequential_env

def create_agent(env, args):
    """SAC ì—ì´ì „íŠ¸ ìƒì„± (ì•ˆì •í™” ê¸°ëŠ¥ ì§€ì›)"""
    LOGGER.info("SAC ì—ì´ì „íŠ¸ ìƒì„± ì¤‘...")

    # í–‰ë™ ì°¨ì› ê²°ì •
    if args.multi_asset:
        action_dim = len(env.envs)
    else:
        action_dim = 1

    # ìƒí˜¸ ë°°íƒ€ì  ê²€ì¦ ì¶”ê°€
    if args.use_cnn and args.use_lstm:
        LOGGER.error("âŒ CNNê³¼ LSTMì„ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ì•ˆì •í™” í”„ë¦¬ì…‹ ì ìš©
    stabilization_config = {}
    if args.stabilization_preset:
        preset_config = get_stabilization_preset(args.stabilization_preset)
        stabilization_config.update(preset_config)
        LOGGER.info(f"âœ… ì•ˆì •í™” í”„ë¦¬ì…‹ ì ìš©ë¨: {args.stabilization_preset}")
    else:
        # ê°œë³„ ì•ˆì •í™” ì„¤ì • ì ìš©
        stabilization_config = {
            'use_stabilization': args.use_stabilization,
            'max_grad_norm': args.max_grad_norm,
            'reward_scale': args.reward_scale,
            'update_frequency': args.update_frequency,
            'warmup_steps': args.warmup_steps,
            'use_layer_norm': args.use_layer_norm,
            'use_reward_normalization': args.use_reward_normalization,
            'use_huber_loss': args.use_huber_loss,
            'target_clipping': args.target_clipping,
            'alpha_clipping': args.alpha_clipping,
            'emergency_lr_reduction': args.emergency_lr_reduction,
            'loss_anomaly_detection': args.loss_anomaly_detection
        }

    # ëª¨ë¸ íƒ€ì… ë¡œê·¸
    if args.use_lstm:
        LOGGER.info("[LSTM ì‚¬ìš©] LSTM ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    elif args.use_cnn:
        LOGGER.info("[CNN ì‚¬ìš©] CNN ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        LOGGER.info("[MLP ì‚¬ìš©] ê¸°ë³¸ MLP ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ì•ˆì •í™” ê¸°ëŠ¥ ë¡œê·¸
    if stabilization_config.get('use_stabilization', False):
        LOGGER.info("âœ… ì•ˆì •í™” ê¸°ëŠ¥ì´ í™œì„±í™”ë©ë‹ˆë‹¤:")
        LOGGER.info(f"   â””â”€ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘: {stabilization_config['max_grad_norm']}")
        LOGGER.info(f"   â””â”€ ë³´ìƒ ìŠ¤ì¼€ì¼ë§: {stabilization_config['reward_scale']}")
        LOGGER.info(f"   â””â”€ ì—…ë°ì´íŠ¸ ë¹ˆë„: {stabilization_config['update_frequency']}")
        LOGGER.info(f"   â””â”€ ì›Œë°ì—… ìŠ¤í…: {stabilization_config['warmup_steps']}")
        LOGGER.info(f"   â””â”€ ë³´ìƒ ì •ê·œí™”: {stabilization_config['use_reward_normalization']}")
        LOGGER.info(f"   â””â”€ Huber Loss: {stabilization_config['use_huber_loss']}")
        LOGGER.info(f"   â””â”€ íƒ€ê²Ÿ í´ë¦¬í•‘: {stabilization_config['target_clipping']}")
        LOGGER.info(f"   â””â”€ Alpha í´ë¦¬í•‘: {stabilization_config['alpha_clipping']}")
    else:
        LOGGER.info("âœ… ê¸°ë³¸ SAC ëª¨ë“œ (ì•ˆì •í™” ê¸°ëŠ¥ ë¹„í™œì„±í™”)")

    # ë²„í¼ íƒ€ì… ë¡œê·¸ ì¶”ê°€
    LOGGER.info(f"[ë²„í¼ íƒ€ì…] {args.buffer_type.upper()} ë¦¬í”Œë ˆì´ ë²„í¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ì—ì´ì „íŠ¸ ìƒì„± - ì•ˆì •í™” íŒŒë¼ë¯¸í„° ì¶”ê°€
    agent = SACAgent(
        state_dim=None,
        action_dim=action_dim,
        hidden_dim=args.hidden_dim,
        input_shape=(args.window_size, env.feature_dim if not args.multi_asset else list(env.envs.values())[0].feature_dim),
        use_cnn=args.use_cnn,
        use_lstm=args.use_lstm,
        buffer_type=args.buffer_type,
        sequence_length=32,
        
        # ì•ˆì •í™” ê¸°ëŠ¥ íŒŒë¼ë¯¸í„°
        stabilization_enabled=stabilization_config.get('use_stabilization', False),
        max_grad_norm=stabilization_config.get('max_grad_norm', 1.0),
        reward_scale=stabilization_config.get('reward_scale', 1.0),
        update_frequency=stabilization_config.get('update_frequency', 1),
        warmup_steps=stabilization_config.get('warmup_steps', 1000),
        use_layer_norm=stabilization_config.get('use_layer_norm', False),
        use_reward_normalization=stabilization_config.get('use_reward_normalization', False),
        use_huber_loss=stabilization_config.get('use_huber_loss', False),
        target_clipping=stabilization_config.get('target_clipping', False),
        alpha_clipping=stabilization_config.get('alpha_clipping', False),
        emergency_lr_reduction=stabilization_config.get('emergency_lr_reduction', False),
        loss_anomaly_detection=stabilization_config.get('loss_anomaly_detection', False)
    )

    # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (ìˆ˜ì •ëœ ë¶€ë¶„)
    agent.actor = agent.actor.to(DEVICE)
    agent.critic = agent.critic.to(DEVICE)
    agent.critic_target = agent.critic_target.to(DEVICE)
    
    # ëª¨ë¸ íƒ€ì… ì •ë³´ë¥¼ ì—ì´ì „íŠ¸ì— ì €ì¥ (ì €ì¥ ì‹œ ì‚¬ìš©)
    if args.use_lstm:
        model_type = 'lstm'
    elif args.use_cnn:
        model_type = 'cnn'
    else:
        model_type = 'mlp'
    agent.model_type = model_type
    agent.training_symbols = args.symbols if args.symbols else TARGET_SYMBOLS

    # ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
    if args.load_model:
        LOGGER.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {args.load_model}")
        try:
            agent.load_model(args.load_model)
            # ë¡œë“œ í›„ì—ë„ ëª…ì‹œì ìœ¼ë¡œ ë‹¤ì‹œ ì´ë™ (GPU ë¡œë“œ ì‹œ í•„ìš”í•¨)
            agent.actor = agent.actor.to(DEVICE)
            agent.critic = agent.critic.to(DEVICE)
            agent.critic_target = agent.critic_target.to(DEVICE)
            LOGGER.info("ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            LOGGER.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            LOGGER.info("âœ… ìƒˆ ëª¨ë¸ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    LOGGER.info(f"SAC ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ (í–‰ë™ ì°¨ì›: {action_dim}, ì€ë‹‰ì¸µ: {args.hidden_dim})")
    if agent.stabilization_enabled:
        LOGGER.info("âœ… ì•ˆì •í™” ê¸°ëŠ¥ ì¤€ë¹„ ì™„ë£Œ")
    return agent

    # âœ… ì¶”ê°€: í•™ìŠµ ì‹œì‘ ì „ í™˜ê²½ ê²€ì¦ í•¨ìˆ˜
def validate_training_environment(train_env, args):
    """í•™ìŠµ í™˜ê²½ ê²€ì¦ ë° ë°ì´í„° ì†ì‹¤ ìœ„í—˜ ì‚¬ì „ ì ê²€"""
    LOGGER.info("ğŸ” í•™ìŠµ í™˜ê²½ ê²€ì¦ ì¤‘...")
    
    # ìˆœì°¨ì  í™˜ê²½ì¸ì§€ í™•ì¸
    if not hasattr(train_env, 'episode_manager'):
        LOGGER.warning("âš ï¸ ìˆœì°¨ì  í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ë°ì´í„° í™œìš©ë¥ ì´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return True
    
    # ì»¤ë²„ë¦¬ì§€ ë¶„ì„
    episode_manager = train_env.episode_manager
    
    if hasattr(episode_manager, 'get_coverage_summary'):
        coverage = episode_manager.get_coverage_summary()
        
        LOGGER.info("ğŸ“Š í™˜ê²½ ê²€ì¦ ê²°ê³¼:")
        LOGGER.info(f"   â””â”€ ì´ ì—í”¼ì†Œë“œ ìˆ˜: {coverage['total_episodes']}")
        LOGGER.info(f"   â””â”€ ë°ì´í„° ì»¤ë²„ë¦¬ì§€: {coverage['unique_coverage_pct']:.1f}%")
        LOGGER.info(f"   â””â”€ í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´: {coverage['average_episode_length']:.1f}")
        
        # ê²½ê³  ë° ê¶Œì¥ì‚¬í•­
        if coverage['unique_coverage_pct'] < 95:
            LOGGER.warning(f"âš ï¸ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ê°€ ë‚®ìŠµë‹ˆë‹¤: {coverage['unique_coverage_pct']:.1f}%")
            LOGGER.info("ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            LOGGER.info("   â””â”€ --adaptive_episode_length ì˜µì…˜ ì‚¬ìš©")
            LOGGER.info("   â””â”€ --max_steps ê°’ ì¦ê°€")
            LOGGER.info("   â””â”€ --episode_overlap_ratio ì¡°ì •")
        
        if coverage['total_episodes'] > args.num_episodes:
            LOGGER.warning(f"âš ï¸ ê³„íšëœ ì—í”¼ì†Œë“œ ìˆ˜({coverage['total_episodes']})ê°€ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜({args.num_episodes})ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤.")
            LOGGER.info("ğŸ’¡ ê¶Œì¥ì‚¬í•­: --num_episodes ê°’ì„ ëŠ˜ë ¤ì„œ ì „ì²´ ë°ì´í„°ë¥¼ í™œìš©í•˜ì„¸ìš”.")
        
        # ì—í”¼ì†Œë“œ íƒ€ì… ë¶„í¬ í‘œì‹œ
        if 'episode_types' in coverage:
            LOGGER.info("ğŸ“Š ì—í”¼ì†Œë“œ íƒ€ì… ë¶„í¬:")
            for ep_type, count in coverage['episode_types'].items():
                percentage = (count / coverage['total_episodes']) * 100
                LOGGER.info(f"   â””â”€ {ep_type}: {count}ê°œ ({percentage:.1f}%)")
        
        LOGGER.info("âœ… í™˜ê²½ ê²€ì¦ ì™„ë£Œ")
        return True
        
    else:
        LOGGER.info("âœ… ê¸°ë³¸ ìˆœì°¨ì  í™˜ê²½ ì‚¬ìš©")
        return True


# âœ… í•™ìŠµ ì™„ë£Œ í›„ ë°ì´í„° í™œìš©ë¥  ë¦¬í¬íŠ¸ ìƒì„±
def generate_data_utilization_report(data_coverage_log, args):
    """ë°ì´í„° í™œìš©ë¥  ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±"""
    if not data_coverage_log:
        return
    
    LOGGER.info("=" * 80)
    LOGGER.info("ğŸ“Š ìµœì¢… ë°ì´í„° í™œìš©ë¥  ë¦¬í¬íŠ¸")
    LOGGER.info("=" * 80)
    
    # ê¸°ë³¸ í†µê³„
    total_episodes = len(data_coverage_log)
    total_planned = sum(c.get('planned_steps', 0) for c in data_coverage_log)
    total_actual = sum(c.get('actual_steps', 0) for c in data_coverage_log)
    
    if total_planned > 0:
        overall_utilization = total_actual / total_planned
        lost_steps = total_planned - total_actual
        
        LOGGER.info(f"ğŸ“ˆ ì „ì²´ í†µê³„:")
        LOGGER.info(f"   â””â”€ ì´ ì—í”¼ì†Œë“œ: {total_episodes}")
        LOGGER.info(f"   â””â”€ ê³„íšëœ ì´ ìŠ¤í…: {total_planned:,}")
        LOGGER.info(f"   â””â”€ ì‹¤ì œ ì²˜ë¦¬ ìŠ¤í…: {total_actual:,}")
        LOGGER.info(f"   â””â”€ ì „ì²´ í™œìš©ë¥ : {overall_utilization:.1%}")
        LOGGER.info(f"   â””â”€ ì†ì‹¤ëœ ìŠ¤í…: {lost_steps:,}")
        
        # í™œìš©ë¥  ë¶„í¬
        utilizations = [c.get('data_utilization', 0) for c in data_coverage_log]
        if utilizations:
            avg_util = np.mean(utilizations)
            min_util = min(utilizations)
            max_util = max(utilizations)
            
            LOGGER.info(f"ğŸ“Š ì—í”¼ì†Œë“œë³„ í™œìš©ë¥ :")
            LOGGER.info(f"   â””â”€ í‰ê· : {avg_util:.1%}")
            LOGGER.info(f"   â””â”€ ë²”ìœ„: {min_util:.1%} ~ {max_util:.1%}")
            
            # ë¬¸ì œê°€ ìˆëŠ” ì—í”¼ì†Œë“œ ì°¾ê¸°
            low_util_episodes = [i for i, util in enumerate(utilizations) if util < 0.8]
            if low_util_episodes:
                LOGGER.warning(f"âš ï¸ ë‚®ì€ í™œìš©ë¥  ì—í”¼ì†Œë“œ ({len(low_util_episodes)}ê°œ):")
                for ep_idx in low_util_episodes[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                    util = utilizations[ep_idx]
                    coverage = data_coverage_log[ep_idx]
                    LOGGER.warning(f"   â””â”€ Episode {ep_idx+1}: {util:.1%} ({coverage.get('actual_steps', 0)}/{coverage.get('planned_steps', 0)})")
                if len(low_util_episodes) > 5:
                    LOGGER.warning(f"   â””â”€ ... ë° {len(low_util_episodes)-5}ê°œ ì¶”ê°€")
        
        # ê¶Œì¥ì‚¬í•­
        LOGGER.info(f"ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        if overall_utilization < 0.9:
            LOGGER.info(f"   â””â”€ MAX_STEPS_PER_EPISODE ê°’ì„ {int(args.max_steps * 1.5)} ì´ìƒìœ¼ë¡œ ì¦ê°€")
            LOGGER.info(f"   â””â”€ --adaptive_episode_length ì˜µì…˜ ì‚¬ìš©")
        if overall_utilization < 0.95:
            LOGGER.info(f"   â””â”€ --force_full_coverage ì˜µì…˜ ê³ ë ¤")
        if overall_utilization >= 0.95:
            LOGGER.info(f"   â””â”€ âœ… ìš°ìˆ˜í•œ ë°ì´í„° í™œìš©ë¥ ì…ë‹ˆë‹¤!")
    
    LOGGER.info("=" * 80)

class StepLossTracker:
    """ìŠ¤í…ë³„ loss ì¶”ì ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, window_size=100):
        self.window_size = window_size
        self.actor_losses = []
        self.critic_losses = []
        self.alpha_losses = []
        self.alpha_values = []
        self.entropy_values = []
        
    def add_stats(self, stats):
        """í†µê³„ ì¶”ê°€"""
        if stats:
            self.actor_losses.append(stats.get('actor_loss', 0.0))
            self.critic_losses.append(stats.get('critic_loss', 0.0))
            self.alpha_losses.append(stats.get('alpha_loss', 0.0))
            self.alpha_values.append(stats.get('alpha', 0.0))
            self.entropy_values.append(stats.get('entropy', 0.0))
            
            # ìœˆë„ìš° í¬ê¸° ìœ ì§€
            if len(self.actor_losses) > self.window_size:
                self.actor_losses.pop(0)
                self.critic_losses.pop(0)
                self.alpha_losses.pop(0)
                self.alpha_values.pop(0)
                self.entropy_values.pop(0)
    
    def get_averages(self):
        """í‰ê· ê°’ ë°˜í™˜"""
        if not self.actor_losses:
            return {
                'avg_actor_loss': 0.0,
                'avg_critic_loss': 0.0,
                'avg_alpha_loss': 0.0,
                'avg_alpha': 0.0,
                'avg_entropy': 0.0,
                'num_samples': 0
            }
        
        return {
            'avg_actor_loss': np.mean(self.actor_losses),
            'avg_critic_loss': np.mean(self.critic_losses),
            'avg_alpha_loss': np.mean(self.alpha_losses),
            'avg_alpha': np.mean(self.alpha_values),
            'avg_entropy': np.mean(self.entropy_values),
            'num_samples': len(self.actor_losses)
        }

def log_step_progress(episode, step, total_episodes, max_episode_steps, 
                     loss_tracker, timer, args, agent):
    """ìŠ¤í…ë³„ ì§„í–‰ìƒí™© ë¡œê¹… (ì•ˆì •í™” ìƒíƒœ í¬í•¨)"""
    
    # ì§„í–‰ë¥  ê³„ì‚°
    episode_progress = (step / max_episode_steps) * 100
    total_progress = ((episode - 1) * 100 + episode_progress) / total_episodes
    
    # í‰ê·  loss ê°€ì ¸ì˜¤ê¸°
    averages = loss_tracker.get_averages()
    
    # ì‹œê°„ ì •ë³´
    elapsed_time = timer.get_training_time()
    eta = timer.get_eta(episode, total_episodes)
    
    LOGGER.info("=" * 100)
    LOGGER.info(f"âœ… STEP PROGRESS | Episode {episode}/{total_episodes} | Step {step}/{max_episode_steps}")
    LOGGER.info("=" * 100)
    LOGGER.info(f"âœ… ì§„í–‰ë¥ :")
    LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ì§„í–‰ë¥ : {episode_progress:.1f}%")
    LOGGER.info(f"   â””â”€ ì „ì²´ ì§„í–‰ë¥ : {total_progress:.1f}%")
    LOGGER.info(f"   â””â”€ ì™„ë£Œëœ ì—í”¼ì†Œë“œ: {episode-1}/{total_episodes}")
    
    LOGGER.info(f"âœ… ì‹œê°„ ì •ë³´:")
    LOGGER.info(f"   â””â”€ ê²½ê³¼ ì‹œê°„: {timer.format_time(elapsed_time)}")
    LOGGER.info(f"   â””â”€ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {timer.format_time(eta)}")
    
    LOGGER.info(f"âœ… í‰ê·  Loss (ìµœê·¼ {averages['num_samples']}ìŠ¤í…):")
    LOGGER.info(f"   â””â”€ Actor Loss: {averages['avg_actor_loss']:.6f}")
    LOGGER.info(f"   â””â”€ Critic Loss: {averages['avg_critic_loss']:.6f}")
    LOGGER.info(f"   â””â”€ Alpha Loss: {averages['avg_alpha_loss']:.6f}")
    LOGGER.info(f"   â””â”€ Alpha: {averages['avg_alpha']:.6f}")
    LOGGER.info(f"   â””â”€ Entropy: {averages['avg_entropy']:.6f}")
    
    # âœ… ì•ˆì •í™” ìƒíƒœ ì •ë³´ (ì•ˆì „í•œ ì†ì„± ì ‘ê·¼)
    if hasattr(agent, 'stabilization_enabled') and agent.stabilization_enabled:
        LOGGER.info(f"âœ… ì•ˆì •í™” ìƒíƒœ:")
        if hasattr(agent, 'train_step_counter'):
            LOGGER.info(f"   â””â”€ í›ˆë ¨ ìŠ¤í…: {agent.train_step_counter}")
            if hasattr(agent, 'warmup_steps'):
                warmup_complete = agent.train_step_counter >= agent.warmup_steps
                LOGGER.info(f"   â””â”€ ì›Œë°ì—… ì™„ë£Œ: {'âœ…' if warmup_complete else 'âŒ'}")
        if hasattr(agent, 'update_counter'):
            LOGGER.info(f"   â””â”€ ì—…ë°ì´íŠ¸ ì¹´ìš´í„°: {agent.update_counter}")
        if hasattr(agent, 'reward_history') and agent.reward_history:
            LOGGER.info(f"   â””â”€ ë³´ìƒ ì •ê·œí™” ìƒ˜í”Œ: {len(agent.reward_history)}")
            if hasattr(agent, 'reward_mean') and hasattr(agent, 'reward_std'):
                LOGGER.info(f"   â””â”€ ë³´ìƒ í‰ê· : {agent.reward_mean:.4f}")
                LOGGER.info(f"   â””â”€ ë³´ìƒ í‘œì¤€í¸ì°¨: {agent.reward_std:.4f}")
    
    LOGGER.info("=" * 100)

def train_agent_sequential(agent, train_env, args, timer):
    """ìˆœì°¨ì  ë°ì´í„° í™œìš© ì—ì´ì „íŠ¸ í•™ìŠµ (ì•ˆì •í™” ê¸°ëŠ¥ í¬í•¨)"""
    LOGGER.info("ìˆœì°¨ì  í•™ìŠµ ì‹œì‘...")
    global episode_actions_history
    
    episode_rewards = []
    portfolio_values = []
    shares_history = []
    data_coverage_log = []
    
    # ìŠ¤í…ë³„ loss ì¶”ì ê¸° ì´ˆê¸°í™”
    loss_tracker = StepLossTracker(window_size=args.step_log_interval)
    global_step_count = 0  # ì „ì²´ ìŠ¤í… ì¹´ìš´í„°
    
    # ì•ˆì •í™” ê¸°ëŠ¥ ì´ˆê¸° ë¡œê·¸ (ì•ˆì „í•œ ì†ì„± ì ‘ê·¼)
    if hasattr(agent, 'stabilization_enabled') and agent.stabilization_enabled:
        LOGGER.info("âœ… ì•ˆì •í™” ê¸°ëŠ¥ì´ í™œì„±í™”ëœ ìƒíƒœë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        if hasattr(agent, 'warmup_steps'):
            LOGGER.info(f"   â””â”€ ì›Œë°ì—… ìŠ¤í…: {agent.warmup_steps}")
        if hasattr(agent, 'max_grad_norm'):
            LOGGER.info(f"   â””â”€ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘: {agent.max_grad_norm}")
        if hasattr(agent, 'update_frequency'):
            LOGGER.info(f"   â””â”€ ì—…ë°ì´íŠ¸ ë¹ˆë„: {agent.update_frequency}")
    
    for episode in range(args.num_episodes):
        episode_actions = []  # ì—í”¼ì†Œë“œë³„ í–‰ë™ ê¸°ë¡ ì´ˆê¸°í™”
        timer.start_episode()
        
        try:
            # ìˆœì°¨ì  ë¦¬ì…‹
            state = train_env.reset(episode_num=episode)
        except Exception as e:
            LOGGER.error(f"âŒ Episode {episode+1} ë¦¬ì…‹ ì‹¤íŒ¨: {e}")
            continue

        episode_reward = 0
        steps = 0
        
        # ì•ˆì „í•œ current_step ì ‘ê·¼
        try:
            if hasattr(train_env, 'base_env') and train_env.base_env:
                actual_current_step = train_env.base_env.current_step
            else:
                actual_current_step = getattr(train_env, 'current_step', 0)
        except AttributeError:
            actual_current_step = 0
            LOGGER.warning("í˜„ì¬ ìŠ¤í… ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì—í”¼ì†Œë“œ ë©”íƒ€ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        meta_info = getattr(train_env, 'episode_meta_info', {})
        expected_start = meta_info.get('start_index', 0)
        
        LOGGER.info(f"âœ… Episode {episode+1} ë¦¬ì…‹ ì™„ë£Œ:")
        LOGGER.info(f"   â””â”€ ì‹¤ì œ current_step: {actual_current_step}")
        LOGGER.info(f"   â””â”€ ì˜ˆìƒ ì‹œì‘ì : {expected_start}")
        LOGGER.info(f"   â””â”€ ì¼ì¹˜ ì—¬ë¶€: {'âœ…' if actual_current_step == expected_start else 'âŒ'}")
        LOGGER.info(f"   â””â”€ ê³„íšëœ ë²”ìœ„: [{meta_info.get('start_index', 'N/A')}~{meta_info.get('end_index', 'N/A')}]")

        # âœ… ê°œì„ ëœ ì—í”¼ì†Œë“œ ê¸¸ì´ ê²°ì • (ë°ì´í„° ì†ì‹¤ ë°©ì§€)
        planned_length = meta_info.get('actual_length', 1000)
        is_last_episode = meta_info.get('is_last_in_cycle', False)
        
        # ìƒˆë¡œìš´ ì „ëµ: ê¸´ ì—í”¼ì†Œë“œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        if planned_length > MAX_STEPS_PER_EPISODE:
            LOGGER.info(f"âœ… ê¸´ ì—í”¼ì†Œë“œ ê°ì§€: {planned_length} > {MAX_STEPS_PER_EPISODE}")
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            chunks_needed = (planned_length + MAX_STEPS_PER_EPISODE - 1) // MAX_STEPS_PER_EPISODE
            LOGGER.info(f"   â””â”€ {chunks_needed}ê°œ ì²­í¬ë¡œ ë¶„í•  ì²˜ë¦¬ ì˜ˆì •")
            
            # í˜„ì¬ ì—í”¼ì†Œë“œëŠ” ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì²˜ë¦¬
            max_episode_steps = MAX_STEPS_PER_EPISODE
            remaining_steps = planned_length - MAX_STEPS_PER_EPISODE
            
            LOGGER.info(f"   â””â”€ í˜„ì¬ ì²­í¬: {max_episode_steps} ìŠ¤í…")
            LOGGER.info(f"   â””â”€ ì”ì—¬ ë°ì´í„°: {remaining_steps} ìŠ¤í… (ë‹¤ìŒ ì—í”¼ì†Œë“œì—ì„œ ì²˜ë¦¬)")
            
        else:
            # ê³„íšëœ ê¸¸ì´ê°€ MAX_STEPS ì´í•˜ì¸ ê²½ìš° ì „ì²´ ì²˜ë¦¬
            max_episode_steps = planned_length
            LOGGER.info(f"âœ… Episode {episode+1}: {max_episode_steps} ìŠ¤í… ì „ì²´ í•™ìŠµ")
        
        # ë§ˆì§€ë§‰ ì—í”¼ì†Œë“œëŠ” í•­ìƒ ì „ì²´ ì²˜ë¦¬
        if is_last_episode:
            max_episode_steps = planned_length
            LOGGER.info(f"âœ… ì‚¬ì´í´ ë§ˆì§€ë§‰ ì—í”¼ì†Œë“œ: {planned_length} ìŠ¤í… ëª¨ë‘ í•™ìŠµ")
        
        # ë°ì´í„° ì†ì‹¤ ê²½ê³  ì œê±° (ì´ì œ ì†ì‹¤ì´ ì—†ì–´ì•¼ í•¨)
        if planned_length > max_episode_steps:
            remaining_data = planned_length - max_episode_steps
            LOGGER.info(f"ğŸ“Š ì²˜ë¦¬ ì˜ˆì •: {max_episode_steps}/{planned_length} ìŠ¤í…")
            LOGGER.info(f"ğŸ“Š ì”ì—¬ ë°ì´í„°: {remaining_data} ìŠ¤í… (ìˆœì°¨ì  í™˜ê²½ì—ì„œ ë‹¤ìŒì— ì²˜ë¦¬)")
        else:
            LOGGER.info(f"ğŸ“Š ì „ì²´ ë°ì´í„° ì²˜ë¦¬: {max_episode_steps} ìŠ¤í…")
            
        # ì—í”¼ì†Œë“œ ì§„í–‰
        done = False
        while not done and steps < max_episode_steps:
            # í–‰ë™ ì„ íƒ
            action = agent.select_action(state, evaluate=False)
            episode_actions.append(action)  # í–‰ë™ ê¸°ë¡
            
            # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            next_state, reward, done, info = train_env.step(action)
            
            # ê²½í—˜ ì €ì¥
            agent.add_experience(state, action, reward, next_state, done)
            
            # ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            if len(agent.replay_buffer) > args.batch_size:
                stats = agent.update_parameters(args.batch_size)
                loss_tracker.add_stats(stats)
            else:
                # ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠëŠ” ê²½ìš° ë¹ˆ stats ì¶”ê°€
                loss_tracker.add_stats({})
            
            episode_reward += reward
            steps += 1
            global_step_count += 1
            state = next_state
            
            # ìŠ¤í…ë³„ ì§„í–‰ìƒí™© ë¡œê¹…
            if args.step_log_interval > 0 and steps % args.step_log_interval == 0:
                log_step_progress(
                    episode + 1, steps, args.num_episodes, max_episode_steps, 
                    loss_tracker, timer, args, agent
                )
        
        # âœ… ëª©í‘œ ìŠ¤í… ë„ë‹¬ ë¡œê¹…
        if steps >= max_episode_steps:
            LOGGER.info(f"Episode {episode+1} ëª©í‘œ ìŠ¤í…({max_episode_steps}) ë‹¬ì„±ìœ¼ë¡œ ì™„ë£Œ")
            if max_episode_steps == planned_length:
                LOGGER.info(f"   âœ… ê³„íšëœ ëª¨ë“  ë°ì´í„° í•™ìŠµ ì™„ë£Œ (ì†ì‹¤ ì—†ìŒ)")
            else:
                LOGGER.info(f"   ğŸ“Š ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ ({max_episode_steps}/{planned_length})")
        
        # âœ… ì—í”¼ì†Œë“œ ì™„ë£Œ í›„ í–‰ë™ ê¸°ë¡ ì¶”ê°€ (ì˜¬ë°”ë¥¸ ìœ„ì¹˜)
        episode_actions_history.append(episode_actions)
        
        episode_time = timer.end_episode()
        episode_rewards.append(episode_reward)
        
        # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ (10 ì—í”¼ì†Œë“œë§ˆë‹¤) by ì œí™˜
        if episode % 10 == 0:
            torch.cuda.empty_cache()
            
        # VRAM ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (50 ì—í”¼ì†Œë“œë§ˆë‹¤) by ì œí™˜
        if episode % 50 == 0:
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            LOGGER.info(f"   â””â”€ VRAM ì‚¬ìš©ëŸ‰: í• ë‹¹ {allocated:.2f}GB, ìºì‹œ {reserved:.2f}GB")
            
        portfolio_values.append(info.get('portfolio_value', args.initial_balance))
        shares_history.append(info.get('shares_held', 0))
        
        # âœ… ë°ì´í„° í™œìš©ë¥  ê³„ì‚° ë° ë¡œê¹…
        data_utilization = steps / planned_length if planned_length > 0 else 1.0
        steps_processed = steps
        steps_remaining = max(0, planned_length - steps)
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ìµœì¢… í‰ê·  loss ì¶œë ¥ (ì•ˆì •í™” ì •ë³´ í¬í•¨)
        final_averages = loss_tracker.get_averages()
        LOGGER.info(f"âœ… Episode {episode+1} ì™„ë£Œ:")
        LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ë³´ìƒ: {episode_reward:.4f}")
        LOGGER.info(f"   â””â”€ ì‹¤í–‰ ìŠ¤í…: {steps_processed}/{max_episode_steps} (ëª©í‘œ ëŒ€ë¹„)")
        LOGGER.info(f"   â””â”€ ë°ì´í„° í™œìš©ë¥ : {data_utilization:.1%} ({steps_processed}/{planned_length})")
        if steps_remaining > 0:
            LOGGER.info(f"   â””â”€ ì”ì—¬ ë°ì´í„°: {steps_remaining} ìŠ¤í… (ë‹¤ìŒ ì‚¬ì´í´ì—ì„œ ì²˜ë¦¬)")
        LOGGER.info(f"   â””â”€ í¬íŠ¸í´ë¦¬ì˜¤: ${info.get('portfolio_value', args.initial_balance):.2f}")
        
        # ë¦¬ìŠ¤í¬ ì •ë³´ ì¶œë ¥(by ë‚˜í˜„)
        if 'risk_limit_exceeded' in info and info['risk_limit_exceeded']:
            LOGGER.warning(f"   âš ï¸ ë¦¬ìŠ¤í¬ í•œë„ ì´ˆê³¼ë¡œ ì¡°ê¸° ì¢…ë£Œ")
        if 'max_drawdown_pct' in info:
            LOGGER.info(f"   â””â”€ ìµœëŒ€ ë‚™í­: {info['max_drawdown_pct']:.2f}%")
        if 'daily_loss_pct' in info:
            LOGGER.info(f"   â””â”€ ì¼ì¼ ì†ì‹¤: {info['daily_loss_pct']:.2f}%")
        LOGGER.info(f"   â””â”€ í‰ê·  Actor Loss: {final_averages['avg_actor_loss']:.6f}")
        LOGGER.info(f"   â””â”€ í‰ê·  Critic Loss: {final_averages['avg_critic_loss']:.6f}")
        LOGGER.info(f"   â””â”€ í‰ê·  Alpha: {final_averages['avg_alpha']:.6f}")
        LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ì‹œê°„: {episode_time:.2f}ì´ˆ")
        
        # ì•ˆì •í™” ê¸°ëŠ¥ ì¶”ê°€ ì •ë³´ (ì•ˆì „í•œ ì†ì„± ì ‘ê·¼)
        if hasattr(agent, 'stabilization_enabled') and agent.stabilization_enabled:
            LOGGER.info(f"âœ… ì•ˆì •í™” ìƒíƒœ:")
            if hasattr(agent, 'train_step_counter'):
                LOGGER.info(f"   â””â”€ ì´ í›ˆë ¨ ìŠ¤í…: {agent.train_step_counter}")
                if hasattr(agent, 'warmup_steps'):
                    progress = min(100, (agent.train_step_counter / agent.warmup_steps) * 100)
                    LOGGER.info(f"   â””â”€ ì›Œë°ì—… ì§„í–‰ë¥ : {progress:.1f}%")
            if hasattr(agent, 'reward_history') and agent.reward_history:
                LOGGER.info(f"   â””â”€ ë³´ìƒ ì •ê·œí™” ìƒ˜í”Œ: {len(agent.reward_history)}")
        
        # âœ… ë°ì´í„° í™œìš©ë¥  ëª¨ë‹ˆí„°ë§ (10 ì—í”¼ì†Œë“œë§ˆë‹¤)
        if episode % 10 == 0 and len(episode_rewards) >= 10:
            recent_rewards = episode_rewards[-10:]
            max_reward = max(recent_rewards)
            min_reward = min(recent_rewards)
            avg_reward = sum(recent_rewards) / len(recent_rewards)
            
            LOGGER.info(f"âœ… ìµœê·¼ 10 ì—í”¼ì†Œë“œ ë¶„ì„:")
            LOGGER.info(f"   â””â”€ ë³´ìƒ ë²”ìœ„: [{min_reward:.4f} ~ {max_reward:.4f}]")
            LOGGER.info(f"   â””â”€ í‰ê·  ë³´ìƒ: {avg_reward:.4f}")
            
            # ë°ì´í„° í™œìš©ë¥  í†µê³„
            recent_coverage = data_coverage_log[-10:] if len(data_coverage_log) >= 10 else data_coverage_log
            if recent_coverage:
                avg_utilization = np.mean([c.get('data_utilization', 1.0) for c in recent_coverage])
                LOGGER.info(f"   â””â”€ í‰ê·  ë°ì´í„° í™œìš©ë¥ : {avg_utilization:.1%}")
            
            # ê·¹ë‹¨ì  ë³´ìƒ ê°ì§€ (ì•ˆì •í™” ê¸°ëŠ¥ì´ ìˆìœ¼ë©´ ë” ì—„ê²©í•˜ê²Œ)
            stabilization_enabled = hasattr(agent, 'stabilization_enabled') and agent.stabilization_enabled
            threshold = 50 if stabilization_enabled else 100
            if abs(max_reward) > threshold or abs(min_reward) > threshold:
                LOGGER.warning(f"âš ï¸ ê·¹ë‹¨ì  ë³´ìƒ ê°ì§€! Episode {episode+1}")
                LOGGER.warning(f"   â””â”€ ìµœëŒ€: {max_reward:.4f}, ìµœì†Œ: {min_reward:.4f}")
                if stabilization_enabled:
                    LOGGER.info("âœ… ì•ˆì •í™” ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì–´ ìˆì–´ ìë™ ì¡°ì •ë©ë‹ˆë‹¤.")

        # âœ… ê°œì„ ëœ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ ë¡œê¹…
        coverage_info = {
            'episode': episode,
            'start_idx': meta_info.get('start_index', 0),
            'end_idx': meta_info.get('end_index', 0),
            'planned_steps': planned_length,
            'actual_steps': steps,
            'data_utilization': data_utilization,
            'coverage_pct': meta_info.get('coverage_start_pct', 0)
        }
        data_coverage_log.append(coverage_info)
        
        # ì£¼ê¸°ì  ë¡œê¹…
        if episode % args.log_interval == 0:
            # ê¸°ì¡´ ë¡œê¹… í•¨ìˆ˜ ì‚¬ìš© + ìˆœì°¨ì  ì •ë³´ ì¶”ê°€
            log_training_progress(
                episode, args, episode_rewards, portfolio_values, 
                info, agent, timer, shares_history
            )
            # í‰ê°€ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            
            # âœ… ê°œì„ ëœ ìˆœì°¨ì  í•™ìŠµ ì •ë³´
            LOGGER.info("âœ… ìˆœì°¨ì  í•™ìŠµ ì •ë³´:")
            LOGGER.info(f"   â””â”€ ë°ì´í„° ë²”ìœ„: [{meta_info.get('start_index', 'N/A')}~{meta_info.get('end_index', 'N/A')}]")
            LOGGER.info(f"   â””â”€ ê³„íšëœ ê¸¸ì´: {planned_length} steps")
            LOGGER.info(f"   â””â”€ ì‹¤ì œ ì²˜ë¦¬: {steps} steps")
            LOGGER.info(f"   â””â”€ í™œìš©ë¥ : {data_utilization:.1%}")
            LOGGER.info(f"   â””â”€ ì»¤ë²„ë¦¬ì§€: {meta_info.get('coverage_start_pct', 0):.1f}%~{meta_info.get('coverage_end_pct', 0):.1f}%")
            LOGGER.info(f"   â””â”€ ì‚¬ì´í´: {meta_info.get('cycle_number', 0)}.{meta_info.get('episode_in_cycle', 0)}")
            LOGGER.info(f"   â””â”€ ì „ì²´ ìŠ¤í… ìˆ˜: {global_step_count:,}")
            
        elif episode < 5:
            LOGGER.info(
                f"Episode {episode+1}: ë³´ìƒ {episode_reward:.4f}, "
                f"ìŠ¤í… {steps}/{planned_length} ({data_utilization:.1%}), "
                f"í¬íŠ¸í´ë¦¬ì˜¤ ${info.get('portfolio_value', args.initial_balance):.2f}, "
                f"ë°ì´í„° ë²”ìœ„ [{meta_info.get('start_index', 'N/A')}~{meta_info.get('end_index', 'N/A')}]"
            )
        
        # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥
        if episode % args.save_interval == 0 and episode > 0:
            try:
                model_path = agent.save_model(prefix=f'checkpoint_episode_{episode+1}_')
                LOGGER.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ì €ì¥: {model_path}")
                torch.cuda.empty_cache() # by ì œí™˜
                
            except Exception as e:
                LOGGER.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # âœ… ìµœì¢… ì»¤ë²„ë¦¬ì§€ ë¶„ì„ (ê°œì„ ëœ ë²„ì „)
    if data_coverage_log:
        total_episodes = len(data_coverage_log)
        total_planned = sum(c['planned_steps'] for c in data_coverage_log)
        total_actual = sum(c['actual_steps'] for c in data_coverage_log)
        overall_utilization = total_actual / total_planned if total_planned > 0 else 0
        
        all_starts = [c['start_idx'] for c in data_coverage_log]
        all_ends = [c['end_idx'] for c in data_coverage_log]
        total_range = max(all_ends) - min(all_starts) if all_ends and all_starts else 0
        
        # í™œìš©ë¥  ë¶„í¬
        utilizations = [c['data_utilization'] for c in data_coverage_log]
        min_util = min(utilizations) if utilizations else 0
        max_util = max(utilizations) if utilizations else 0
        avg_util = np.mean(utilizations) if utilizations else 0
        
        LOGGER.info("=" * 60)
        LOGGER.info("âœ… ìµœì¢… ìˆœì°¨ì  í•™ìŠµ ì»¤ë²„ë¦¬ì§€ ë¶„ì„")
        LOGGER.info("=" * 60)
        LOGGER.info(f"ì´ ì—í”¼ì†Œë“œ: {total_episodes}")
        LOGGER.info(f"ì´ ìŠ¤í… ìˆ˜: {global_step_count:,}")
        LOGGER.info(f"ë°ì´í„° ë²”ìœ„: [{min(all_starts)}~{max(all_ends)}] ({total_range} steps)")
        LOGGER.info(f"ê³„íšëœ ì´ ìŠ¤í…: {total_planned:,}")
        LOGGER.info(f"ì‹¤ì œ ì²˜ë¦¬ ìŠ¤í…: {total_actual:,}")
        LOGGER.info(f"ì „ì²´ ë°ì´í„° í™œìš©ë¥ : {overall_utilization:.1%}")
        LOGGER.info(f"ì—í”¼ì†Œë“œë³„ í™œìš©ë¥ : ìµœì†Œ {min_util:.1%}, í‰ê·  {avg_util:.1%}, ìµœëŒ€ {max_util:.1%}")
        
        # ì†ì‹¤ ë°ì´í„° ë¶„ì„
        if overall_utilization < 0.95:  # 95% ë¯¸ë§Œì´ë©´ ê²½ê³ 
            lost_steps = total_planned - total_actual
            LOGGER.warning(f"âš ï¸ ë°ì´í„° ì†ì‹¤ ê°ì§€: {lost_steps:,} ìŠ¤í… ({(1-overall_utilization)*100:.1f}%)")
            LOGGER.info("ğŸ’¡ ê°œì„  ì œì•ˆ: MAX_STEPS_PER_EPISODE ê°’ì„ ëŠ˜ë¦¬ê±°ë‚˜ ì²­í¬ ì²˜ë¦¬ ë¡œì§ì„ ê°œì„ í•˜ì„¸ìš”.")
        else:
            LOGGER.info(f"âœ… ìš°ìˆ˜í•œ ë°ì´í„° í™œìš©ë¥ : {overall_utilization:.1%}")
        
        if hasattr(agent, 'stabilization_enabled') and agent.stabilization_enabled:
            train_steps = getattr(agent, 'train_step_counter', 0)
            LOGGER.info(f"âœ… ì•ˆì •í™” ê¸°ëŠ¥: í™œì„±í™” (ì´ í›ˆë ¨ ìŠ¤í…: {train_steps:,})")
        LOGGER.info("=" * 60)
    
    LOGGER.info("ìˆœì°¨ì  í•™ìŠµ ì™„ë£Œ!")
    return episode_rewards, portfolio_values, shares_history

def analyze_recent_actions(episode_actions_history, num_recent=10):
    """ìµœê·¼ ì—í”¼ì†Œë“œë“¤ì˜ í–‰ë™ íŒ¨í„´ ê°„ë‹¨ ë¶„ì„"""
    if not episode_actions_history:
        return None
    
    # ìµœê·¼ Nê°œ ì—í”¼ì†Œë“œ ì„ íƒ
    recent_episodes = episode_actions_history[-num_recent:] if len(episode_actions_history) >= num_recent else episode_actions_history
    
    # ëª¨ë“  í–‰ë™ í•©ì¹˜ê¸°
    all_actions = []
    for ep_actions in recent_episodes:
        all_actions.extend(ep_actions)
    
    if not all_actions:
        return None
    
    # í–‰ë™ ë¶„ë¥˜
    buy_threshold = 0.1
    sell_threshold = -0.1
    
    buy_count = sum(1 for a in all_actions if a > buy_threshold)
    sell_count = sum(1 for a in all_actions if a < sell_threshold)
    hold_count = len(all_actions) - buy_count - sell_count
    
    total = len(all_actions)
    buy_ratio = buy_count / total * 100
    sell_ratio = sell_count / total * 100
    hold_ratio = hold_count / total * 100
    
    # í‰ê·  í–‰ë™ ê°•ë„
    avg_action = np.mean(all_actions)
    action_intensity = np.mean(np.abs(all_actions))
    
    return {
        'buy_ratio': buy_ratio,
        'sell_ratio': sell_ratio,
        'hold_ratio': hold_ratio,
        'avg_action': avg_action,
        'intensity': action_intensity,
        'episodes_analyzed': len(recent_episodes)
    }

def log_training_progress(episode, args, episode_rewards, portfolio_values, 
                         info, agent, timer, shares_history):
    """í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹… (ì™„ì „íˆ ìˆ˜ì •ëœ ë²„ì „)"""
    
    # í›ˆë ¨ ì„±ëŠ¥ ê³„ì‚°
    recent_rewards = episode_rewards[-args.log_interval:] if len(episode_rewards) >= args.log_interval else episode_rewards
    recent_portfolios = portfolio_values[-args.log_interval:] if len(portfolio_values) >= args.log_interval else portfolio_values
    recent_shares = shares_history[-args.log_interval:] if len(shares_history) >= args.log_interval else shares_history
    
    avg_reward = np.mean(recent_rewards)
    avg_portfolio = np.mean(recent_portfolios)
    total_return = (avg_portfolio - args.initial_balance) / args.initial_balance * 100
    
    # ì•ˆì „í•œ ì£¼ì‹ ì§€í‘œ ê³„ì‚°
    current_shares = info.get('shares_held', 0)  
    
    if recent_shares:
        avg_shares = np.mean(recent_shares)
        min_shares = np.min(recent_shares)
        max_shares = np.max(recent_shares)
        
        # í¬ì§€ì…˜ íƒ€ì… ê²°ì •
        if abs(current_shares) < 0.001:
            position_type = "í˜„ê¸ˆ í¬ì§€ì…˜"
        elif current_shares > avg_shares * 1.2:
            position_type = "í‰ê·  ëŒ€ë¹„ ë†’ìŒ"
        elif current_shares < avg_shares * 0.8:
            position_type = "í‰ê·  ëŒ€ë¹„ ë‚®ìŒ"
        else:
            position_type = "í‰ê·  ìˆ˜ì¤€"
    else:
        avg_shares = 0
        min_shares = 0
        max_shares = 0
        position_type = "ë°ì´í„° ë¶€ì¡±"
    
    # ì•ˆì „í•œ ì£¼ì‹ ë³€í™” ê³„ì‚° ì£¼ì‹ ë³€í™” ë¡œê·¸
    shares_change = 0
    shares_change_display = "ì£¼ì‹ ë³€í™”: ë°ì´í„° ë¶€ì¡±"
    
    if len(shares_history) >= 2:
        prev_shares = shares_history[-2]
        shares_change = current_shares - prev_shares
        
        # ì•ˆì „í•œ í¼ì„¼íŠ¸ ê³„ì‚°
        MIN_THRESHOLD = 0.001  # 0.001ì£¼ ë¯¸ë§Œì€ 0ìœ¼ë¡œ ê°„ì£¼
        MAX_PERCENT = 1000.0   # 1000% ì´ˆê³¼ ì‹œ ì œí•œ
        
        if abs(prev_shares) >= MIN_THRESHOLD:
            shares_change_percent = (shares_change / abs(prev_shares)) * 100
            # í¼ì„¼íŠ¸ ì œí•œ ì ìš©
            shares_change_percent = max(-MAX_PERCENT, min(MAX_PERCENT, shares_change_percent))
            
            if abs(shares_change_percent) >= MAX_PERCENT:
                if shares_change > 0:
                    shares_change_display = f"ì£¼ì‹ ë³€í™”: +{shares_change:.4f} (ëŒ€í­ ì¦ê°€) ğŸ“ˆ"
                else:
                    shares_change_display = f"ì£¼ì‹ ë³€í™”: {shares_change:.4f} (ëŒ€í­ ê°ì†Œ) ğŸ“‰"
            else:
                if shares_change > 0:
                    shares_change_display = f"ì£¼ì‹ ë³€í™”: +{shares_change:.4f} (+{shares_change_percent:.2f}%) ğŸ“ˆ"
                elif shares_change < 0:
                    shares_change_display = f"ì£¼ì‹ ë³€í™”: {shares_change:.4f} ({shares_change_percent:.2f}%) ğŸ“‰"
                else:
                    shares_change_display = f"ì£¼ì‹ ë³€í™”: {shares_change:.4f} (0.00%) â¡ï¸"
        else:
            # ì´ì „ ë³´ìœ ëŸ‰ì´ ê±°ì˜ 0ì¸ ê²½ìš°
            if current_shares > 0.001:
                shares_change_display = f"ì£¼ì‹ ë³€í™”: +{shares_change:.4f} (ì‹ ê·œ ë§¤ìˆ˜) ğŸ“ˆ"
            else:
                shares_change_display = f"ì£¼ì‹ ë³€í™”: {shares_change:.4f} (ë³€í™”ì—†ìŒ) â¡ï¸"
    
    # ì‹œê°„ ì •ë³´
    elapsed_time = timer.get_training_time()
    avg_episode_time = timer.get_avg_episode_time()
    eta = timer.get_eta(episode, args.num_episodes)
    progress = episode / args.num_episodes * 100
    
    # ì•ˆì •í™” í‘œì‹œ
    stabilization_indicator = " ì•ˆì •í™” " if hasattr(agent, 'stabilization_enabled') and agent.stabilization_enabled else ""
    
    # ë¡œê·¸ ì¶œë ¥
    LOGGER.info("=" * 80)
    LOGGER.info(f"EPISODE {episode+1:,}/{args.num_episodes:,} | ì§„í–‰ë¥ : {progress:.1f}%{stabilization_indicator}")
    LOGGER.info("=" * 80)
    
    # ì‹œê°„ ì •ë³´
    LOGGER.info(f"â±ì‹œê°„ ì •ë³´:")
    LOGGER.info(f"   â””â”€ ê²½ê³¼ ì‹œê°„: {timer.format_time(elapsed_time)}")
    LOGGER.info(f"   â””â”€ í‰ê·  ì—í”¼ì†Œë“œ ì‹œê°„: {avg_episode_time:.2f}ì´ˆ")
    LOGGER.info(f"   â””â”€ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {timer.format_time(eta)}")
    
    # í›ˆë ¨ ì„±ëŠ¥
    LOGGER.info(f"í›ˆë ¨ ì„±ëŠ¥ (ìµœê·¼ {len(recent_rewards)}ê°œ ì—í”¼ì†Œë“œ):")
    LOGGER.info(f"   â””â”€ í‰ê·  ë³´ìƒ: {avg_reward:.4f}")
    LOGGER.info(f"   â””â”€ í‰ê·  í¬íŠ¸í´ë¦¬ì˜¤: ${avg_portfolio:,.2f}")
    current_balance = info.get('balance', 0)  
    LOGGER.info(f"   â””â”€ í˜„ì¬ í˜„ê¸ˆ: ${current_balance:,.2f}")
    
    # âœ… í†µí•©ëœ ì£¼ì‹ ì •ë³´ (ì¤‘ë³µ ì œê±°)
    LOGGER.info(f"   â””â”€ í˜„ì¬ ë³´ìœ  ì£¼ì‹: {current_shares:.4f}")
    if recent_shares:
        LOGGER.info(f"   â””â”€ ìµœê·¼ {len(recent_shares)}íšŒ í‰ê· : {avg_shares:.4f} (ë²”ìœ„: {min_shares:.2f}~{max_shares:.2f})")
        LOGGER.info(f"   â””â”€ í¬ì§€ì…˜ ìƒíƒœ: {position_type}")
    else:
        LOGGER.info(f"   â””â”€ ë³´ìœ ëŸ‰ íˆìŠ¤í† ë¦¬: ë°ì´í„° ë¶€ì¡±")
    
    # ì•ˆì „í•œ ì£¼ì‹ ë³€í™” í‘œì‹œ
    LOGGER.info(f"   â””â”€ {shares_change_display}")
    LOGGER.info(f"   â””â”€ ìˆ˜ìµë¥ : {total_return:.2f}%")
    
    # âœ… í•™ìŠµ í†µê³„ (ì•ˆì „í•œ ì†ì„± ì ‘ê·¼)
    if hasattr(agent, 'actor_losses') and len(agent.actor_losses) > 0:
        LOGGER.info(f"í•™ìŠµ í†µê³„:")
        LOGGER.info(f"   â””â”€ Actor Loss: {agent.actor_losses[-1]:.6f}")
        if hasattr(agent, 'critic_losses') and len(agent.critic_losses) > 0:
            LOGGER.info(f"   â””â”€ Critic Loss: {agent.critic_losses[-1]:.6f}")
        if hasattr(agent, 'alpha'):
            LOGGER.info(f"   â””â”€ Alpha: {agent.alpha.item():.6f}")
        if hasattr(agent, 'replay_buffer'):
            LOGGER.info(f"   â””â”€ ë²„í¼ í¬ê¸°: {len(agent.replay_buffer):,}")
    
    # í–‰ë™ íŒ¨í„´ ì •ë³´ (ì„ íƒì‚¬í•­)
    global episode_actions_history
    action_pattern = analyze_recent_actions(episode_actions_history, args.log_interval)
    
    if action_pattern:
        LOGGER.info(f"âœ… í–‰ë™ íŒ¨í„´ (ìµœê·¼ {action_pattern['episodes_analyzed']}ê°œ ì—í”¼ì†Œë“œ):")
        LOGGER.info(f"   â””â”€ ë§¤ìˆ˜ {action_pattern['buy_ratio']:.1f}% | ë§¤ë„ {action_pattern['sell_ratio']:.1f}% | í™€ë“œ {action_pattern['hold_ratio']:.1f}%")
        LOGGER.info(f"   â””â”€ í‰ê·  í–‰ë™ê°’: {action_pattern['avg_action']:+.3f} | í–‰ë™ ê°•ë„: {action_pattern['intensity']:.3f}")
        
        # ì§€ë°°ì  í–‰ë™ í‘œì‹œ
        if action_pattern['buy_ratio'] > 40:
            LOGGER.info(f"   â””â”€âœ… ì„±í–¥: ì ê·¹ì  ë§¤ìˆ˜ ì„±í–¥")
        elif action_pattern['sell_ratio'] > 40:
            LOGGER.info(f"   â””â”€âœ… ì„±í–¥: ì ê·¹ì  ë§¤ë„ ì„±í–¥")
        elif action_pattern['hold_ratio'] > 60:
            LOGGER.info(f"   â””â”€âœ… ì„±í–¥: ë³´ìˆ˜ì  í™€ë“œ ì„±í–¥")
        else:
            LOGGER.info(f"   â””â”€âœ… ì„±í–¥: ê· í˜•ì  ê±°ë˜ ì„±í–¥")
    
    # ì•ˆì •í™” ê¸°ëŠ¥ ì •ë³´ (ì•ˆì „ ì²´í¬ ì¶”ê°€)
    if hasattr(agent, 'stabilization_enabled') and agent.stabilization_enabled:
        LOGGER.info(f"âœ… ì•ˆì •í™” ìƒíƒœ:")
        LOGGER.info(f"   â””â”€ ì´ í›ˆë ¨ ìŠ¤í…: {agent.train_step_counter:,}")
        LOGGER.info(f"   â””â”€ ì›Œë°ì—… ì™„ë£Œ: {'âœ…' if agent.train_step_counter >= agent.warmup_steps else f'âŒ ({agent.train_step_counter}/{agent.warmup_steps})'}")
        LOGGER.info(f"   â””â”€ ì—…ë°ì´íŠ¸ ë¹ˆë„: ë§¤ {agent.update_frequency} ìŠ¤í…")
        if hasattr(agent, 'reward_history') and agent.reward_history:
            LOGGER.info(f"   â””â”€ ë³´ìƒ ì •ê·œí™”: {len(agent.reward_history)} ìƒ˜í”Œ (í‰ê· : {agent.reward_mean:.4f}, í‘œì¤€í¸ì°¨: {agent.reward_std:.4f})")
    
    LOGGER.info("=" * 80)

def main():
    """ë©”ì¸ í•¨ìˆ˜ (ë°ì´í„° ì†ì‹¤ ë°©ì§€ ê°œì„  ë²„ì „)"""
    timer = TrainingTimer()
    timer.start_training()
    
    print('=' * 50)
    LOGGER.info('SAC ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë°ì´í„° ì†ì‹¤ ë°©ì§€ ê°œì„  ë²„ì „)')
    
    # ì¸ì íŒŒì‹±
    args = parse_args()
    
    # ì‹¬ë³¼ ëª©ë¡ ì„¤ì •
    symbols = args.symbols if args.symbols else TARGET_SYMBOLS
    
    LOGGER.info(f"í•™ìŠµ ëŒ€ìƒ ì‹¬ë³¼: {symbols}")
    LOGGER.info(f"í•™ìŠµ ì„¤ì •:")
    LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ìˆ˜: {args.num_episodes:,}")
    LOGGER.info(f"   â””â”€ ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    LOGGER.info(f"   â””â”€ ìœˆë„ìš° í¬ê¸°: {args.window_size}")
    LOGGER.info(f"   â””â”€ ì´ˆê¸° ìë³¸ê¸ˆ: ${args.initial_balance:,.2f}")
    LOGGER.info(f"   â””â”€ ìµœëŒ€ ì—í”¼ì†Œë“œ ìŠ¤í…: {args.max_steps}")
    
    # âœ… ìˆœì°¨ì  í•™ìŠµ ê´€ë ¨ ì„¤ì • ë¡œê·¸
    LOGGER.info(f"âœ… ìˆœì°¨ì  í•™ìŠµ ì„¤ì •:")
    LOGGER.info(f"   â””â”€ ì ì‘í˜• ê¸¸ì´: {getattr(args, 'adaptive_episode_length', True)}")
    LOGGER.info(f"   â””â”€ ìµœì†Œ ì—í”¼ì†Œë“œ ê¸¸ì´: {getattr(args, 'min_episode_steps', 100)}")
    LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ê²¹ì¹¨: {getattr(args, 'episode_overlap_ratio', 0.0):.1%}")
    LOGGER.info(f"   â””â”€ ë°ì´í„° ì†ì‹¤ ë°©ì§€: {'í™œì„±í™”' if getattr(args, 'force_full_coverage', False) else 'ê¸°ë³¸ê°’'}")
    
    # ì•ˆì •í™” ê¸°ëŠ¥ ì„¤ì • ë¡œê·¸
    if args.use_stabilization or args.stabilization_preset:
        LOGGER.info(f"âœ… ì•ˆì •í™” ê¸°ëŠ¥:")
        if args.stabilization_preset:
            LOGGER.info(f"   â””â”€ í”„ë¦¬ì…‹: {args.stabilization_preset}")
        else:
            LOGGER.info(f"   â””â”€ ì»¤ìŠ¤í…€ ì„¤ì •")
    else:
        LOGGER.info("âœ… ê¸°ë³¸ SAC ëª¨ë“œ (ì•ˆì •í™” ê¸°ëŠ¥ ë¹„í™œì„±í™”)")
    
    # ë°ì´í„° ìˆ˜ì§‘
    LOGGER.info("ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    collector = DataCollector(symbols=symbols)
    
    if args.collect_data:
        LOGGER.info("ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        data = collector.load_and_save()
    else:
        LOGGER.info("ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì¤‘...")
        data = collector.load_all_data()
        
        if not data:
            LOGGER.warning("ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ì–´ ìƒˆë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
            data = collector.load_and_save()
    
    if not data:
        LOGGER.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return
    
    LOGGER.info(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(data)}ê°œ ì‹¬ë³¼")
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    LOGGER.info("ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    processor = DataProcessor(window_size=args.window_size)
    results = processor.process_all_symbols(data)
    
    if not results:
        LOGGER.error("ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨")
        return
    
    LOGGER.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ì‹¬ë³¼")
    
    # âœ… ê°œì„ ëœ í™˜ê²½ ìƒì„± (í›ˆë ¨ìš©ë§Œ)
    train_env = create_training_environment(results, symbols, args)
    
    if train_env is None:
        LOGGER.error("í›ˆë ¨ í™˜ê²½ ìƒì„± ì‹¤íŒ¨")
        return
    
    # âœ… í™˜ê²½ ê²€ì¦ (ë°ì´í„° ì†ì‹¤ ìœ„í—˜ ì‚¬ì „ ì ê²€)
    validation_success = validate_training_environment(train_env, args)
    if not validation_success:
        LOGGER.error("í™˜ê²½ ê²€ì¦ ì‹¤íŒ¨")
        return
    
    # ì—ì´ì „íŠ¸ ìƒì„± (ì•ˆì •í™” ê¸°ëŠ¥ í¬í•¨)
    agent = create_agent(train_env, args)

    if agent is None:
        LOGGER.error("ì—ì´ì „íŠ¸ ìƒì„± ì‹¤íŒ¨")
        return

    # âœ… í•™ìŠµ ì‹¤í–‰ (ê°œì„ ëœ ë²„ì „)
    LOGGER.info("=" * 60)
    LOGGER.info("ê°œì„ ëœ ìˆœì°¨ì  í•™ìŠµ ì‹œì‘")
    LOGGER.info("=" * 60)
    
    episode_rewards, portfolio_values, shares_history = train_agent_sequential(agent, train_env, args, timer)

    # âœ… ë°ì´í„° í™œìš©ë¥  ìƒì„¸ ë¶„ì„
    if hasattr(train_env, 'episode_manager'):
        # í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        episode_manager = train_env.episode_manager
        
        if hasattr(episode_manager, 'get_coverage_summary'):
            final_coverage = episode_manager.get_coverage_summary()
            
            LOGGER.info("=" * 80)
            LOGGER.info("ğŸ“Š ìµœì¢… ìˆœì°¨ì  í•™ìŠµ ì„±ê³¼ ë¶„ì„")
            LOGGER.info("=" * 80)
            LOGGER.info(f"âœ… ë°ì´í„° í™œìš©ë¥ :")
            LOGGER.info(f"   â””â”€ ê³ ìœ  ì»¤ë²„ë¦¬ì§€: {final_coverage['unique_coverage_pct']:.1f}%")
            LOGGER.info(f"   â””â”€ ì´ ì»¤ë²„ë¦¬ì§€: {final_coverage['total_coverage_pct']:.1f}%")
            LOGGER.info(f"   â””â”€ ì²˜ë¦¬ëœ ì—í”¼ì†Œë“œ: {len(episode_rewards)}/{final_coverage['total_episodes']}")
            
            # ì„±ê³¼ ë“±ê¸‰ ë¶€ì—¬
            if final_coverage['unique_coverage_pct'] >= 98:
                grade = "ğŸ¥‡ EXCELLENT"
            elif final_coverage['unique_coverage_pct'] >= 95:
                grade = "ğŸ¥ˆ VERY GOOD"
            elif final_coverage['unique_coverage_pct'] >= 90:
                grade = "ğŸ¥‰ GOOD"
            elif final_coverage['unique_coverage_pct'] >= 80:
                grade = "âš ï¸ FAIR"
            else:
                grade = "âŒ POOR"
            
            LOGGER.info(f"âœ… ë°ì´í„° í™œìš© ë“±ê¸‰: {grade}")
            
            if final_coverage['unique_coverage_pct'] < 95:
                LOGGER.info("ğŸ’¡ ê°œì„  ì œì•ˆ:")
                LOGGER.info("   â””â”€ ë‹¤ìŒ í•™ìŠµ ì‹œ --max_steps ê°’ì„ ëŠ˜ë ¤ë³´ì„¸ìš”")
                LOGGER.info("   â””â”€ --adaptive_episode_length ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”")
                LOGGER.info("   â””â”€ --num_episodes ê°’ì„ ëŠ˜ë ¤ ì „ì²´ ì‚¬ì´í´ì„ ì™„ë£Œí•˜ì„¸ìš”")
            
            LOGGER.info("=" * 80)

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = agent.save_model(
        save_dir="models",
        prefix='',  # ì ‘ë‘ì‚¬ ì—†ìŒ
        model_type=getattr(agent, 'model_type', 'mlp'),
        symbol=symbols[0] if len(symbols) == 1 else None,
        symbols=symbols if len(symbols) > 1 else None
    )

    LOGGER.info(f"ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}")
    
    # âœ… ìµœì¢… ê²°ê³¼ ì¶œë ¥ (ê°œì„ ëœ ë²„ì „)
    total_time = timer.get_training_time()
    final_portfolio = portfolio_values[-1] if portfolio_values else args.initial_balance
    final_return = (final_portfolio - args.initial_balance) / args.initial_balance * 100
    final_shares = shares_history[-1] if shares_history else 0
    
    stabilization_indicator = "ì•ˆì •í™”" if agent.stabilization_enabled else ""
    LOGGER.info("=" * 80)
    LOGGER.info(f"ğŸ‰ í•™ìŠµ ì™„ë£Œ - ìµœì¢… ê²°ê³¼{stabilization_indicator}")
    LOGGER.info("=" * 80)
    LOGGER.info(f"â± í•™ìŠµ ì‹œê°„:")
    LOGGER.info(f"   â””â”€ ì´ í•™ìŠµ ì‹œê°„: {timer.format_time(total_time)}")
    LOGGER.info(f"   â””â”€ í‰ê·  ì—í”¼ì†Œë“œ ì‹œê°„: {timer.get_avg_episode_time():.2f}ì´ˆ")
    LOGGER.info(f"   â””â”€ í•™ìŠµëœ ì—í”¼ì†Œë“œ: {len(episode_rewards):,}ê°œ")
    LOGGER.info("")
    LOGGER.info(f"ğŸ’° í›ˆë ¨ í™˜ê²½ ìµœì¢… ì„±ëŠ¥:")
    LOGGER.info(f"   â””â”€ ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤: ${final_portfolio:,.2f}")
    LOGGER.info(f"   â””â”€ ì´ ìˆ˜ìµë¥ : {final_return:.2f}%")
    LOGGER.info(f"   â””â”€ ìµœì¢… ë³´ìœ  ì£¼ì‹: {final_shares:.4f}")
    LOGGER.info(f"   â””â”€ í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒ: {np.mean(episode_rewards):.4f}")
    LOGGER.info("")
    LOGGER.info(f"ğŸ¤– ìµœì¢… í•™ìŠµ í†µê³„:")
    LOGGER.info(f"   â””â”€ ì´ í•™ìŠµ ìŠ¤í…: {agent.train_step_counter:,}")
    LOGGER.info(f"   â””â”€ ìµœì¢… ë²„í¼ í¬ê¸°: {len(agent.replay_buffer):,}")
    LOGGER.info(f"   â””â”€ ìµœì¢… Alpha ê°’: {agent.alpha.item():.6f}")
    if agent.actor_losses:
        LOGGER.info(f"   â””â”€ ìµœì¢… Actor Loss: {agent.actor_losses[-1]:.6f}")
        LOGGER.info(f"   â””â”€ ìµœì¢… Critic Loss: {agent.critic_losses[-1]:.6f}")
    
    # âœ… ë°ì´í„° í™œìš©ë¥  ìš”ì•½ (ê°„ë‹¨ ë²„ì „)
    if hasattr(train_env, 'episode_manager') and hasattr(train_env.episode_manager, 'get_coverage_summary'):
        coverage = train_env.episode_manager.get_coverage_summary()
        LOGGER.info("")
        LOGGER.info(f"ğŸ“Š ë°ì´í„° í™œìš© ìš”ì•½:")
        LOGGER.info(f"   â””â”€ ê³ ìœ  ë°ì´í„° ì»¤ë²„ë¦¬ì§€: {coverage['unique_coverage_pct']:.1f}%")
        LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ê¸¸ì´ ë²”ìœ„: {coverage['min_episode_length']}~{coverage['max_episode_length']}")
        LOGGER.info(f"   â””â”€ í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´: {coverage['average_episode_length']:.1f}")
        
        if coverage['unique_coverage_pct'] >= 95:
            LOGGER.info(f"   â””â”€ âœ… ìš°ìˆ˜í•œ ë°ì´í„° í™œìš©ë¥ !")
        elif coverage['unique_coverage_pct'] >= 90:
            LOGGER.info(f"   â””â”€ âš ï¸ ì–‘í˜¸í•œ ë°ì´í„° í™œìš©ë¥ ")
        else:
            LOGGER.info(f"   â””â”€ âŒ ê°œì„  í•„ìš”í•œ ë°ì´í„° í™œìš©ë¥ ")
    
    # ì•ˆì •í™” ê¸°ëŠ¥ ìµœì¢… ì •ë³´
    if agent.stabilization_enabled:
        LOGGER.info("")
        LOGGER.info(f"ğŸ›¡ï¸ ì•ˆì •í™” ê¸°ëŠ¥ ìµœì¢… ìƒíƒœ:")
        LOGGER.info(f"   â””â”€ ì›Œë°ì—… ì™„ë£Œ: {'âœ…' if agent.train_step_counter >= agent.warmup_steps else 'âŒ'}")
        LOGGER.info(f"   â””â”€ ì´ ì—…ë°ì´íŠ¸ íšŸìˆ˜: {agent.update_counter}")
        if hasattr(agent, 'reward_history') and agent.reward_history:
            LOGGER.info(f"   â””â”€ ë³´ìƒ ì •ê·œí™” ìƒ˜í”Œ: {len(agent.reward_history)}")
            LOGGER.info(f"   â””â”€ ìµœì¢… ë³´ìƒ í‰ê· : {agent.reward_mean:.4f}")
            LOGGER.info(f"   â””â”€ ìµœì¢… ë³´ìƒ í‘œì¤€í¸ì°¨: {agent.reward_std:.4f}")
    
    LOGGER.info("=" * 80)
    LOGGER.info(f"ğŸ í•™ìŠµ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # âœ… ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    LOGGER.info("")
    LOGGER.info("ğŸ“š ë‹¤ìŒ ë‹¨ê³„:")
    LOGGER.info("   â””â”€ í‰ê°€: run_evaluation.py --model_path [ëª¨ë¸ê²½ë¡œ] --data_type valid")
    LOGGER.info("   â””â”€ ë°±í…ŒìŠ¤íŠ¸: run_backtest.py --model_path [ëª¨ë¸ê²½ë¡œ] --data_type test")
    LOGGER.info("   â””â”€ ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”©: run_realtime_trading.py --model_path [ëª¨ë¸ê²½ë¡œ]")
    
    # ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ (Buy & Hold)
    if len(episode_rewards) > 0:
        LOGGER.info("")
        LOGGER.info(f"ğŸ“ˆ ì„±ê³¼ ë¹„êµ:")
        LOGGER.info(f"   â””â”€ SAC ëª¨ë¸ ìˆ˜ìµë¥ : {final_return:.2f}%")
        
        # Buy & Hold ìˆ˜ìµë¥  ì¶”ì • (ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ í¬íŠ¸í´ë¦¬ì˜¤ ê°’ ê¸°ì¤€)
        if len(portfolio_values) >= 2:
            buy_hold_return = ((portfolio_values[-1] / portfolio_values[0]) - 1) * 100
            LOGGER.info(f"   â””â”€ Buy & Hold ì¶”ì •: {buy_hold_return:.2f}%")
            
            if final_return > buy_hold_return:
                outperformance = final_return - buy_hold_return
                LOGGER.info(f"   â””â”€ âœ… SAC ëª¨ë¸ì´ {outperformance:.2f}%p ë” ìš°ìˆ˜")
            else:
                underperformance = buy_hold_return - final_return
                LOGGER.info(f"   â””â”€ âŒ Buy & Holdê°€ {underperformance:.2f}%p ë” ìš°ìˆ˜")
    # ìµœì¢… ë¦¬ìŠ¤í¬ ê´€ë¦¬ ìš”ì•½ by ë‚˜í˜„
    LOGGER.info(f"ğŸ›¡ï¸ ìµœì¢… ë¦¬ìŠ¤í¬ ê´€ë¦¬ ìš”ì•½:")
    try:
        # í™˜ê²½ì—ì„œ ìµœì¢… ë¦¬ìŠ¤í¬ ì§€í‘œ ê°€ì ¸ì˜¤ê¸°
        if hasattr(train_env, 'get_risk_metrics'):
            final_risk = train_env.get_risk_metrics()
        elif hasattr(train_env, 'base_env') and hasattr(train_env.base_env, 'get_risk_metrics'):
            final_risk = train_env.base_env.get_risk_metrics()
        else:
            final_risk = {}
        
        if final_risk:
            LOGGER.info(f"   â””â”€ ìµœì¢… ìµœëŒ€ ë‚™í­: {final_risk.get('max_drawdown_pct', 0):.2f}%")
            LOGGER.info(f"   â””â”€ ìµœì¢… ì¼ì¼ ì†ì‹¤: {final_risk.get('max_daily_loss_pct', 0):.2f}%")
            LOGGER.info(f"   â””â”€ ìµœê³  í¬íŠ¸í´ë¦¬ì˜¤: ${final_risk.get('peak_portfolio_value', 0):,.2f}")
            
            # ì„í¬íŠ¸ MAX_DRAWDOWN, MAX_DAILY_LOSS ë§¨ ìœ„ë¡œ ì˜¬ë¦¼ ì˜¤ë¥˜ì‹œ ìˆ˜ì •
            drawdown_safety = (MAX_DRAWDOWN * 100 - final_risk.get('max_drawdown_pct', 0)) / (MAX_DRAWDOWN * 100) * 100
            daily_loss_safety = (MAX_DAILY_LOSS * 100 - final_risk.get('max_daily_loss_pct', 0)) / (MAX_DAILY_LOSS * 100) * 100
            
            LOGGER.info(f"   â””â”€ ë‚™í­ ì•ˆì „ ì—¬ìœ ë„: {drawdown_safety:.1f}%")
            LOGGER.info(f"   â””â”€ ì¼ì¼ì†ì‹¤ ì•ˆì „ ì—¬ìœ ë„: {daily_loss_safety:.1f}%")
        else:
            LOGGER.info(f"   â””â”€ ë¦¬ìŠ¤í¬ ì§€í‘œ ìˆ˜ì§‘ ì‹¤íŒ¨")
    except Exception as e:
        LOGGER.warning(f"   â””â”€ ë¦¬ìŠ¤í¬ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")    
    LOGGER.info("=" * 80)
    LOGGER.info("ë°ì´í„° ì†ì‹¤ ìµœì†Œí™” í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()