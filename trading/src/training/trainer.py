"""
SAC ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë„ˆ ëª¨ë“ˆ (SAC ì •ì±… ì¤€ìˆ˜)
"""
import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Union, Optional
from pathlib import Path
import time

from src.config.ea_teb_config import (
    DEVICE,
    BATCH_SIZE,
    NUM_EPISODES,
    EVALUATE_INTERVAL,
    SAVE_MODEL_INTERVAL,
    MAX_STEPS_PER_EPISODE,
    MODELS_DIR,
    RESULTS_DIR,
    LOGGER
)
from src.models.sac_agent import SACAgent
from src.environment.trading_env import TradingEnvironment, MultiAssetTradingEnvironment
from src.utils.utils import create_directory, get_timestamp

class SACTrainer:
    """
    SAC ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤ (SAC ì •ì±… ì¤€ìˆ˜)
    - ìë™ ì—”íŠ¸ë¡œí”¼ íŠœë‹ ì§€ì›
    - í‘œì¤€ SAC í•™ìŠµ ë£¨í”„
    - ë¦¬ìŠ¤í¬ ê´€ë¦¬ í†µí•©
    """
    
    def __init__(
        self,
        agent: SACAgent,
        env: Union[TradingEnvironment, MultiAssetTradingEnvironment],
        valid_env: Optional[Union[TradingEnvironment, MultiAssetTradingEnvironment]] = None,
        batch_size: int = BATCH_SIZE,
        num_episodes: int = NUM_EPISODES,
        evaluate_interval: int = EVALUATE_INTERVAL,
        save_interval: int = SAVE_MODEL_INTERVAL,
        max_steps: int = MAX_STEPS_PER_EPISODE,
        models_dir: Union[str, Path] = MODELS_DIR,
        results_dir: Union[str, Path] = RESULTS_DIR
    ):
        """íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”"""
        self.agent = agent
        self.env = env
        self.valid_env = valid_env
        self.batch_size = batch_size
        self.num_episodes = num_episodes
        self.evaluate_interval = evaluate_interval
        self.save_interval = save_interval
        self.max_steps = max_steps
        self.models_dir = Path(models_dir)
        self.results_dir = Path(results_dir)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        create_directory(self.models_dir)
        create_directory(self.results_dir)
        
        # í•™ìŠµ í†µê³„
        self.episode_rewards: List[float] = []
        self.episode_lengths: List[int] = []
        self.eval_rewards: List[float] = []
        self.train_losses: List[Dict[str, float]] = []
        
        LOGGER.info("âœ… SAC Trainer ì´ˆê¸°í™” ì™„ë£Œ")
        LOGGER.info(f"   ğŸ“Š ì´ ì—í”¼ì†Œë“œ: {num_episodes}")
        LOGGER.info(f"   ğŸ¯ ë°°ì¹˜ í¬ê¸°: {batch_size}")
        LOGGER.info(f"   ğŸ”„ í‰ê°€ ê°„ê²©: {evaluate_interval}")
    
    def train(self) -> Dict[str, List[float]]:
        """
        SAC ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰
        
        Returns:
            í•™ìŠµ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        timestamp = get_timestamp()
        
        LOGGER.info("ğŸš€ SAC í•™ìŠµ ì‹œì‘")
        
        for episode in range(1, self.num_episodes + 1):
            episode_start_time = time.time()
            
            # í™˜ê²½ ë¦¬ì…‹
            state = self.env.reset()
            
            episode_reward = 0.0
            episode_loss = {"actor_loss": 0.0, "critic_loss": 0.0, "alpha_loss": 0.0, "entropy": 0.0}
            episode_steps = 0
            done = False
            
            # ì—í”¼ì†Œë“œ ì§„í–‰
            while not done and episode_steps < self.max_steps:
                # í–‰ë™ ì„ íƒ
                action = self.agent.select_action(state)
                
                # í™˜ê²½ì—ì„œ í•œ ìŠ¤í… ì§„í–‰
                next_state, reward, done, info = self.env.step(action)
                
                # ê²½í—˜ ì €ì¥
                self.agent.add_experience(state, action, reward, next_state, done)
                
                # ëª¨ë¸ ì—…ë°ì´íŠ¸ (ì¶©ë¶„í•œ ê²½í—˜ì´ ìŒ“ì¸ í›„)
                if len(self.agent.replay_buffer) > self.batch_size:
                    loss = self.agent.update_parameters(self.batch_size)
                    
                    # ì†ì‹¤ ëˆ„ì 
                    for k, v in loss.items():
                        episode_loss[k] += v
                
                state = next_state
                episode_reward += reward
                episode_steps += 1
            
            # ì—í”¼ì†Œë“œ í†µê³„ ê¸°ë¡
            self.episode_rewards.append(episode_reward)
            self.episode_lengths.append(episode_steps)
            
            # ì†ì‹¤ í‰ê·  ê³„ì‚°
            if episode_steps > 0:
                for k in episode_loss:
                    episode_loss[k] /= episode_steps
            self.train_losses.append(episode_loss)
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            if episode % 10 == 0:
                avg_reward = np.mean(self.episode_rewards[-10:])
                episode_time = time.time() - episode_start_time
                
                LOGGER.info(
                    f"Episode {episode:4d} | "
                    f"Reward: {episode_reward:8.2f} | "
                    f"Avg(10): {avg_reward:8.2f} | "
                    f"Steps: {episode_steps:3d} | "
                    f"Time: {episode_time:.1f}s"
                )
                
                # ì†ì‹¤ ì •ë³´ (SAC íŠ¹í™”)
                if episode_loss["actor_loss"] > 0:
                    LOGGER.info(
                        f"         Losses -> "
                        f"Actor: {episode_loss['actor_loss']:.4f} | "
                        f"Critic: {episode_loss['critic_loss']:.4f} | "
                        f"Alpha: {episode_loss['alpha_loss']:.4f} | "
                        f"Entropy: {episode_loss['entropy']:.4f}"
                    )
            
            # ì£¼ê¸°ì  í‰ê°€
            if episode % self.evaluate_interval == 0 and self.valid_env:
                eval_reward = self.evaluate()
                self.eval_rewards.append(eval_reward)
                LOGGER.info(f"ğŸ“Š í‰ê°€ ê²°ê³¼ (Episode {episode}): {eval_reward:.2f}")
            
            # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥
            if episode % self.save_interval == 0:
                model_path = self.agent.save_model(
                    save_dir=str(self.models_dir),
                    prefix=f"episode_{episode}_",
                    model_type=getattr(self.agent, 'model_type', 'sac')
                )
                LOGGER.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")
            
            # í•™ìŠµ ê³¡ì„  ì—…ë°ì´íŠ¸
            if episode % 50 == 0:
                self._plot_training_curves(timestamp)
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_model_path = self.agent.save_model(
            save_dir=str(self.models_dir),
            prefix='final_',
            model_type=getattr(self.agent, 'model_type', 'sac')
        )
        
        total_time = time.time() - start_time
        LOGGER.info(f"ğŸ‰ SAC í•™ìŠµ ì™„ë£Œ!")
        LOGGER.info(f"   â±ï¸  ì´ í•™ìŠµ ì‹œê°„: {total_time/3600:.2f}ì‹œê°„")
        LOGGER.info(f"   ğŸ’¾ ìµœì¢… ëª¨ë¸: {final_model_path}")
        
        # ìµœì¢… í•™ìŠµ ê³¡ì„  ì €ì¥
        self._plot_training_curves(timestamp)
        
        return {
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'eval_rewards': self.eval_rewards,
            'train_losses': self.train_losses
        }
    
    def evaluate(self, num_episodes: int = 1) -> float:
        """
        ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
        
        Args:
            num_episodes: í‰ê°€í•  ì—í”¼ì†Œë“œ ìˆ˜
            
        Returns:
            í‰ê·  ë³´ìƒ
        """
        if not self.valid_env:
            LOGGER.warning("ê²€ì¦ í™˜ê²½ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 0.0
        
        total_reward = 0.0
        
        for _ in range(num_episodes):
            state = self.valid_env.reset()
            episode_reward = 0.0
            done = False
            steps = 0
            
            while not done and steps < self.max_steps:
                # í‰ê°€ ëª¨ë“œë¡œ í–‰ë™ ì„ íƒ (íƒí—˜ ì—†ìŒ)
                action = self.agent.select_action(state, evaluate=True)
                next_state, reward, done, _ = self.valid_env.step(action)
                state = next_state
                episode_reward += reward
                steps += 1
            
            total_reward += episode_reward
        
        avg_reward = total_reward / num_episodes
        return avg_reward
    
    def _plot_training_curves(self, timestamp: str) -> None:
        """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
        if not self.episode_rewards:
            return
        
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'SAC Training Progress - {timestamp}', fontsize=16)
            
            # ì—í”¼ì†Œë“œ ë³´ìƒ
            axes[0, 0].plot(self.episode_rewards, alpha=0.6)
            if len(self.episode_rewards) > 10:
                # ì´ë™í‰ê·  ì¶”ê°€
                window = min(50, len(self.episode_rewards) // 10)
                moving_avg = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')
                axes[0, 0].plot(range(window-1, len(self.episode_rewards)), moving_avg, 'r-', linewidth=2)
            axes[0, 0].set_title('Episode Rewards')
            axes[0, 0].set_xlabel('Episode')
            axes[0, 0].set_ylabel('Reward')
            axes[0, 0].grid(True, alpha=0.3)
            
            # ì—í”¼ì†Œë“œ ê¸¸ì´
            axes[0, 1].plot(self.episode_lengths, 'g-', alpha=0.7)
            axes[0, 1].set_title('Episode Lengths')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('Steps')
            axes[0, 1].grid(True, alpha=0.3)
            
            # í‰ê°€ ë³´ìƒ (ìˆëŠ” ê²½ìš°)
            if self.eval_rewards:
                eval_episodes = [i * self.evaluate_interval for i in range(1, len(self.eval_rewards) + 1)]
                axes[1, 0].plot(eval_episodes, self.eval_rewards, 'b-o', linewidth=2, markersize=4)
                axes[1, 0].set_title('Evaluation Rewards')
                axes[1, 0].set_xlabel('Episode')
                axes[1, 0].set_ylabel('Eval Reward')
                axes[1, 0].grid(True, alpha=0.3)
            
            # ì†ì‹¤ (SAC íŠ¹í™”)
            if self.train_losses:
                episodes = list(range(1, len(self.train_losses) + 1))
                actor_losses = [loss.get('actor_loss', 0) for loss in self.train_losses]
                critic_losses = [loss.get('critic_loss', 0) for loss in self.train_losses]
                alpha_losses = [loss.get('alpha_loss', 0) for loss in self.train_losses]
                
                axes[1, 1].plot(episodes, actor_losses, label='Actor Loss', alpha=0.7)
                axes[1, 1].plot(episodes, critic_losses, label='Critic Loss', alpha=0.7)
                axes[1, 1].plot(episodes, alpha_losses, label='Alpha Loss', alpha=0.7)
                axes[1, 1].set_title('SAC Losses')
                axes[1, 1].set_xlabel('Episode')
                axes[1, 1].set_ylabel('Loss')
                axes[1, 1].legend()
                axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ì €ì¥
            plot_path = self.results_dir / f"sac_training_curves_{timestamp}.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            LOGGER.debug(f"ğŸ“Š í•™ìŠµ ê³¡ì„  ì €ì¥: {plot_path}")
            
        except Exception as e:
            LOGGER.warning(f"í•™ìŠµ ê³¡ì„  ìƒì„± ì‹¤íŒ¨: {e}")
    
    def get_training_stats(self) -> Dict[str, float]:
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        if not self.episode_rewards:
            return {}
        
        return {
            'total_episodes': len(self.episode_rewards),
            'mean_reward': np.mean(self.episode_rewards),
            'std_reward': np.std(self.episode_rewards),
            'max_reward': np.max(self.episode_rewards),
            'min_reward': np.min(self.episode_rewards),
            'mean_episode_length': np.mean(self.episode_lengths),
            'total_steps': sum(self.episode_lengths)
        }
