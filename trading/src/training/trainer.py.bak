"""
SAC ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë„ˆ ëª¨ë“ˆ (ì•ˆì •í™” ê¸°ëŠ¥ ì§€ì›)
"""
import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Union, Optional, Any
from pathlib import Path
import time
from tqdm import tqdm
from src.environment.sequential_episode_manager import SequentialEpisodeManager, SequentialTradingEnvironment, create_sequential_training_setup
from src.environment.sequential_episode_manager import create_sequential_training_setup

from src.config.ea_teb_config import (
    DEVICE,
    BATCH_SIZE,
    NUM_EPISODES,
    EVALUATE_INTERVAL,
    SAVE_MODEL_INTERVAL,
    MAX_STEPS_PER_EPISODE,
    MODELS_DIR,
    RESULTS_DIR,
    LOGGER,
    overlap_ratio
)
from src.models.sac_agent import SACAgent  # í†µí•©ëœ SACAgent ì‚¬ìš©
from src.environment.trading_env import TradingEnvironment, MultiAssetTradingEnvironment
from src.utils.utils import create_directory, plot_learning_curve, plot_equity_curve, get_timestamp

class StabilizedTrainer:
    """
    SAC ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤ (ì•ˆì •í™” ê¸°ëŠ¥ ì§€ì›)
    """
    
    def __init__(
        self,
        agent: SACAgent,
        env: Union[TradingEnvironment, MultiAssetTradingEnvironment],
        train_env: Optional[Union[TradingEnvironment, MultiAssetTradingEnvironment]] = None,
        valid_env: Optional[Union[TradingEnvironment, MultiAssetTradingEnvironment]] = None,
        test_env: Optional[Union[TradingEnvironment, MultiAssetTradingEnvironment]] = None,
        batch_size: int = BATCH_SIZE,
        num_episodes: int = NUM_EPISODES,
        evaluate_interval: int = EVALUATE_INTERVAL,
        save_interval: int = SAVE_MODEL_INTERVAL,
        max_steps: int = MAX_STEPS_PER_EPISODE,
        models_dir: Union[str, Path] = MODELS_DIR,
        results_dir: Union[str, Path] = RESULTS_DIR,
        # ì•ˆì •í™” ê´€ë ¨ ì¶”ê°€ íŒŒë¼ë¯¸í„°
        use_early_stopping: bool = False,
        early_stopping_patience: int = 50,
        loss_monitoring_enabled: bool = True,
        performance_monitoring_enabled: bool = True,
        adaptive_learning_rate: bool = False,
        lr_decay_factor: float = 0.95,
        lr_decay_patience: int = 20
    ):
        """
        StabilizedTrainer í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            agent: í•™ìŠµí•  SAC ì—ì´ì „íŠ¸
            env: í•™ìŠµì— ì‚¬ìš©í•  íŠ¸ë ˆì´ë”© í™˜ê²½
            train_env: í›ˆë ¨ í™˜ê²½ (ë³„ë„ ì§€ì • ì‹œ)
            valid_env: ê²€ì¦ í™˜ê²½
            test_env: í…ŒìŠ¤íŠ¸ í™˜ê²½
            batch_size: ë°°ì¹˜ í¬ê¸°
            num_episodes: í•™ìŠµí•  ì´ ì—í”¼ì†Œë“œ ìˆ˜
            evaluate_interval: í‰ê°€ ê°„ê²© (ì—í”¼ì†Œë“œ ë‹¨ìœ„)
            save_interval: ëª¨ë¸ ì €ì¥ ê°„ê²© (ì—í”¼ì†Œë“œ ë‹¨ìœ„)
            max_steps: ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜
            models_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
            results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            
            # ì•ˆì •í™” ê´€ë ¨ íŒŒë¼ë¯¸í„°
            use_early_stopping: ì¡°ê¸° ì¢…ë£Œ ì‚¬ìš© ì—¬ë¶€
            early_stopping_patience: ì¡°ê¸° ì¢…ë£Œ patience
            loss_monitoring_enabled: Loss ëª¨ë‹ˆí„°ë§ í™œì„±í™”
            performance_monitoring_enabled: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í™œì„±í™”
            adaptive_learning_rate: ì ì‘í˜• í•™ìŠµë¥  ì‚¬ìš©
            lr_decay_factor: í•™ìŠµë¥  ê°ì†Œ ê³„ìˆ˜
            lr_decay_patience: í•™ìŠµë¥  ê°ì†Œ patience
        """
        self.agent = agent
        self.env = env
        self.batch_size = batch_size
        self.num_episodes = num_episodes
        self.evaluate_interval = evaluate_interval
        self.save_interval = save_interval
        self.max_steps = max_steps
        self.models_dir = Path(models_dir)
        self.results_dir = Path(results_dir)
        
        self.train_env = train_env
        self.valid_env = valid_env
        self.test_env = test_env
        
        # ì•ˆì •í™” ê´€ë ¨ ì„¤ì •
        self.use_early_stopping = use_early_stopping
        self.early_stopping_patience = early_stopping_patience
        self.loss_monitoring_enabled = loss_monitoring_enabled
        self.performance_monitoring_enabled = performance_monitoring_enabled
        self.adaptive_learning_rate = adaptive_learning_rate
        self.lr_decay_factor = lr_decay_factor
        self.lr_decay_patience = lr_decay_patience
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        create_directory(self.models_dir)
        create_directory(self.results_dir)
        
        # í•™ìŠµ í†µê³„
        self.episode_rewards = []
        self.episode_lengths = []
        self.eval_rewards = []
        self.train_losses = []
        
        # ì•ˆì •í™” ëª¨ë‹ˆí„°ë§ìš© ë³€ìˆ˜
        self.loss_history = []
        self.performance_history = []
        self.best_performance = float('-inf')
        self.best_performance_episode = 0
        self.lr_decay_counter = 0
        self.early_stop_counter = 0
        
        # ë¦¬ìŠ¤í¬ ê´€ë¦¬ í†µê³„
        self.risk_episodes = []  # ë¦¬ìŠ¤í¬ í•œë„ ì´ˆê³¼ ì—í”¼ì†Œë“œ ê¸°ë¡
        self.max_drawdown_episodes = []  # ìµœëŒ€ ë‚™í­ ê¸°ë¡
        self.daily_loss_episodes = []  # ì¼ì¼ ì†ì‹¤ ê¸°ë¡
        # ì•ˆì •í™” ê¸°ëŠ¥ ë¡œê¹…
        stabilization_status = "ì•ˆì •í™” ê¸°ëŠ¥ í¬í•¨" if agent.stabilization_enabled else "ê¸°ë³¸ ë²„ì „"
        LOGGER.info(f"Trainer ì´ˆê¸°í™” ì™„ë£Œ ({stabilization_status}): {num_episodes}ê°œ ì—í”¼ì†Œë“œ, ë°°ì¹˜ í¬ê¸° {batch_size}")
        
        if agent.stabilization_enabled:
            LOGGER.info("âœ… ì¶”ê°€ ì•ˆì •í™” ê¸°ëŠ¥:")
            LOGGER.info(f"   â””â”€ ì¡°ê¸° ì¢…ë£Œ: {'í™œì„±í™”' if use_early_stopping else 'ë¹„í™œì„±í™”'}")
            LOGGER.info(f"   â””â”€ Loss ëª¨ë‹ˆí„°ë§: {'í™œì„±í™”' if loss_monitoring_enabled else 'ë¹„í™œì„±í™”'}")
            LOGGER.info(f"   â””â”€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§: {'í™œì„±í™”' if performance_monitoring_enabled else 'ë¹„í™œì„±í™”'}")
            LOGGER.info(f"   â””â”€ ì ì‘í˜• í•™ìŠµë¥ : {'í™œì„±í™”' if adaptive_learning_rate else 'ë¹„í™œì„±í™”'}")
    
    def train(self) -> Dict[str, List[float]]:
        """
        SAC ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰ (ì•ˆì •í™” ê¸°ëŠ¥ í¬í•¨)
        
        Returns:
            í•™ìŠµ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        timestamp = get_timestamp()
        
        stabilization_info = " (ì•ˆì •í™” ê¸°ëŠ¥)" if self.agent.stabilization_enabled else ""
        LOGGER.info(f"í•™ìŠµ ì‹œì‘{stabilization_info}: {self.num_episodes}ê°œ ì—í”¼ì†Œë“œ")
        
        # ìˆœì°¨ì  í™˜ê²½ ì„¤ì • í™•ì¸ ë° ìƒì„±
        if not hasattr(self.env, 'episode_manager'):
            self.env, self.episode_manager = create_sequential_training_setup(
                self.env, 
                overlap_ratio=overlap_ratio,
                logger=LOGGER
            )
            LOGGER.info("âœ… ìˆœì°¨ì  ì—í”¼ì†Œë“œ í•™ìŠµ ëª¨ë“œ í™œì„±í™”")
        
        for episode in range(1, self.num_episodes + 1):
            episode_start_time = time.time()
            
            # ìˆœì°¨ì  ë¦¬ì…‹ ì§€ì›
            if hasattr(self.env, 'reset') and 'episode_num' in self.env.reset.__code__.co_varnames:
                state = self.env.reset(episode_num=episode - 1)
            else:
                state = self.env.reset()
            
            episode_reward = 0
            episode_loss = {"actor_loss": 0, "critic_loss": 0, "alpha_loss": 0, "entropy": 0}
            episode_steps = 0
            done = False
            
            # ì—í”¼ì†Œë“œ ì§„í–‰
            while not done and episode_steps < self.max_steps:
                # í–‰ë™ ì„ íƒ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ìƒíƒœë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬)
                action = self.agent.select_action(state)
                
                # í™˜ê²½ì—ì„œ í•œ ìŠ¤í… ì§„í–‰
                next_state, reward, done, info = self.env.step(action)
                
                # ê²½í—˜ ì €ì¥ (ì•ˆì •í™” ê¸°ëŠ¥ì´ add_experienceì—ì„œ ìë™ ì²˜ë¦¬ë¨)
                self.agent.add_experience(state, action, reward, next_state, done)
                
                # ëª¨ë¸ ì—…ë°ì´íŠ¸
                if len(self.agent.replay_buffer) > self.batch_size:
                    loss = self.agent.update_parameters(self.batch_size)
                    
                    # ì†ì‹¤ ëˆ„ì 
                    for k, v in loss.items():
                        if k in episode_loss:
                            episode_loss[k] += v
                        else:
                            episode_loss[k] = v  # ìƒˆë¡œìš´ í‚¤ê°€ ìˆì„ ê²½ìš° ì¶”ê°€
                    
                    # ì•ˆì •í™” ê¸°ëŠ¥: Loss ëª¨ë‹ˆí„°ë§
                    if self.agent.stabilization_enabled and self.loss_monitoring_enabled:
                        self._monitor_loss(loss)
                
                state = next_state
                episode_reward += reward
                episode_steps += 1
            
            # ì—í”¼ì†Œë“œ í†µê³„ ê¸°ë¡
            self.episode_rewards.append(episode_reward)
            self.episode_lengths.append(episode_steps)
            
            # ì†ì‹¤ í‰ê·  ê³„ì‚° ë° ê¸°ë¡
            if episode_steps > 0:
                for k in episode_loss:
                    episode_loss[k] /= episode_steps
            self.train_losses.append(episode_loss)
            
            # ì•ˆì •í™” ê¸°ëŠ¥: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            if self.agent.stabilization_enabled and self.performance_monitoring_enabled:
                self._monitor_performance(episode_reward, episode)
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            episode_time = time.time() - episode_start_time
            
            # ì•ˆì •í™” ìƒíƒœ ì •ë³´ í¬í•¨ ë¡œê¹…
            stabilization_status = ""
            if self.agent.stabilization_enabled:
                warmup_status = "âœ…" if self.agent.train_step_counter >= self.agent.warmup_steps else "âŒ"
                stabilization_status = f" | ì›Œë°ì—…: {warmup_status} | í›ˆë ¨ìŠ¤í…: {self.agent.train_step_counter}"
            
            # ë¦¬ìŠ¤í¬ ì •ë³´ ì¶”ê°€
            current_max_drawdown = self.max_drawdown_episodes[-1] if self.max_drawdown_episodes else 0
            current_daily_loss = self.daily_loss_episodes[-1] if self.daily_loss_episodes else 0
            risk_status = f" | ë‚™í­: {current_max_drawdown:.1f}% | ì¼ì¼ì†ì‹¤: {current_daily_loss:.1f}%"
            LOGGER.info(f"ì—í”¼ì†Œë“œ {episode}/{self.num_episodes} - ë³´ìƒ: {episode_reward:.2f}, ìŠ¤í…: {episode_steps}, ì‹œê°„: {episode_time:.2f}ì´ˆ{stabilization_status}")
            
            # ì£¼ê¸°ì  í‰ê°€
            if episode % self.evaluate_interval == 0:
                eval_reward = self.evaluate()
                self.eval_rewards.append(eval_reward)
                LOGGER.info(f"í‰ê°€ ê²°ê³¼ (ì—í”¼ì†Œë“œ {episode}) - ë³´ìƒ: {eval_reward:.2f}")
                
                # ì•ˆì •í™” ê¸°ëŠ¥: ì ì‘í˜• í•™ìŠµë¥ 
                if self.agent.stabilization_enabled and self.adaptive_learning_rate:
                    self._adjust_learning_rate(eval_reward, episode)
            
            # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥
            if episode % self.save_interval == 0:
                model_path = self.agent.save_model(self.models_dir, f"episode_{episode}_")
                LOGGER.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
            
            # ì•ˆì •í™” ê¸°ëŠ¥: ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if self.agent.stabilization_enabled and self.use_early_stopping:
                if self._check_early_stopping(episode):
                    LOGGER.info(f"âœ… ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ì¶©ì¡± (ì—í”¼ì†Œë“œ {episode})")
                    break
            
            # í•™ìŠµ ê³¡ì„  ì—…ë°ì´íŠ¸ (ëœ ìì£¼)
            if episode % 50 == 0:
                self._plot_training_curves(timestamp)

        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_model_path = self.agent.save_model(
            save_dir=str(self.models_dir),
            prefix='',  # ì ‘ë‘ì‚¬ ì—†ìŒ
            model_type=getattr(self.agent, 'model_type', 'mlp'),
            symbol=getattr(self, 'symbol', None),
            symbols=getattr(self, 'symbols', None)
        )
        LOGGER.info(f"ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}")
        
        # ìµœì¢… í•™ìŠµ ê³¡ì„  ì €ì¥
        self._plot_training_curves(timestamp)
        
        # í•™ìŠµ ì‹œê°„ ê³„ì‚°
        total_time = time.time() - start_time
        
        # ìµœì¢… í•™ìŠµ ìš”ì•½
        stabilization_info = " (ì•ˆì •í™” ê¸°ëŠ¥)" if self.agent.stabilization_enabled else ""
        LOGGER.info(f"í•™ìŠµ ì™„ë£Œ{stabilization_info}: ì´ ì‹œê°„ {total_time:.2f}ì´ˆ ({total_time/60:.2f}ë¶„)")
        
        if self.agent.stabilization_enabled:
            LOGGER.info("âœ… ì•ˆì •í™” ê¸°ëŠ¥ ìµœì¢… ìš”ì•½:")
            LOGGER.info(f"   â””â”€ ì´ í›ˆë ¨ ìŠ¤í…: {self.agent.train_step_counter:,}")
            LOGGER.info(f"   â””â”€ ì›Œë°ì—… ì™„ë£Œ: {'âœ…' if self.agent.train_step_counter >= self.agent.warmup_steps else 'âŒ'}")
            LOGGER.info(f"   â””â”€ ìµœê³  ì„±ëŠ¥ ì—í”¼ì†Œë“œ: {self.best_performance_episode}")
            LOGGER.info(f"   â””â”€ ìµœê³  ì„±ëŠ¥ ì ìˆ˜: {self.best_performance:.4f}")
            if self.adaptive_learning_rate and self.lr_decay_counter > 0:
                LOGGER.info(f"   â””â”€ í•™ìŠµë¥  ì¡°ì • íšŸìˆ˜: {self.lr_decay_counter}")

        # í•™ìŠµ í†µê³„ ë°˜í™˜
        return {
            "episode_rewards": self.episode_rewards,
            "episode_lengths": self.episode_lengths,
            "eval_rewards": self.eval_rewards,
            "actor_losses": [loss["actor_loss"] for loss in self.train_losses],
            "critic_losses": [loss["critic_loss"] for loss in self.train_losses],
            "alpha_losses": [loss["alpha_loss"] for loss in self.train_losses],
            "entropy_values": [loss["entropy"] for loss in self.train_losses]
        }
    
    def _monitor_loss(self, loss: Dict[str, float]) -> None:
        """Loss ëª¨ë‹ˆí„°ë§ (ì•ˆì •í™” ê¸°ëŠ¥)"""
        self.loss_history.append(loss)
        
        # ìµœê·¼ loss ì´ìƒê°’ ê°ì§€
        if len(self.loss_history) > 10:
            recent_losses = self.loss_history[-10:]
            avg_actor_loss = np.mean([l.get('actor_loss', 0) for l in recent_losses])
            avg_critic_loss = np.mean([l.get('critic_loss', 0) for l in recent_losses])
            
            # ì„ê³„ê°’ ì„¤ì • (ë™ì ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥)
            actor_threshold = 100.0
            critic_threshold = 100.0
            
            if avg_actor_loss > actor_threshold or avg_critic_loss > critic_threshold:
                LOGGER.warning(f"âœ… ë†’ì€ Loss ê°ì§€: Actor={avg_actor_loss:.4f}, Critic={avg_critic_loss:.4f}")
                
                # ìë™ ëŒ€ì‘ (ì„ íƒì )
                if hasattr(self.agent, 'emergency_lr_reduction') and self.agent.emergency_lr_reduction:
                    self._emergency_lr_reduction()
    
    def _monitor_performance(self, reward: float, episode: int) -> None:
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (ì•ˆì •í™” ê¸°ëŠ¥)"""
        self.performance_history.append(reward)
        
        # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
        if reward > self.best_performance:
            self.best_performance = reward
            self.best_performance_episode = episode
            self.early_stop_counter = 0  # ì¹´ìš´í„° ë¦¬ì…‹
        else:
            self.early_stop_counter += 1
    
    def _adjust_learning_rate(self, current_performance: float, episode: int) -> None:
        """ì ì‘í˜• í•™ìŠµë¥  ì¡°ì • (ì•ˆì •í™” ê¸°ëŠ¥)"""
        if len(self.eval_rewards) < 2:
            return
        
        # ìµœê·¼ ì„±ëŠ¥ ê°œì„ ì´ ì—†ìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ
        recent_performance = np.mean(self.eval_rewards[-3:]) if len(self.eval_rewards) >= 3 else current_performance
        prev_performance = np.mean(self.eval_rewards[-6:-3]) if len(self.eval_rewards) >= 6 else self.eval_rewards[0]
        
        if recent_performance <= prev_performance:
            self.lr_decay_counter += 1
            
            if self.lr_decay_counter >= self.lr_decay_patience:
                # í•™ìŠµë¥  ê°ì†Œ
                for param_group in self.agent.actor_optimizer.param_groups:
                    param_group['lr'] *= self.lr_decay_factor
                for param_group in self.agent.critic_optimizer.param_groups:
                    param_group['lr'] *= self.lr_decay_factor
                
                new_actor_lr = self.agent.actor_optimizer.param_groups[0]['lr']
                new_critic_lr = self.agent.critic_optimizer.param_groups[0]['lr']
                
                LOGGER.info(f"âœ… í•™ìŠµë¥  ì¡°ì •ë¨ (ì—í”¼ì†Œë“œ {episode}): Actor={new_actor_lr:.6f}, Critic={new_critic_lr:.6f}")
                self.lr_decay_counter = 0
        else:
            self.lr_decay_counter = 0
    
    def _emergency_lr_reduction(self) -> None:
        """ì‘ê¸‰ í•™ìŠµë¥  ê°ì†Œ (ì•ˆì •í™” ê¸°ëŠ¥)"""
        emergency_factor = 0.1
        
        for param_group in self.agent.actor_optimizer.param_groups:
            param_group['lr'] *= emergency_factor
        for param_group in self.agent.critic_optimizer.param_groups:
            param_group['lr'] *= emergency_factor
        
        LOGGER.warning(f"âŒ ì‘ê¸‰ í•™ìŠµë¥  ê°ì†Œ ì ìš©: {emergency_factor}ë°° ê°ì†Œ")
    
    def _check_early_stopping(self, episode: int) -> bool:
        """ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸ (ì•ˆì •í™” ê¸°ëŠ¥)"""
        if self.early_stop_counter >= self.early_stopping_patience:
            LOGGER.info(f"ì¡°ê¸° ì¢…ë£Œ: {self.early_stopping_patience} ì—í”¼ì†Œë“œ ë™ì•ˆ ì„±ëŠ¥ ê°œì„  ì—†ìŒ")
            return True
        
        # ì¶”ê°€ ì¡°ê±´: ë§¤ìš° ë‚®ì€ ì„±ëŠ¥ì´ ì§€ì†ë˜ëŠ” ê²½ìš°
        if len(self.episode_rewards) >= 100:
            recent_avg = np.mean(self.episode_rewards[-100:])
            if recent_avg < -1000:  # ì„ê³„ê°’ ì„¤ì •
                LOGGER.info(f"ì¡°ê¸° ì¢…ë£Œ: ìµœê·¼ ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ìŒ (í‰ê· : {recent_avg:.2f})")
                return True
        
        return False
    
    def evaluate(self, num_episodes: int = 1) -> float:
        """
        í˜„ì¬ ì •ì±… í‰ê°€ (ì•ˆì •í™” ê¸°ëŠ¥ í¬í•¨)

        Args:
            num_episodes: í‰ê°€í•  ì—í”¼ì†Œë“œ ìˆ˜

        Returns:
            í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒ
        """
        total_reward = 0
        env = self.train_env if self.train_env is not None else self.env

        for _ in range(num_episodes):
            state = env.reset()
            episode_reward = 0
            done = False

            while not done:
                # í‰ê°€ ëª¨ë“œë¡œ í–‰ë™ ì„ íƒ
                action = self.agent.select_action(state, evaluate=True)
                next_state, reward, done, _ = env.step(action)
                state = next_state
                episode_reward += reward

            total_reward += episode_reward

        avg_reward = total_reward / num_episodes
        
        # ì•ˆì •í™” ê¸°ëŠ¥: í‰ê°€ ê²°ê³¼ ëª¨ë‹ˆí„°ë§
        if self.agent.stabilization_enabled and self.performance_monitoring_enabled:
            self._monitor_performance(avg_reward, len(self.eval_rewards) + 1)
        
        return avg_reward

    def train_with_sequential_episodes(self) -> Dict[str, List[float]]:
        """
        ìˆœì°¨ì  ì—í”¼ì†Œë“œ ë°ì´í„° í™œìš© í•™ìŠµ ìˆ˜í–‰ (ì•ˆì •í™” ê¸°ëŠ¥ í¬í•¨)
        
        Returns:
            í•™ìŠµ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        timestamp = get_timestamp()
        
        # ìˆœì°¨ì  í™˜ê²½ ì„¤ì • ìƒì„±
        if not hasattr(self.env, 'episode_manager'):
            from sequential_episode_manager import create_sequential_training_setup
            self.env, self.episode_manager = create_sequential_training_setup(
                self.env, 
                overlap_ratio=overlap_ratio,  # ì•ˆê²¹ì¹¨
                logger=LOGGER
            )
            LOGGER.info("âœ… ìˆœì°¨ì  ì—í”¼ì†Œë“œ í•™ìŠµ ëª¨ë“œ í™œì„±í™”")
        
        stabilization_info = " (ì•ˆì •í™” ê¸°ëŠ¥)" if self.agent.stabilization_enabled else ""
        LOGGER.info(f"í•™ìŠµ ì‹œì‘{stabilization_info}: {self.num_episodes}ê°œ ì—í”¼ì†Œë“œ (ìˆœì°¨ì  ë°ì´í„° í™œìš©)")
        
        for episode in range(1, self.num_episodes + 1):
            episode_start_time = time.time()
            
            # ìˆœì°¨ì  ë¦¬ì…‹ (ì—í”¼ì†Œë“œ ë²ˆí˜¸ ì „ë‹¬)
            state = self.env.reset(episode_num=episode - 1)
            
            episode_reward = 0
            episode_loss = {"actor_loss": 0, "critic_loss": 0, "alpha_loss": 0, "entropy": 0}
            episode_steps = 0
            done = False
            
            # ì—í”¼ì†Œë“œ ë©”íƒ€ì •ë³´ ë¡œê¹…
            if episode <= 5 or episode % 20 == 0:
                meta = getattr(self.env, 'episode_meta_info', {})
                LOGGER.info(
                    f"âœ… Episode {episode}: ë°ì´í„° ë²”ìœ„ "
                    f"[{meta.get('start_index', 'N/A')}~{meta.get('end_index', 'N/A')}] "
                    f"({meta.get('actual_length', 'N/A')} steps)"
                )
            
            # ì—í”¼ì†Œë“œ ì§„í–‰
            while not done:
                # í–‰ë™ ì„ íƒ
                action = self.agent.select_action(state)
                
                # í™˜ê²½ì—ì„œ í•œ ìŠ¤í… ì§„í–‰
                next_state, reward, done, info = self.env.step(action)
                
                # ê²½í—˜ ì €ì¥ (ì•ˆì •í™” ê¸°ëŠ¥ì´ add_experienceì—ì„œ ìë™ ì²˜ë¦¬ë¨)
                self.agent.add_experience(state, action, reward, next_state, done)
                
                # ëª¨ë¸ ì—…ë°ì´íŠ¸
                if len(self.agent.replay_buffer) > self.batch_size:
                    loss = self.agent.update_parameters(self.batch_size)
                    
                    # ì†ì‹¤ ëˆ„ì 
                    for k, v in loss.items():
                        if k in episode_loss:
                            episode_loss[k] += v
                        else:
                            episode_loss[k] = v
                    
                    # ì•ˆì •í™” ê¸°ëŠ¥: Loss ëª¨ë‹ˆí„°ë§
                    if self.agent.stabilization_enabled and self.loss_monitoring_enabled:
                        self._monitor_loss(loss)
                
                state = next_state
                episode_reward += reward
                episode_steps += 1
                # ë¦¬ìŠ¤í¬ ì •ë³´ ìˆ˜ì§‘
                if 'risk_limit_exceeded' in info and info['risk_limit_exceeded']:
                    self.risk_episodes.append({
                        'episode': episode,
                        'step': episode_steps,
                        'max_drawdown_pct': info.get('max_drawdown_pct', 0),
                        'daily_loss_pct': info.get('daily_loss_pct', 0),
                        'portfolio_value': info.get('portfolio_value', 0)
                    })
                    LOGGER.warning(f"ğŸš¨ ì—í”¼ì†Œë“œ {episode} ë¦¬ìŠ¤í¬ í•œë„ ì´ˆê³¼ë¡œ ì¡°ê¸° ì¢…ë£Œ")
                
                # ë‚™í­/ì†ì‹¤ í†µê³„ ê¸°ë¡
                self.max_drawdown_episodes.append(info.get('max_drawdown_pct', 0))
                self.daily_loss_episodes.append(info.get('daily_loss_pct', 0))
                
                # ì•ˆì „ì¥ì¹˜: ë„ˆë¬´ ê¸´ ì—í”¼ì†Œë“œ ë°©ì§€
                if episode_steps >= self.max_steps:
                    LOGGER.warning(f"Episode {episode}: ìµœëŒ€ ìŠ¤í… ë„ë‹¬ë¡œ ê°•ì œ ì¢…ë£Œ")
                    break
            
            # ì—í”¼ì†Œë“œ í†µê³„ ê¸°ë¡
            self.episode_rewards.append(episode_reward)
            self.episode_lengths.append(episode_steps)
            
            # ì†ì‹¤ í‰ê·  ê³„ì‚° ë° ê¸°ë¡
            if episode_steps > 0:
                for k in episode_loss:
                    episode_loss[k] /= episode_steps
            self.train_losses.append(episode_loss)
            
            # ì•ˆì •í™” ê¸°ëŠ¥: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            if self.agent.stabilization_enabled and self.performance_monitoring_enabled:
                self._monitor_performance(episode_reward, episode)
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            episode_time = time.time() - episode_start_time
            
            # ë°ì´í„° í™œìš©ë¥  ê³„ì‚°
            meta = getattr(self.env, 'episode_meta_info', {})
            data_utilization = meta.get('data_utilization', episode_steps / self.max_steps)
            
            # ì•ˆì •í™” ìƒíƒœ ì •ë³´ í¬í•¨
            stabilization_status = ""
            if self.agent.stabilization_enabled:
                warmup_status = "âœ…" if self.agent.train_step_counter >= self.agent.warmup_steps else "âŒ"
                stabilization_status = f" | ì›Œë°ì—…: {warmup_status}"
            
            LOGGER.info(
                f"Episode {episode}/{self.num_episodes} - "
                f"ë³´ìƒ: {episode_reward:.2f}, "
                f"ìŠ¤í…: {episode_steps}, "
                f"ë°ì´í„°í™œìš©: {data_utilization:.1%}, "
                f"ì‹œê°„: {episode_time:.2f}ì´ˆ{stabilization_status}"
            )
            
            # ì‚¬ì´í´ ì™„ë£Œ ë¡œê¹…
            if meta.get('is_last_in_cycle', False):
                coverage_summary = self.episode_manager.get_coverage_summary()
                LOGGER.info(f"âœ… ë°ì´í„° ì‚¬ì´í´ ì™„ë£Œ - ê³ ìœ  ì»¤ë²„ë¦¬ì§€: {coverage_summary['unique_coverage_pct']:.1f}%")
            
            # ì£¼ê¸°ì  í‰ê°€
            if episode % self.evaluate_interval == 0:
                eval_reward = self.evaluate()
                self.eval_rewards.append(eval_reward)
                LOGGER.info(f"í‰ê°€ ê²°ê³¼ (ì—í”¼ì†Œë“œ {episode}) - ë³´ìƒ: {eval_reward:.2f}")
                
                # ì•ˆì •í™” ê¸°ëŠ¥: ì ì‘í˜• í•™ìŠµë¥ 
                if self.agent.stabilization_enabled and self.adaptive_learning_rate:
                    self._adjust_learning_rate(eval_reward, episode)
            
            # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥
            if episode % self.save_interval == 0:
                model_path = self.agent.save_model(self.models_dir, f"episode_{episode}_")
                LOGGER.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
            
            # ì•ˆì •í™” ê¸°ëŠ¥: ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if self.agent.stabilization_enabled and self.use_early_stopping:
                if self._check_early_stopping(episode):
                    LOGGER.info(f"âœ… ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ì¶©ì¡± (ì—í”¼ì†Œë“œ {episode})")
                    break
        
        # ìµœì¢… ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸
        self._log_final_coverage_report()
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_model_path = self.agent.save_model(
            save_dir=str(self.models_dir),
            prefix='',
            model_type=getattr(self.agent, 'model_type', 'mlp'),
            symbol=getattr(self, 'symbol', None),
            symbols=getattr(self, 'symbols', None)
        )
        LOGGER.info(f"ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}")
        
        # í•™ìŠµ ì‹œê°„ ê³„ì‚°
        total_time = time.time() - start_time
        
        # ìµœì¢… ìš”ì•½
        stabilization_info = " (ì•ˆì •í™” ê¸°ëŠ¥)" if self.agent.stabilization_enabled else ""
        LOGGER.info(f"ìˆœì°¨ì  í•™ìŠµ ì™„ë£Œ{stabilization_info}: ì´ ì‹œê°„ {total_time:.2f}ì´ˆ ({total_time/60:.2f}ë¶„)")
        
        if self.agent.stabilization_enabled:
            LOGGER.info("âœ… ì•ˆì •í™” ê¸°ëŠ¥ ìµœì¢… ìš”ì•½:")
            LOGGER.info(f"   â””â”€ ì´ í›ˆë ¨ ìŠ¤í…: {self.agent.train_step_counter:,}")
            LOGGER.info(f"   â””â”€ ì›Œë°ì—… ì™„ë£Œ: {'âœ…' if self.agent.train_step_counter >= self.agent.warmup_steps else 'âŒ'}")
            LOGGER.info(f"   â””â”€ ìµœê³  ì„±ëŠ¥ ì—í”¼ì†Œë“œ: {self.best_performance_episode}")
            LOGGER.info(f"   â””â”€ ìµœê³  ì„±ëŠ¥ ì ìˆ˜: {self.best_performance:.4f}")

        return {
            "episode_rewards": self.episode_rewards,
            "episode_lengths": self.episode_lengths,
            "eval_rewards": self.eval_rewards,
            "actor_losses": [loss["actor_loss"] for loss in self.train_losses],
            "critic_losses": [loss["critic_loss"] for loss in self.train_losses],
            "alpha_losses": [loss["alpha_loss"] for loss in self.train_losses],
            "entropy_values": [loss["entropy"] for loss in self.train_losses]
        }

    def _log_final_coverage_report(self):
        """ìµœì¢… ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸ ë¡œê¹…"""
        if hasattr(self, 'episode_manager'):
            coverage_summary = self.episode_manager.get_coverage_summary()
            LOGGER.info("âœ… ìµœì¢… ë°ì´í„° ì»¤ë²„ë¦¬ì§€:")
            LOGGER.info(f"   â””â”€ ê³ ìœ  ì»¤ë²„ë¦¬ì§€: {coverage_summary['unique_coverage_pct']:.1f}%")
            LOGGER.info(f"   â””â”€ ì´ ì—í”¼ì†Œë“œ: {coverage_summary['total_episodes']}")
            LOGGER.info(f"   â””â”€ ë°ì´í„° ê¸¸ì´: {coverage_summary['usable_length']:,}")

    def _plot_training_curves(self, timestamp: str) -> None:
        """
        í•™ìŠµ ê³¡ì„  ì‹œê°í™” ë° ì €ì¥ (ì•ˆì •í™” ì •ë³´ í¬í•¨)
        
        Args:
            timestamp: íŒŒì¼ëª…ì— ì‚¬ìš©í•  íƒ€ì„ìŠ¤íƒ¬í”„
        """
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        stabilization_suffix = "_stabilized" if self.agent.stabilization_enabled else ""
        result_dir = self.results_dir / f"training_{timestamp}{stabilization_suffix}"
        create_directory(result_dir)
        
        # ì—í”¼ì†Œë“œ ë³´ìƒ ê³¡ì„ 
        plt.figure(figsize=(12, 8))
        plt.subplot(2, 2, 1)
        plt.plot(self.episode_rewards, alpha=0.6, label='Episode Rewards')
        if len(self.episode_rewards) > 10:
            # ì´ë™ í‰ê·  ì¶”ê°€
            window = min(50, len(self.episode_rewards) // 10)
            moving_avg = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')
            plt.plot(range(window-1, len(self.episode_rewards)), moving_avg, 'r-', label=f'Moving Avg ({window})')
        
        # ì•ˆì •í™” ê¸°ëŠ¥ ì •ë³´ í‘œì‹œ
        if self.agent.stabilization_enabled and self.best_performance_episode > 0:
            plt.axvline(x=self.best_performance_episode, color='g', linestyle='--', label=f'Best Performance (Ep {self.best_performance_episode})')
        
        plt.title('ì—í”¼ì†Œë“œ ë³´ìƒ')
        plt.xlabel('ì—í”¼ì†Œë“œ')
        plt.ylabel('ë³´ìƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ì—í”¼ì†Œë“œ ê¸¸ì´ ê³¡ì„ 
        plt.subplot(2, 2, 2)
        plt.plot(self.episode_lengths)
        plt.title('ì—í”¼ì†Œë“œ ê¸¸ì´')
        plt.xlabel('ì—í”¼ì†Œë“œ')
        plt.ylabel('ìŠ¤í… ìˆ˜')
        plt.grid(True, alpha=0.3)
        
        # ì†ì‹¤ ê³¡ì„ ë“¤
        if self.train_losses:
            # Actor ì†ì‹¤
            plt.subplot(2, 2, 3)
            actor_losses = [loss["actor_loss"] for loss in self.train_losses]
            plt.plot(actor_losses)
            plt.title('Actor ì†ì‹¤')
            plt.xlabel('ì—í”¼ì†Œë“œ')
            plt.ylabel('ì†ì‹¤')
            plt.grid(True, alpha=0.3)
            
            # Critic ì†ì‹¤
            plt.subplot(2, 2, 4)
            critic_losses = [loss["critic_loss"] for loss in self.train_losses]
            plt.plot(critic_losses)
            plt.title('Critic ì†ì‹¤')
            plt.xlabel('ì—í”¼ì†Œë“œ')
            plt.ylabel('ì†ì‹¤')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(result_dir / "training_curves.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # í‰ê°€ ë³´ìƒ ê³¡ì„  (ë³„ë„)
        if self.eval_rewards:
            plt.figure(figsize=(10, 6))
            episodes = list(range(self.evaluate_interval, self.num_episodes + 1, self.evaluate_interval))[:len(self.eval_rewards)]
            plt.plot(episodes, self.eval_rewards, 'o-')
            
            # ì•ˆì •í™” ê¸°ëŠ¥ ì •ë³´
            if self.agent.stabilization_enabled and self.best_performance > float('-inf'):
                plt.axhline(y=self.best_performance, color='r', linestyle='--', label=f'Best Performance: {self.best_performance:.2f}')
                plt.legend()
            
            plt.title('í‰ê°€ ë³´ìƒ')
            plt.xlabel('ì—í”¼ì†Œë“œ')
            plt.ylabel('í‰ê·  ë³´ìƒ')
            plt.grid(True, alpha=0.3)
            plt.savefig(result_dir / "eval_rewards.png", dpi=300, bbox_inches='tight')
            plt.close()

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ Trainer í´ë˜ìŠ¤ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±)
class Trainer(StabilizedTrainer):
    """ê¸°ì¡´ Trainer í´ë˜ìŠ¤ì˜ í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€"""
    
    def __init__(self, *args, **kwargs):
        # ì•ˆì •í™” ê´€ë ¨ íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        stabilization_defaults = {
            'use_early_stopping': False,
            'early_stopping_patience': 50,
            'loss_monitoring_enabled': True,
            'performance_monitoring_enabled': True,
            'adaptive_learning_rate': False,
            'lr_decay_factor': 0.95,
            'lr_decay_patience': 20
        }
        
        # ê¸°ì¡´ kwargsì— ì•ˆì •í™” ê¸°ë³¸ê°’ ì¶”ê°€ (ë®ì–´ì“°ì§€ ì•ŠìŒ)
        for key, value in stabilization_defaults.items():
            kwargs.setdefault(key, value)
        
        super().__init__(*args, **kwargs)
        
        # ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„± ë¡œê·¸
        LOGGER.info("ê¸°ì¡´ Trainer í´ë˜ìŠ¤ í˜¸í™˜ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")