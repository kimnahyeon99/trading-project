"""
SAC 모델 학습을 위한 트레이너 모듈 (안정화 기능 지원)
"""
import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Union, Optional, Any
from pathlib import Path
import time
from tqdm import tqdm
from src.environment.sequential_episode_manager import SequentialEpisodeManager, SequentialTradingEnvironment, create_sequential_training_setup
from src.environment.sequential_episode_manager import create_sequential_training_setup

from src.config.ea_teb_config import (
    DEVICE,
    BATCH_SIZE,
    NUM_EPISODES,
    EVALUATE_INTERVAL,
    SAVE_MODEL_INTERVAL,
    MAX_STEPS_PER_EPISODE,
    MODELS_DIR,
    RESULTS_DIR,
    LOGGER,
    overlap_ratio
)
from src.models.sac_agent import SACAgent  # 통합된 SACAgent 사용
from src.environment.trading_env import TradingEnvironment, MultiAssetTradingEnvironment
from src.utils.utils import create_directory, plot_learning_curve, plot_equity_curve, get_timestamp

class StabilizedTrainer:
    """
    SAC 모델 학습을 위한 트레이너 클래스 (안정화 기능 지원)
    """
    
    def __init__(
        self,
        agent: SACAgent,
        env: Union[TradingEnvironment, MultiAssetTradingEnvironment],
        train_env: Optional[Union[TradingEnvironment, MultiAssetTradingEnvironment]] = None,
        valid_env: Optional[Union[TradingEnvironment, MultiAssetTradingEnvironment]] = None,
        test_env: Optional[Union[TradingEnvironment, MultiAssetTradingEnvironment]] = None,
        batch_size: int = BATCH_SIZE,
        num_episodes: int = NUM_EPISODES,
        evaluate_interval: int = EVALUATE_INTERVAL,
        save_interval: int = SAVE_MODEL_INTERVAL,
        max_steps: int = MAX_STEPS_PER_EPISODE,
        models_dir: Union[str, Path] = MODELS_DIR,
        results_dir: Union[str, Path] = RESULTS_DIR,
        # 안정화 관련 추가 파라미터
        use_early_stopping: bool = False,
        early_stopping_patience: int = 50,
        loss_monitoring_enabled: bool = True,
        performance_monitoring_enabled: bool = True,
        adaptive_learning_rate: bool = False,
        lr_decay_factor: float = 0.95,
        lr_decay_patience: int = 20
    ):
        """
        StabilizedTrainer 클래스 초기화
        
        Args:
            agent: 학습할 SAC 에이전트
            env: 학습에 사용할 트레이딩 환경
            train_env: 훈련 환경 (별도 지정 시)
            valid_env: 검증 환경
            test_env: 테스트 환경
            batch_size: 배치 크기
            num_episodes: 학습할 총 에피소드 수
            evaluate_interval: 평가 간격 (에피소드 단위)
            save_interval: 모델 저장 간격 (에피소드 단위)
            max_steps: 에피소드당 최대 스텝 수
            models_dir: 모델 저장 디렉토리
            results_dir: 결과 저장 디렉토리
            
            # 안정화 관련 파라미터
            use_early_stopping: 조기 종료 사용 여부
            early_stopping_patience: 조기 종료 patience
            loss_monitoring_enabled: Loss 모니터링 활성화
            performance_monitoring_enabled: 성능 모니터링 활성화
            adaptive_learning_rate: 적응형 학습률 사용
            lr_decay_factor: 학습률 감소 계수
            lr_decay_patience: 학습률 감소 patience
        """
        self.agent = agent
        self.env = env
        self.batch_size = batch_size
        self.num_episodes = num_episodes
        self.evaluate_interval = evaluate_interval
        self.save_interval = save_interval
        self.max_steps = max_steps
        self.models_dir = Path(models_dir)
        self.results_dir = Path(results_dir)
        
        self.train_env = train_env
        self.valid_env = valid_env
        self.test_env = test_env
        
        # 안정화 관련 설정
        self.use_early_stopping = use_early_stopping
        self.early_stopping_patience = early_stopping_patience
        self.loss_monitoring_enabled = loss_monitoring_enabled
        self.performance_monitoring_enabled = performance_monitoring_enabled
        self.adaptive_learning_rate = adaptive_learning_rate
        self.lr_decay_factor = lr_decay_factor
        self.lr_decay_patience = lr_decay_patience
        
        # 디렉토리 생성
        create_directory(self.models_dir)
        create_directory(self.results_dir)
        
        # 학습 통계
        self.episode_rewards = []
        self.episode_lengths = []
        self.eval_rewards = []
        self.train_losses = []
        
        # 안정화 모니터링용 변수
        self.loss_history = []
        self.performance_history = []
        self.best_performance = float('-inf')
        self.best_performance_episode = 0
        self.lr_decay_counter = 0
        self.early_stop_counter = 0
        
        # 리스크 관리 통계
        self.risk_episodes = []  # 리스크 한도 초과 에피소드 기록
        self.max_drawdown_episodes = []  # 최대 낙폭 기록
        self.daily_loss_episodes = []  # 일일 손실 기록
        # 안정화 기능 로깅
        stabilization_status = "안정화 기능 포함" if agent.stabilization_enabled else "기본 버전"
        LOGGER.info(f"Trainer 초기화 완료 ({stabilization_status}): {num_episodes}개 에피소드, 배치 크기 {batch_size}")
        
        if agent.stabilization_enabled:
            LOGGER.info("✅ 추가 안정화 기능:")
            LOGGER.info(f"   └─ 조기 종료: {'활성화' if use_early_stopping else '비활성화'}")
            LOGGER.info(f"   └─ Loss 모니터링: {'활성화' if loss_monitoring_enabled else '비활성화'}")
            LOGGER.info(f"   └─ 성능 모니터링: {'활성화' if performance_monitoring_enabled else '비활성화'}")
            LOGGER.info(f"   └─ 적응형 학습률: {'활성화' if adaptive_learning_rate else '비활성화'}")
    
    def train(self) -> Dict[str, List[float]]:
        """
        SAC 모델 학습 수행 (안정화 기능 포함)
        
        Returns:
            학습 통계 딕셔너리
        """
        start_time = time.time()
        timestamp = get_timestamp()
        
        stabilization_info = " (안정화 기능)" if self.agent.stabilization_enabled else ""
        LOGGER.info(f"학습 시작{stabilization_info}: {self.num_episodes}개 에피소드")
        
        # 순차적 환경 설정 확인 및 생성
        if not hasattr(self.env, 'episode_manager'):
            self.env, self.episode_manager = create_sequential_training_setup(
                self.env, 
                overlap_ratio=overlap_ratio,
                logger=LOGGER
            )
            LOGGER.info("✅ 순차적 에피소드 학습 모드 활성화")
        
        for episode in range(1, self.num_episodes + 1):
            episode_start_time = time.time()
            
            # 순차적 리셋 지원
            if hasattr(self.env, 'reset') and 'episode_num' in self.env.reset.__code__.co_varnames:
                state = self.env.reset(episode_num=episode - 1)
            else:
                state = self.env.reset()
            
            episode_reward = 0
            episode_loss = {"actor_loss": 0, "critic_loss": 0, "alpha_loss": 0, "entropy": 0}
            episode_steps = 0
            done = False
            
            # 에피소드 진행
            while not done and episode_steps < self.max_steps:
                # 행동 선택 (딕셔너리 형태의 상태를 그대로 전달)
                action = self.agent.select_action(state)
                
                # 환경에서 한 스텝 진행
                next_state, reward, done, info = self.env.step(action)
                
                # 경험 저장 (안정화 기능이 add_experience에서 자동 처리됨)
                self.agent.add_experience(state, action, reward, next_state, done)
                
                # 모델 업데이트
                if len(self.agent.replay_buffer) > self.batch_size:
                    loss = self.agent.update_parameters(self.batch_size)
                    
                    # 손실 누적
                    for k, v in loss.items():
                        if k in episode_loss:
                            episode_loss[k] += v
                        else:
                            episode_loss[k] = v  # 새로운 키가 있을 경우 추가
                    
                    # 안정화 기능: Loss 모니터링
                    if self.agent.stabilization_enabled and self.loss_monitoring_enabled:
                        self._monitor_loss(loss)
                
                state = next_state
                episode_reward += reward
                episode_steps += 1
            
            # 에피소드 통계 기록
            self.episode_rewards.append(episode_reward)
            self.episode_lengths.append(episode_steps)
            
            # 손실 평균 계산 및 기록
            if episode_steps > 0:
                for k in episode_loss:
                    episode_loss[k] /= episode_steps
            self.train_losses.append(episode_loss)
            
            # 안정화 기능: 성능 모니터링
            if self.agent.stabilization_enabled and self.performance_monitoring_enabled:
                self._monitor_performance(episode_reward, episode)
            
            # 진행 상황 로깅
            episode_time = time.time() - episode_start_time
            
            # 안정화 상태 정보 포함 로깅
            stabilization_status = ""
            if self.agent.stabilization_enabled:
                warmup_status = "✅" if self.agent.train_step_counter >= self.agent.warmup_steps else "❌"
                stabilization_status = f" | 워밍업: {warmup_status} | 훈련스텝: {self.agent.train_step_counter}"
            
            # 리스크 정보 추가
            current_max_drawdown = self.max_drawdown_episodes[-1] if self.max_drawdown_episodes else 0
            current_daily_loss = self.daily_loss_episodes[-1] if self.daily_loss_episodes else 0
            risk_status = f" | 낙폭: {current_max_drawdown:.1f}% | 일일손실: {current_daily_loss:.1f}%"
            LOGGER.info(f"에피소드 {episode}/{self.num_episodes} - 보상: {episode_reward:.2f}, 스텝: {episode_steps}, 시간: {episode_time:.2f}초{stabilization_status}")
            
            # 주기적 평가
            if episode % self.evaluate_interval == 0:
                eval_reward = self.evaluate()
                self.eval_rewards.append(eval_reward)
                LOGGER.info(f"평가 결과 (에피소드 {episode}) - 보상: {eval_reward:.2f}")
                
                # 안정화 기능: 적응형 학습률
                if self.agent.stabilization_enabled and self.adaptive_learning_rate:
                    self._adjust_learning_rate(eval_reward, episode)
            
            # 주기적 모델 저장
            if episode % self.save_interval == 0:
                model_path = self.agent.save_model(self.models_dir, f"episode_{episode}_")
                LOGGER.info(f"모델 저장 완료: {model_path}")
            
            # 안정화 기능: 조기 종료 체크
            if self.agent.stabilization_enabled and self.use_early_stopping:
                if self._check_early_stopping(episode):
                    LOGGER.info(f"✅ 조기 종료 조건 충족 (에피소드 {episode})")
                    break
            
            # 학습 곡선 업데이트 (덜 자주)
            if episode % 50 == 0:
                self._plot_training_curves(timestamp)

        # 최종 모델 저장
        final_model_path = self.agent.save_model(
            save_dir=str(self.models_dir),
            prefix='',  # 접두사 없음
            model_type=getattr(self.agent, 'model_type', 'mlp'),
            symbol=getattr(self, 'symbol', None),
            symbols=getattr(self, 'symbols', None)
        )
        LOGGER.info(f"최종 모델 저장 완료: {final_model_path}")
        
        # 최종 학습 곡선 저장
        self._plot_training_curves(timestamp)
        
        # 학습 시간 계산
        total_time = time.time() - start_time
        
        # 최종 학습 요약
        stabilization_info = " (안정화 기능)" if self.agent.stabilization_enabled else ""
        LOGGER.info(f"학습 완료{stabilization_info}: 총 시간 {total_time:.2f}초 ({total_time/60:.2f}분)")
        
        if self.agent.stabilization_enabled:
            LOGGER.info("✅ 안정화 기능 최종 요약:")
            LOGGER.info(f"   └─ 총 훈련 스텝: {self.agent.train_step_counter:,}")
            LOGGER.info(f"   └─ 워밍업 완료: {'✅' if self.agent.train_step_counter >= self.agent.warmup_steps else '❌'}")
            LOGGER.info(f"   └─ 최고 성능 에피소드: {self.best_performance_episode}")
            LOGGER.info(f"   └─ 최고 성능 점수: {self.best_performance:.4f}")
            if self.adaptive_learning_rate and self.lr_decay_counter > 0:
                LOGGER.info(f"   └─ 학습률 조정 횟수: {self.lr_decay_counter}")

        # 학습 통계 반환
        return {
            "episode_rewards": self.episode_rewards,
            "episode_lengths": self.episode_lengths,
            "eval_rewards": self.eval_rewards,
            "actor_losses": [loss["actor_loss"] for loss in self.train_losses],
            "critic_losses": [loss["critic_loss"] for loss in self.train_losses],
            "alpha_losses": [loss["alpha_loss"] for loss in self.train_losses],
            "entropy_values": [loss["entropy"] for loss in self.train_losses]
        }
    
    def _monitor_loss(self, loss: Dict[str, float]) -> None:
        """Loss 모니터링 (안정화 기능)"""
        self.loss_history.append(loss)
        
        # 최근 loss 이상값 감지
        if len(self.loss_history) > 10:
            recent_losses = self.loss_history[-10:]
            avg_actor_loss = np.mean([l.get('actor_loss', 0) for l in recent_losses])
            avg_critic_loss = np.mean([l.get('critic_loss', 0) for l in recent_losses])
            
            # 임계값 설정 (동적으로 조정 가능)
            actor_threshold = 100.0
            critic_threshold = 100.0
            
            if avg_actor_loss > actor_threshold or avg_critic_loss > critic_threshold:
                LOGGER.warning(f"✅ 높은 Loss 감지: Actor={avg_actor_loss:.4f}, Critic={avg_critic_loss:.4f}")
                
                # 자동 대응 (선택적)
                if hasattr(self.agent, 'emergency_lr_reduction') and self.agent.emergency_lr_reduction:
                    self._emergency_lr_reduction()
    
    def _monitor_performance(self, reward: float, episode: int) -> None:
        """성능 모니터링 (안정화 기능)"""
        self.performance_history.append(reward)
        
        # 최고 성능 업데이트
        if reward > self.best_performance:
            self.best_performance = reward
            self.best_performance_episode = episode
            self.early_stop_counter = 0  # 카운터 리셋
        else:
            self.early_stop_counter += 1
    
    def _adjust_learning_rate(self, current_performance: float, episode: int) -> None:
        """적응형 학습률 조정 (안정화 기능)"""
        if len(self.eval_rewards) < 2:
            return
        
        # 최근 성능 개선이 없으면 학습률 감소
        recent_performance = np.mean(self.eval_rewards[-3:]) if len(self.eval_rewards) >= 3 else current_performance
        prev_performance = np.mean(self.eval_rewards[-6:-3]) if len(self.eval_rewards) >= 6 else self.eval_rewards[0]
        
        if recent_performance <= prev_performance:
            self.lr_decay_counter += 1
            
            if self.lr_decay_counter >= self.lr_decay_patience:
                # 학습률 감소
                for param_group in self.agent.actor_optimizer.param_groups:
                    param_group['lr'] *= self.lr_decay_factor
                for param_group in self.agent.critic_optimizer.param_groups:
                    param_group['lr'] *= self.lr_decay_factor
                
                new_actor_lr = self.agent.actor_optimizer.param_groups[0]['lr']
                new_critic_lr = self.agent.critic_optimizer.param_groups[0]['lr']
                
                LOGGER.info(f"✅ 학습률 조정됨 (에피소드 {episode}): Actor={new_actor_lr:.6f}, Critic={new_critic_lr:.6f}")
                self.lr_decay_counter = 0
        else:
            self.lr_decay_counter = 0
    
    def _emergency_lr_reduction(self) -> None:
        """응급 학습률 감소 (안정화 기능)"""
        emergency_factor = 0.1
        
        for param_group in self.agent.actor_optimizer.param_groups:
            param_group['lr'] *= emergency_factor
        for param_group in self.agent.critic_optimizer.param_groups:
            param_group['lr'] *= emergency_factor
        
        LOGGER.warning(f"❌ 응급 학습률 감소 적용: {emergency_factor}배 감소")
    
    def _check_early_stopping(self, episode: int) -> bool:
        """조기 종료 조건 확인 (안정화 기능)"""
        if self.early_stop_counter >= self.early_stopping_patience:
            LOGGER.info(f"조기 종료: {self.early_stopping_patience} 에피소드 동안 성능 개선 없음")
            return True
        
        # 추가 조건: 매우 낮은 성능이 지속되는 경우
        if len(self.episode_rewards) >= 100:
            recent_avg = np.mean(self.episode_rewards[-100:])
            if recent_avg < -1000:  # 임계값 설정
                LOGGER.info(f"조기 종료: 최근 성능이 매우 낮음 (평균: {recent_avg:.2f})")
                return True
        
        return False
    
    def evaluate(self, num_episodes: int = 1) -> float:
        """
        현재 정책 평가 (안정화 기능 포함)

        Args:
            num_episodes: 평가할 에피소드 수

        Returns:
            평균 에피소드 보상
        """
        total_reward = 0
        env = self.train_env if self.train_env is not None else self.env

        for _ in range(num_episodes):
            state = env.reset()
            episode_reward = 0
            done = False

            while not done:
                # 평가 모드로 행동 선택
                action = self.agent.select_action(state, evaluate=True)
                next_state, reward, done, _ = env.step(action)
                state = next_state
                episode_reward += reward

            total_reward += episode_reward

        avg_reward = total_reward / num_episodes
        
        # 안정화 기능: 평가 결과 모니터링
        if self.agent.stabilization_enabled and self.performance_monitoring_enabled:
            self._monitor_performance(avg_reward, len(self.eval_rewards) + 1)
        
        return avg_reward

    def train_with_sequential_episodes(self) -> Dict[str, List[float]]:
        """
        순차적 에피소드 데이터 활용 학습 수행 (안정화 기능 포함)
        
        Returns:
            학습 통계 딕셔너리
        """
        start_time = time.time()
        timestamp = get_timestamp()
        
        # 순차적 환경 설정 생성
        if not hasattr(self.env, 'episode_manager'):
            from sequential_episode_manager import create_sequential_training_setup
            self.env, self.episode_manager = create_sequential_training_setup(
                self.env, 
                overlap_ratio=overlap_ratio,  # 안겹침
                logger=LOGGER
            )
            LOGGER.info("✅ 순차적 에피소드 학습 모드 활성화")
        
        stabilization_info = " (안정화 기능)" if self.agent.stabilization_enabled else ""
        LOGGER.info(f"학습 시작{stabilization_info}: {self.num_episodes}개 에피소드 (순차적 데이터 활용)")
        
        for episode in range(1, self.num_episodes + 1):
            episode_start_time = time.time()
            
            # 순차적 리셋 (에피소드 번호 전달)
            state = self.env.reset(episode_num=episode - 1)
            
            episode_reward = 0
            episode_loss = {"actor_loss": 0, "critic_loss": 0, "alpha_loss": 0, "entropy": 0}
            episode_steps = 0
            done = False
            
            # 에피소드 메타정보 로깅
            if episode <= 5 or episode % 20 == 0:
                meta = getattr(self.env, 'episode_meta_info', {})
                LOGGER.info(
                    f"✅ Episode {episode}: 데이터 범위 "
                    f"[{meta.get('start_index', 'N/A')}~{meta.get('end_index', 'N/A')}] "
                    f"({meta.get('actual_length', 'N/A')} steps)"
                )
            
            # 에피소드 진행
            while not done:
                # 행동 선택
                action = self.agent.select_action(state)
                
                # 환경에서 한 스텝 진행
                next_state, reward, done, info = self.env.step(action)
                
                # 경험 저장 (안정화 기능이 add_experience에서 자동 처리됨)
                self.agent.add_experience(state, action, reward, next_state, done)
                
                # 모델 업데이트
                if len(self.agent.replay_buffer) > self.batch_size:
                    loss = self.agent.update_parameters(self.batch_size)
                    
                    # 손실 누적
                    for k, v in loss.items():
                        if k in episode_loss:
                            episode_loss[k] += v
                        else:
                            episode_loss[k] = v
                    
                    # 안정화 기능: Loss 모니터링
                    if self.agent.stabilization_enabled and self.loss_monitoring_enabled:
                        self._monitor_loss(loss)
                
                state = next_state
                episode_reward += reward
                episode_steps += 1
                # 리스크 정보 수집
                if 'risk_limit_exceeded' in info and info['risk_limit_exceeded']:
                    self.risk_episodes.append({
                        'episode': episode,
                        'step': episode_steps,
                        'max_drawdown_pct': info.get('max_drawdown_pct', 0),
                        'daily_loss_pct': info.get('daily_loss_pct', 0),
                        'portfolio_value': info.get('portfolio_value', 0)
                    })
                    LOGGER.warning(f"🚨 에피소드 {episode} 리스크 한도 초과로 조기 종료")
                
                # 낙폭/손실 통계 기록
                self.max_drawdown_episodes.append(info.get('max_drawdown_pct', 0))
                self.daily_loss_episodes.append(info.get('daily_loss_pct', 0))
                
                # 안전장치: 너무 긴 에피소드 방지
                if episode_steps >= self.max_steps:
                    LOGGER.warning(f"Episode {episode}: 최대 스텝 도달로 강제 종료")
                    break
            
            # 에피소드 통계 기록
            self.episode_rewards.append(episode_reward)
            self.episode_lengths.append(episode_steps)
            
            # 손실 평균 계산 및 기록
            if episode_steps > 0:
                for k in episode_loss:
                    episode_loss[k] /= episode_steps
            self.train_losses.append(episode_loss)
            
            # 안정화 기능: 성능 모니터링
            if self.agent.stabilization_enabled and self.performance_monitoring_enabled:
                self._monitor_performance(episode_reward, episode)
            
            # 진행 상황 로깅
            episode_time = time.time() - episode_start_time
            
            # 데이터 활용률 계산
            meta = getattr(self.env, 'episode_meta_info', {})
            data_utilization = meta.get('data_utilization', episode_steps / self.max_steps)
            
            # 안정화 상태 정보 포함
            stabilization_status = ""
            if self.agent.stabilization_enabled:
                warmup_status = "✅" if self.agent.train_step_counter >= self.agent.warmup_steps else "❌"
                stabilization_status = f" | 워밍업: {warmup_status}"
            
            LOGGER.info(
                f"Episode {episode}/{self.num_episodes} - "
                f"보상: {episode_reward:.2f}, "
                f"스텝: {episode_steps}, "
                f"데이터활용: {data_utilization:.1%}, "
                f"시간: {episode_time:.2f}초{stabilization_status}"
            )
            
            # 사이클 완료 로깅
            if meta.get('is_last_in_cycle', False):
                coverage_summary = self.episode_manager.get_coverage_summary()
                LOGGER.info(f"✅ 데이터 사이클 완료 - 고유 커버리지: {coverage_summary['unique_coverage_pct']:.1f}%")
            
            # 주기적 평가
            if episode % self.evaluate_interval == 0:
                eval_reward = self.evaluate()
                self.eval_rewards.append(eval_reward)
                LOGGER.info(f"평가 결과 (에피소드 {episode}) - 보상: {eval_reward:.2f}")
                
                # 안정화 기능: 적응형 학습률
                if self.agent.stabilization_enabled and self.adaptive_learning_rate:
                    self._adjust_learning_rate(eval_reward, episode)
            
            # 주기적 모델 저장
            if episode % self.save_interval == 0:
                model_path = self.agent.save_model(self.models_dir, f"episode_{episode}_")
                LOGGER.info(f"모델 저장 완료: {model_path}")
            
            # 안정화 기능: 조기 종료 체크
            if self.agent.stabilization_enabled and self.use_early_stopping:
                if self._check_early_stopping(episode):
                    LOGGER.info(f"✅ 조기 종료 조건 충족 (에피소드 {episode})")
                    break
        
        # 최종 커버리지 리포트
        self._log_final_coverage_report()
        
        # 최종 모델 저장
        final_model_path = self.agent.save_model(
            save_dir=str(self.models_dir),
            prefix='',
            model_type=getattr(self.agent, 'model_type', 'mlp'),
            symbol=getattr(self, 'symbol', None),
            symbols=getattr(self, 'symbols', None)
        )
        LOGGER.info(f"최종 모델 저장 완료: {final_model_path}")
        
        # 학습 시간 계산
        total_time = time.time() - start_time
        
        # 최종 요약
        stabilization_info = " (안정화 기능)" if self.agent.stabilization_enabled else ""
        LOGGER.info(f"순차적 학습 완료{stabilization_info}: 총 시간 {total_time:.2f}초 ({total_time/60:.2f}분)")
        
        if self.agent.stabilization_enabled:
            LOGGER.info("✅ 안정화 기능 최종 요약:")
            LOGGER.info(f"   └─ 총 훈련 스텝: {self.agent.train_step_counter:,}")
            LOGGER.info(f"   └─ 워밍업 완료: {'✅' if self.agent.train_step_counter >= self.agent.warmup_steps else '❌'}")
            LOGGER.info(f"   └─ 최고 성능 에피소드: {self.best_performance_episode}")
            LOGGER.info(f"   └─ 최고 성능 점수: {self.best_performance:.4f}")

        return {
            "episode_rewards": self.episode_rewards,
            "episode_lengths": self.episode_lengths,
            "eval_rewards": self.eval_rewards,
            "actor_losses": [loss["actor_loss"] for loss in self.train_losses],
            "critic_losses": [loss["critic_loss"] for loss in self.train_losses],
            "alpha_losses": [loss["alpha_loss"] for loss in self.train_losses],
            "entropy_values": [loss["entropy"] for loss in self.train_losses]
        }

    def _log_final_coverage_report(self):
        """최종 커버리지 리포트 로깅"""
        if hasattr(self, 'episode_manager'):
            coverage_summary = self.episode_manager.get_coverage_summary()
            LOGGER.info("✅ 최종 데이터 커버리지:")
            LOGGER.info(f"   └─ 고유 커버리지: {coverage_summary['unique_coverage_pct']:.1f}%")
            LOGGER.info(f"   └─ 총 에피소드: {coverage_summary['total_episodes']}")
            LOGGER.info(f"   └─ 데이터 길이: {coverage_summary['usable_length']:,}")

    def _plot_training_curves(self, timestamp: str) -> None:
        """
        학습 곡선 시각화 및 저장 (안정화 정보 포함)
        
        Args:
            timestamp: 파일명에 사용할 타임스탬프
        """
        # 결과 저장 디렉토리
        stabilization_suffix = "_stabilized" if self.agent.stabilization_enabled else ""
        result_dir = self.results_dir / f"training_{timestamp}{stabilization_suffix}"
        create_directory(result_dir)
        
        # 에피소드 보상 곡선
        plt.figure(figsize=(12, 8))
        plt.subplot(2, 2, 1)
        plt.plot(self.episode_rewards, alpha=0.6, label='Episode Rewards')
        if len(self.episode_rewards) > 10:
            # 이동 평균 추가
            window = min(50, len(self.episode_rewards) // 10)
            moving_avg = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')
            plt.plot(range(window-1, len(self.episode_rewards)), moving_avg, 'r-', label=f'Moving Avg ({window})')
        
        # 안정화 기능 정보 표시
        if self.agent.stabilization_enabled and self.best_performance_episode > 0:
            plt.axvline(x=self.best_performance_episode, color='g', linestyle='--', label=f'Best Performance (Ep {self.best_performance_episode})')
        
        plt.title('에피소드 보상')
        plt.xlabel('에피소드')
        plt.ylabel('보상')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 에피소드 길이 곡선
        plt.subplot(2, 2, 2)
        plt.plot(self.episode_lengths)
        plt.title('에피소드 길이')
        plt.xlabel('에피소드')
        plt.ylabel('스텝 수')
        plt.grid(True, alpha=0.3)
        
        # 손실 곡선들
        if self.train_losses:
            # Actor 손실
            plt.subplot(2, 2, 3)
            actor_losses = [loss["actor_loss"] for loss in self.train_losses]
            plt.plot(actor_losses)
            plt.title('Actor 손실')
            plt.xlabel('에피소드')
            plt.ylabel('손실')
            plt.grid(True, alpha=0.3)
            
            # Critic 손실
            plt.subplot(2, 2, 4)
            critic_losses = [loss["critic_loss"] for loss in self.train_losses]
            plt.plot(critic_losses)
            plt.title('Critic 손실')
            plt.xlabel('에피소드')
            plt.ylabel('손실')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(result_dir / "training_curves.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 평가 보상 곡선 (별도)
        if self.eval_rewards:
            plt.figure(figsize=(10, 6))
            episodes = list(range(self.evaluate_interval, self.num_episodes + 1, self.evaluate_interval))[:len(self.eval_rewards)]
            plt.plot(episodes, self.eval_rewards, 'o-')
            
            # 안정화 기능 정보
            if self.agent.stabilization_enabled and self.best_performance > float('-inf'):
                plt.axhline(y=self.best_performance, color='r', linestyle='--', label=f'Best Performance: {self.best_performance:.2f}')
                plt.legend()
            
            plt.title('평가 보상')
            plt.xlabel('에피소드')
            plt.ylabel('평균 보상')
            plt.grid(True, alpha=0.3)
            plt.savefig(result_dir / "eval_rewards.png", dpi=300, bbox_inches='tight')
            plt.close()

# 하위 호환성을 위한 Trainer 클래스 (기존 코드와의 호환성)
class Trainer(StabilizedTrainer):
    """기존 Trainer 클래스의 하위 호환성 유지"""
    
    def __init__(self, *args, **kwargs):
        # 안정화 관련 파라미터를 기본값으로 설정
        stabilization_defaults = {
            'use_early_stopping': False,
            'early_stopping_patience': 50,
            'loss_monitoring_enabled': True,
            'performance_monitoring_enabled': True,
            'adaptive_learning_rate': False,
            'lr_decay_factor': 0.95,
            'lr_decay_patience': 20
        }
        
        # 기존 kwargs에 안정화 기본값 추가 (덮어쓰지 않음)
        for key, value in stabilization_defaults.items():
            kwargs.setdefault(key, value)
        
        super().__init__(*args, **kwargs)
        
        # 기존 코드 호환성 로그
        LOGGER.info("기존 Trainer 클래스 호환 모드로 실행됩니다.")